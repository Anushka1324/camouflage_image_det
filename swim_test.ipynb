{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ff373d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:05.794778Z",
     "iopub.status.busy": "2026-01-10T07:45:05.794172Z",
     "iopub.status.idle": "2026-01-10T07:45:07.021493Z",
     "shell.execute_reply": "2026-01-10T07:45:07.020852Z"
    },
    "papermill": {
     "duration": 1.235549,
     "end_time": "2026-01-10T07:45:07.022918",
     "exception": false,
     "start_time": "2026-01-10T07:45:05.787369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43933f2a",
   "metadata": {
    "_cell_guid": "53c610f7-732b-435e-9459-de2f9e3d80c7",
    "_uuid": "d0b94887-4b0f-4317-84c8-1a7777d760d0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:07.036946Z",
     "iopub.status.busy": "2026-01-10T07:45:07.036425Z",
     "iopub.status.idle": "2026-01-10T07:45:19.205319Z",
     "shell.execute_reply": "2026-01-10T07:45:19.204699Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 12.176315,
     "end_time": "2026-01-10T07:45:19.206664",
     "exception": false,
     "start_time": "2026-01-10T07:45:07.030349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import RandAugment\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa98bafd",
   "metadata": {
    "_cell_guid": "63fe8b36-a02c-4487-a769-3e87f2989073",
    "_uuid": "23ff4136-a399-4879-b737-c64e8a7b9c6d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:19.218174Z",
     "iopub.status.busy": "2026-01-10T07:45:19.217638Z",
     "iopub.status.idle": "2026-01-10T07:45:21.299723Z",
     "shell.execute_reply": "2026-01-10T07:45:21.299140Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.088761,
     "end_time": "2026-01-10T07:45:21.301025",
     "exception": false,
     "start_time": "2026-01-10T07:45:19.212264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "from graphviz import Digraph\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12667885",
   "metadata": {
    "_cell_guid": "8c96bf9f-8a4a-4b5b-ba3c-cc2904b1ab58",
    "_uuid": "2072ad41-1769-40e0-9dc2-ff2324551b11",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:21.311947Z",
     "iopub.status.busy": "2026-01-10T07:45:21.311603Z",
     "iopub.status.idle": "2026-01-10T07:45:21.398917Z",
     "shell.execute_reply": "2026-01-10T07:45:21.398185Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.093752,
     "end_time": "2026-01-10T07:45:21.399971",
     "exception": false,
     "start_time": "2026-01-10T07:45:21.306219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\": torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cacd86",
   "metadata": {
    "_cell_guid": "1ca1b022-43ef-4a4f-9bc8-9502667f799e",
    "_uuid": "b9c000da-bf1a-4523-bbb1-7b28ff015f10",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:21.411046Z",
     "iopub.status.busy": "2026-01-10T07:45:21.410670Z",
     "iopub.status.idle": "2026-01-10T07:45:21.414868Z",
     "shell.execute_reply": "2026-01-10T07:45:21.414341Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010664,
     "end_time": "2026-01-10T07:45:21.415850",
     "exception": false,
     "start_time": "2026-01-10T07:45:21.405186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8         # adjust if OOM\n",
    "EPOCHS = 15\n",
    "NUM_WORKERS = 4         # set 0 if worker issues on Kaggle\n",
    "LR = 3e-4\n",
    "LABEL_SMOOTH = 0.1\n",
    "SAVE_PATH = \"best_model.pth\"\n",
    "USE_SEGMENTATION = True\n",
    "\n",
    "# Loss weights from PDF suggestion\n",
    "ALPHA_DOM = 0.5\n",
    "BETA_SUPCON = 0.3\n",
    "ETA_CONS = 0.1\n",
    "GAMMA_SEG = 0.5\n",
    "\n",
    "# Mixup/CutMix probabilities and alphas\n",
    "PROB_MIXUP = 0.5\n",
    "PROB_CUTMIX = 0.5\n",
    "MIXUP_ALPHA = 0.2\n",
    "CUTMIX_ALPHA = 1.0\n",
    "\n",
    "# warmup epochs\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "# early stopping\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "FREEZE_EPOCHS = 5\n",
    "ACCUMULATION_STEPS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fff71",
   "metadata": {
    "_cell_guid": "6b93dda5-4bd4-4cc5-897f-2e3f7ce8d878",
    "_uuid": "32b3ea07-c3c9-4592-81c8-7e7c5f53b060",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004491,
     "end_time": "2026-01-10T07:45:21.425145",
     "exception": false,
     "start_time": "2026-01-10T07:45:21.420654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Noise + Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0399f9",
   "metadata": {
    "_cell_guid": "a36a568a-587b-4e7d-95ea-3a45d5fa1652",
    "_uuid": "8610d19e-aa2c-4c65-8dd3-63efff268002",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:21.435861Z",
     "iopub.status.busy": "2026-01-10T07:45:21.435651Z",
     "iopub.status.idle": "2026-01-10T07:45:21.442413Z",
     "shell.execute_reply": "2026-01-10T07:45:21.441863Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013088,
     "end_time": "2026-01-10T07:45:21.443464",
     "exception": false,
     "start_time": "2026-01-10T07:45:21.430376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.05):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        noisy_tensor = tensor + noise\n",
    "        return torch.clamp(noisy_tensor, 0., 1.)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
    "\n",
    "weak_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.02),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "strong_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.05),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad13e65",
   "metadata": {
    "_cell_guid": "0439529c-c10a-4627-b237-6d1ea7c53173",
    "_uuid": "8a0980ba-9609-4279-9de2-cd19d8a166b9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:21.453989Z",
     "iopub.status.busy": "2026-01-10T07:45:21.453781Z",
     "iopub.status.idle": "2026-01-10T07:45:21.471996Z",
     "shell.execute_reply": "2026-01-10T07:45:21.471338Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024789,
     "end_time": "2026-01-10T07:45:21.473018",
     "exception": false,
     "start_time": "2026-01-10T07:45:21.448229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weak_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n",
    "# strong_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n",
    "# val_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n",
    "\n",
    "# reading a file with multiple encodings\n",
    "def read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n",
    "    \"\"\"Tries to read a file using a list of common encodings.\"\"\"\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read {file_path} with encoding {encoding}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n",
    "\n",
    "def load_testing_dataset_info(info_file, image_dir):\n",
    "    \"\"\"Loads image paths and labels from the testing dataset info file.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # List of encodings to try\n",
    "    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n",
    "    lines = []\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(info_file, 'r', encoding=encoding) as f:\n",
    "                lines = f.readlines()\n",
    "            break  # If successful, break out of the loop\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read {info_file} with encoding {encoding}. Trying another encoding...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {info_file}: {e}\")\n",
    "            raise \n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            image_filename = parts[0]\n",
    "            # Ensure label is always 0 or 1\n",
    "            try:\n",
    "                label = int(parts[1])\n",
    "            except ValueError:\n",
    "                continue # Skip malformed lines\n",
    "                \n",
    "            label = 1 if label == 1 else 0  # Map label '1' to CAM and '0' to Non-CAM\n",
    "            image_full_path = os.path.join(image_dir, image_filename)  # Combine with image directory\n",
    "            image_paths.append(image_full_path)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "# MultiDataset class with proper encoding handling\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n",
    "        self.root_dirs = root_dirs\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "        self.use_masks = use_masks\n",
    "        self.samples = []\n",
    "\n",
    "        # Handle the testing dataset (testing-dataset images and labels)\n",
    "        if testing_image_paths is not None and testing_labels is not None:\n",
    "            for img_path, label in zip(testing_image_paths, testing_labels):\n",
    "                # Samples from testing-dataset are stored as 2-element tuples (img_path, label)\n",
    "                self.samples.append((img_path, label)) \n",
    "        \n",
    "        # Process other datasets (COD10K, CAMO, etc.)\n",
    "        if isinstance(txt_files, str):\n",
    "            txt_files = [txt_files]\n",
    "\n",
    "        all_lines = []\n",
    "        for t in txt_files:\n",
    "            if not os.path.exists(t):\n",
    "                raise RuntimeError(f\"TXT file not found: {t}\")\n",
    "\n",
    "            # Use the read_file_with_encoding function to handle different encodings\n",
    "            lines = read_file_with_encoding(t)\n",
    "            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n",
    "\n",
    "        for line, src_txt in all_lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 0:\n",
    "                continue\n",
    "\n",
    "            fname = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    lbl = int(parts[1])\n",
    "                except:\n",
    "                    # Fallback classification if label is not an integer\n",
    "                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            else:\n",
    "                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            \n",
    "            # Map labels to binary (CAM=1, Non-CAM=0)\n",
    "            lbl = 1 if lbl == 1 else 0\n",
    "\n",
    "            found = False\n",
    "            search_subs = [\n",
    "                \"\",  # If image is directly in root_dir (less common)\n",
    "                \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", # Common image folders \n",
    "                \"Images/Train\", \"Images/Test\", # CAMO-COCO style paths\n",
    "            ]\n",
    "            \n",
    "            base_fname = os.path.basename(fname)  \n",
    "\n",
    "            for rdir in self.root_dirs:\n",
    "                for sub in search_subs:\n",
    "                    img_path = os.path.join(rdir, sub, base_fname)\n",
    "                    if os.path.exists(img_path):\n",
    "                        # Samples from COD/CAMO are stored as 3-element tuples (img_path, lbl, rdir)\n",
    "                        self.samples.append((img_path, lbl, rdir))\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "            if not found:\n",
    "                print(f\"[WARN] File not found in any root: {base_fname} (Searched in {self.root_dirs})\")\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global IMG_SIZE \n",
    "        \n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        img_path = sample[0]\n",
    "        lbl = sample[1]\n",
    "        \n",
    "        # Define rdir for consistent mask lookup logic\n",
    "        if len(sample) == 3:\n",
    "            # If it's a 3-element tuple (COD/CAMO), rdir is the third element\n",
    "            rdir = sample[2]\n",
    "            # Root directory for testing-dataset (used as fallback for mask lookup)\n",
    "            testing_root = None \n",
    "        else:\n",
    "            rdir = os.path.dirname(os.path.dirname(img_path))\n",
    "            # Set testing_root explicitly for clarity if needed later, but rdir is now defined.\n",
    "\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.weak_transform:\n",
    "            weak = self.weak_transform(img)\n",
    "        else:\n",
    "            weak = transforms.ToTensor()(img)\n",
    "            \n",
    "        if self.strong_transform:\n",
    "            strong = self.strong_transform(img)\n",
    "        else:\n",
    "            strong = weak.clone()\n",
    "\n",
    "        mask = None\n",
    "        if self.use_masks:\n",
    "            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n",
    "            \n",
    "            # Use the defined rdir for mask search\n",
    "            found_mask = False\n",
    "            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n",
    "                mask_path = os.path.join(rdir, mask_dir, mask_name)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n",
    "                    m = np.array(m).astype(np.float32) / 255.0\n",
    "                    # Convert to binary mask: 0 or 1\n",
    "                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n",
    "                    found_mask = True\n",
    "                    break\n",
    "\n",
    "            if mask is None:\n",
    "                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n",
    "                \n",
    "        return weak, strong, lbl, mask\n",
    "\n",
    "def build_weighted_sampler(dataset):\n",
    "    \"\"\"\n",
    "    Builds a WeightedRandomSampler based on class imbalance.\n",
    "    FIXED: Safely extracts label (index 1) from both 2-element and 3-element tuples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Safely extract labels (always index 1, regardless of tuple length)\n",
    "    labels = [sample[1] for sample in dataset.samples]  \n",
    "    \n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    \n",
    "    # Ensure there are at least two classes to calculate class_weights\n",
    "    if len(counts) <= 1:\n",
    "        print(f\"[WARN] Only {len(counts)} class(es) found. Using equal weights.\")\n",
    "        weights = [1.0] * total\n",
    "    else:\n",
    "        # Calculate inverse frequency weights\n",
    "        class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n",
    "        weights = [class_weights[lbl] for lbl in labels]\n",
    "        \n",
    "    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38c226",
   "metadata": {
    "_cell_guid": "b86311ae-c1bb-4f22-91c0-d3d43ff59d08",
    "_uuid": "0e0569d2-4f3f-47be-ba5a-80aac8370acb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0046,
     "end_time": "2026-01-10T07:45:21.482438",
     "exception": false,
     "start_time": "2026-01-10T07:45:21.477838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a62a790",
   "metadata": {
    "_cell_guid": "8612016c-9564-42f6-be27-48e77e56c6eb",
    "_uuid": "2977418b-5a9b-4b00-bd2b-6e6d1380c03a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:21.492562Z",
     "iopub.status.busy": "2026-01-10T07:45:21.492355Z",
     "iopub.status.idle": "2026-01-10T07:45:34.610139Z",
     "shell.execute_reply": "2026-01-10T07:45:34.609176Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 13.124545,
     "end_time": "2026-01-10T07:45:34.611545",
     "exception": false,
     "start_time": "2026-01-10T07:45:21.487000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 8605 samples from 4 root directories.\n",
      "✅ Loaded 1652 samples from 4 root directories.\n",
      "Total Train samples: 8605 Total Val samples: 1652\n"
     ]
    }
   ],
   "source": [
    "info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n",
    "train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \n",
    "test_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n",
    "    \n",
    "# COD10K Info files\n",
    "train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n",
    "train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n",
    "test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n",
    "test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n",
    "\n",
    "# CAMO-COCO PATHS\n",
    "info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n",
    "\n",
    "# CAMO-COCO Info files\n",
    "train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n",
    "train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n",
    "test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n",
    "test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n",
    "\n",
    "# CAMO-COCO Root Directories\n",
    "train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n",
    "train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n",
    "\n",
    "# third dataset - NC4K + non-camouflage (places 365)\n",
    "testing_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\n",
    "testing_images_dir = \"/kaggle/input/testing-dataset/Images\"\n",
    "testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n",
    "# 1. Load the testing dataset info\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    testing_image_paths, testing_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "# 1. All Root Directories\n",
    "ALL_ROOT_DIRS = [\n",
    "    train_dir_cod,       \n",
    "    test_dir_cod,       \n",
    "    train_dir_camo_cam,  \n",
    "    train_dir_camo_noncam\n",
    "]\n",
    "# 2. Training TXT files: ALL COD10K/CAMO-COCO data (both train and test splits)\n",
    "ALL_TRAIN_TXTS = [\n",
    "    train_cam_txt2, train_noncam_txt2, \n",
    "]\n",
    "# 3. Validation TXT files: ONLY the 80% testing-dataset split will be used, so this list is empty.\n",
    "ALL_VAL_TXTS = []\n",
    "train_ds = MultiDataset(\n",
    "    root_dirs=ALL_ROOT_DIRS, \n",
    "    txt_files=ALL_TRAIN_TXTS,               \n",
    "    testing_image_paths=train_paths,        \n",
    "    testing_labels=train_labels,            \n",
    "    weak_transform=weak_tf, \n",
    "    strong_transform=strong_tf, \n",
    "    use_masks=USE_SEGMENTATION\n",
    ")   # Validation Dataset: No external data (via empty ALL_VAL_TXTS) + 80% testing-dataset split (via val_paths)\n",
    "val_ds = MultiDataset(\n",
    "    root_dirs=ALL_ROOT_DIRS,  \n",
    "    txt_files=ALL_VAL_TXTS,                 \n",
    "    testing_image_paths=val_paths,              \n",
    "    testing_labels=val_labels,              \n",
    "    weak_transform=val_tf, \n",
    "    strong_transform=None, \n",
    "    use_masks=USE_SEGMENTATION\n",
    ")\n",
    "\n",
    "# Build Sampler and DataLoader\n",
    "train_sampler = build_weighted_sampler(train_ds)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(\"Total Train samples:\", len(train_ds), \"Total Val samples:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690091b",
   "metadata": {
    "_cell_guid": "27410294-010a-4911-a7fe-8047a41f96b7",
    "_uuid": "9147384c-1b50-4cf1-93a9-da0d89a0615f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005413,
     "end_time": "2026-01-10T07:45:34.623030",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.617617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa220d3b",
   "metadata": {
    "_cell_guid": "7c590a73-9329-416b-b0e9-3cc63b6c61f7",
    "_uuid": "0562f881-a9a2-425b-84f7-b87f9f1de827",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:34.634966Z",
     "iopub.status.busy": "2026-01-10T07:45:34.634726Z",
     "iopub.status.idle": "2026-01-10T07:45:34.641533Z",
     "shell.execute_reply": "2026-01-10T07:45:34.640677Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014327,
     "end_time": "2026-01-10T07:45:34.642814",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.628487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DenseNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.densenet201(pretrained=pretrained).features\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n",
    "                feats.append(x)\n",
    "        return feats\n",
    "\n",
    "\n",
    "class MobileNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        out = x\n",
    "        for i, layer in enumerate(self.features):\n",
    "            out = layer(out)\n",
    "            if i in (2,5,9,12):\n",
    "                feats.append(out)\n",
    "        if len(feats) < 4:\n",
    "            feats.append(out)\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4ebce5",
   "metadata": {
    "_cell_guid": "602ab64f-0bcd-48fd-9c92-789c331c1ced",
    "_uuid": "4b565d5e-4ecb-457c-af1a-b89084719789",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:34.654783Z",
     "iopub.status.busy": "2026-01-10T07:45:34.654552Z",
     "iopub.status.idle": "2026-01-10T07:45:34.658928Z",
     "shell.execute_reply": "2026-01-10T07:45:34.658113Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011643,
     "end_time": "2026-01-10T07:45:34.660088",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.648445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwinExtractor(nn.Module):\n",
    "    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "277d0b29",
   "metadata": {
    "_cell_guid": "595c7ef4-f65e-4c97-818d-db3dea350d48",
    "_uuid": "77d2613c-fd8c-4bc9-a03a-3fbfa9258379",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:34.671994Z",
     "iopub.status.busy": "2026-01-10T07:45:34.671759Z",
     "iopub.status.idle": "2026-01-10T07:45:34.677216Z",
     "shell.execute_reply": "2026-01-10T07:45:34.676444Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012751,
     "end_time": "2026-01-10T07:45:34.678443",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.665692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CBAMlite(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, max(channels//reduction,4), 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(max(channels//reduction,4), channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.spatial = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x) * self.spatial(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c3ff1b",
   "metadata": {
    "_cell_guid": "3691f4eb-fe5e-4a90-8355-281bfb975eee",
    "_uuid": "a7ab254f-2a14-444a-bd57-e45aa8620035",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:34.690350Z",
     "iopub.status.busy": "2026-01-10T07:45:34.690082Z",
     "iopub.status.idle": "2026-01-10T07:45:34.695493Z",
     "shell.execute_reply": "2026-01-10T07:45:34.694670Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012838,
     "end_time": "2026-01-10T07:45:34.696758",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.683920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g_fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(dim, max(dim//4, 4), 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(max(dim//4,4), dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, H, X):\n",
    "        if H.shape[2:] != X.shape[2:]:\n",
    "            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n",
    "        g = self.g_fc(H)\n",
    "        return g * H + (1 - g) * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6404f9",
   "metadata": {
    "_cell_guid": "8e151f36-8629-46d4-8978-9dc6f44a874d",
    "_uuid": "95e9bd01-a25d-40bd-95d3-0d52401e3fd5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:34.708718Z",
     "iopub.status.busy": "2026-01-10T07:45:34.708494Z",
     "iopub.status.idle": "2026-01-10T07:45:34.715594Z",
     "shell.execute_reply": "2026-01-10T07:45:34.714973Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014388,
     "end_time": "2026-01-10T07:45:34.716729",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.702341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_cnn, d_swin, d_out):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(d_cnn, d_out)\n",
    "        self.k = nn.Linear(d_swin, d_out)\n",
    "        self.v = nn.Linear(d_swin, d_out)\n",
    "        self.scale = d_out ** -0.5\n",
    "    def forward(self, feat_cnn, feat_swin):\n",
    "        B, Cc, H, W = feat_cnn.shape\n",
    "        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n",
    "        if feat_swin.dim() == 4:\n",
    "            Bs, Cs, Hs, Ws = feat_swin.shape\n",
    "            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n",
    "        else:\n",
    "            kv = feat_swin\n",
    "        K = self.k(kv)\n",
    "        V = self.v(kv)\n",
    "        Q = self.q(q)\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d180e",
   "metadata": {
    "_cell_guid": "8d1a2d8c-d55a-4a1c-8242-9619a3169e93",
    "_uuid": "fdfd48ad-a120-426f-812d-eab414adf245",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004976,
     "end_time": "2026-01-10T07:45:34.727161",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.722185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Segmentation Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b34d5a93",
   "metadata": {
    "_cell_guid": "5ea62e96-bc4e-4218-a0ba-7009414c6204",
    "_uuid": "59784957-96f9-43f9-ab4c-ba4c5069c266",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:34.737844Z",
     "iopub.status.busy": "2026-01-10T07:45:34.737639Z",
     "iopub.status.idle": "2026-01-10T07:45:34.742862Z",
     "shell.execute_reply": "2026-01-10T07:45:34.742344Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011836,
     "end_time": "2026-01-10T07:45:34.743937",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.732101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SegDecoder(nn.Module):\n",
    "    def __init__(self, in_channels_list, mid_channels=128):\n",
    "        super().__init__()\n",
    "        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n",
    "        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n",
    "        self.out = nn.Conv2d(mid_channels, 1, 1)\n",
    "    def forward(self, feat_list):\n",
    "        target_size = feat_list[0].shape[2:]\n",
    "        ups = []\n",
    "        for f, p in zip(feat_list, self.projs):\n",
    "            x = p(f)\n",
    "            if x.shape[2:] != target_size:\n",
    "                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
    "            ups.append(x)\n",
    "        x = torch.cat(ups, dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6f22b",
   "metadata": {
    "_cell_guid": "226cd599-783c-4f41-8d35-ebcf350381b9",
    "_uuid": "f9354086-efdb-49de-9956-72bdc5c1c804",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004821,
     "end_time": "2026-01-10T07:45:34.753917",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.749096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Probing Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "014bc1fb",
   "metadata": {
    "_cell_guid": "2a3f1c47-b68b-4563-becc-b0d6171967fa",
    "_uuid": "c92e84d5-b62f-4c5c-baff-5897fd5bc9fe",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:34.764185Z",
     "iopub.status.busy": "2026-01-10T07:45:34.763993Z",
     "iopub.status.idle": "2026-01-10T07:45:42.426739Z",
     "shell.execute_reply": "2026-01-10T07:45:42.425887Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 7.669177,
     "end_time": "2026-01-10T07:45:42.428020",
     "exception": false,
     "start_time": "2026-01-10T07:45:34.758843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
      "100%|██████████| 77.4M/77.4M [00:00<00:00, 186MB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n",
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 59.3MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f140aac5c58428a9b0438d948df17a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet channels: [256, 512, 1792, 1920]\n",
      "MobileNet channels: [24, 40, 80, 112]\n",
      "Swin channels: [56, 28, 14, 7]\n"
     ]
    }
   ],
   "source": [
    "dnet = DenseNetExtractor().to(device).eval()\n",
    "mnet = MobileNetExtractor().to(device).eval()\n",
    "snet = SwinExtractor().to(device).eval()\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n",
    "    featsA = dnet(dummy)\n",
    "    featsB = mnet(dummy)\n",
    "    featsS = snet(dummy)\n",
    "chA = [f.shape[1] for f in featsA]\n",
    "chB = [f.shape[1] for f in featsB]\n",
    "chS = [f.shape[1] for f in featsS]\n",
    "print(\"DenseNet channels:\", chA)\n",
    "print(\"MobileNet channels:\", chB)\n",
    "print(\"Swin channels:\", chS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a88371",
   "metadata": {
    "_cell_guid": "93c9df34-5655-4f06-8252-ebbc742d4db7",
    "_uuid": "42955826-da73-49eb-a2b6-1eb868ac3be0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00552,
     "end_time": "2026-01-10T07:45:42.439801",
     "exception": false,
     "start_time": "2026-01-10T07:45:42.434281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fusion Model (DenseNet + MobileNet + Swin cross attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59e789fe",
   "metadata": {
    "_cell_guid": "e1370fdf-9622-4230-b0ee-8c6cfe884584",
    "_uuid": "71df8e36-6644-423d-86c1-0b9cc366e4b4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:42.452081Z",
     "iopub.status.busy": "2026-01-10T07:45:42.451853Z",
     "iopub.status.idle": "2026-01-10T07:45:43.638112Z",
     "shell.execute_reply": "2026-01-10T07:45:43.637284Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.193987,
     "end_time": "2026-01-10T07:45:43.639257",
     "exception": false,
     "start_time": "2026-01-10T07:45:42.445270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters (M): 51.586615\n"
     ]
    }
   ],
   "source": [
    "class FusionWithSwin(nn.Module):\n",
    "    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backA = DenseNetExtractor()\n",
    "        self.backB = MobileNetExtractor()\n",
    "        self.backS = SwinExtractor()\n",
    "        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n",
    "        self.L = L\n",
    "        self.d = d\n",
    "        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n",
    "        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n",
    "        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n",
    "        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n",
    "        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n",
    "        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n",
    "        self.reduce = nn.Conv2d(d * L, d, 1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.use_seg = use_seg\n",
    "        if self.use_seg:\n",
    "            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n",
    "\n",
    "        # Domain head for DANN (simple MLP)\n",
    "        self.domain_head = nn.Sequential(\n",
    "            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, grl_lambda=0.0):\n",
    "        fa = self.backA(x)\n",
    "        fb = self.backB(x)\n",
    "        fs = self.backS(x)\n",
    "        fused_feats = []\n",
    "        aligned_for_dec = []\n",
    "        for i in range(self.L):\n",
    "            a = self.alignA[i](fa[i])\n",
    "            a = self.cbamA[i](a)\n",
    "            b = self.alignB[i](fb[i])\n",
    "            b = self.cbamB[i](b)\n",
    "            if b.shape[2:] != a.shape[2:]:\n",
    "                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n",
    "            fused = self.gates[i](a, b)\n",
    "            swin_feat = fs[i]\n",
    "            swin_att = self.cross_atts[i](fused, swin_feat)\n",
    "            if swin_att.shape[2:] != fused.shape[2:]:\n",
    "                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n",
    "            fused = fused + swin_att\n",
    "            fused_feats.append(fused)\n",
    "            aligned_for_dec.append(fused)\n",
    "        target = fused_feats[-1]\n",
    "        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n",
    "        concat = torch.cat(upsampled, dim=1)\n",
    "        fused = self.reduce(concat)\n",
    "        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n",
    "        logits = self.classifier(z)\n",
    "        out = {\"logits\": logits, \"feat\": z}\n",
    "        if self.use_seg:\n",
    "            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n",
    "\n",
    "        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n",
    "        if grl_lambda > 0.0:\n",
    "            # GRL implemented outside (we'll pass z through GRL function)\n",
    "            pass\n",
    "        out[\"domain_logits\"] = self.domain_head(z)\n",
    "        return out\n",
    "\n",
    "# instantiate model\n",
    "model = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\n",
    "print(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3eb8a3",
   "metadata": {
    "_cell_guid": "8981b77b-e686-4024-a7ef-106fc71c5573",
    "_uuid": "66ed0d4e-2188-4630-b4b8-da16939ceaf1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:43.651979Z",
     "iopub.status.busy": "2026-01-10T07:45:43.651742Z",
     "iopub.status.idle": "2026-01-10T07:45:43.894323Z",
     "shell.execute_reply": "2026-01-10T07:45:43.893742Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.250152,
     "end_time": "2026-01-10T07:45:43.895541",
     "exception": false,
     "start_time": "2026-01-10T07:45:43.645389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.s = smoothing\n",
    "    def forward(self, logits, target):\n",
    "        c = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logp)\n",
    "            true_dist.fill_(self.s / (c - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n",
    "        return (-true_dist * logp).sum(dim=-1).mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, target):\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        ce = F.cross_entropy(logits, target, reduction='none')\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()\n",
    "\n",
    "def dice_loss_logits(pred_logits, target):\n",
    "    pred = torch.sigmoid(pred_logits)\n",
    "    target = target.float()\n",
    "    inter = (pred * target).sum(dim=(1,2,3))\n",
    "    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n",
    "    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n",
    "    return 1.0 - dice.mean()\n",
    "\n",
    "clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n",
    "clf_loss_focal = FocalLoss(gamma=1.5)\n",
    "seg_bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    num = 2 * (pred * target).sum() + smooth\n",
    "    den = pred.sum() + target.sum() + smooth\n",
    "    return 1 - (num / den)\n",
    "\n",
    "def seg_loss_fn(pred, mask):\n",
    "    if pred.shape[-2:] != mask.shape[-2:]:\n",
    "        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "960680a8",
   "metadata": {
    "_cell_guid": "6ab89b5b-ccc8-4af3-9590-bc566204b65a",
    "_uuid": "27aad2d9-4477-4430-b87c-2805045736df",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:43.908420Z",
     "iopub.status.busy": "2026-01-10T07:45:43.908003Z",
     "iopub.status.idle": "2026-01-10T07:45:43.914223Z",
     "shell.execute_reply": "2026-01-10T07:45:43.913745Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013454,
     "end_time": "2026-01-10T07:45:43.915229",
     "exception": false,
     "start_time": "2026-01-10T07:45:43.901775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Supervised contrastive Loss\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "    def forward(self, features, labels):\n",
    "        # features: [N, D], labels: [N]\n",
    "        device = features.device\n",
    "        f = F.normalize(features, dim=1)\n",
    "        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n",
    "        labels = labels.contiguous().view(-1,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        # remove diagonal\n",
    "        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n",
    "        logits = sim - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n",
    "        denom = exp_logits.sum(1, keepdim=True)\n",
    "        # for each i, positive samples are where mask==1 (excluding self)\n",
    "        pos_mask = mask - torch.eye(len(features), device=device)\n",
    "        pos_exp = (exp_logits * pos_mask).sum(1)\n",
    "        # avoid divide by zero\n",
    "        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n",
    "        # average only across anchors that have positives\n",
    "        valid = (pos_mask.sum(1) > 0).float()\n",
    "        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n",
    "        return loss\n",
    "supcon_loss_fn = SupConLoss(temperature=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eb3ce4c",
   "metadata": {
    "_cell_guid": "937a9a70-b252-4638-91b4-0acbcd8214f9",
    "_uuid": "ffb8ae99-636a-43c6-9797-f85ea97f4b17",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:43.927941Z",
     "iopub.status.busy": "2026-01-10T07:45:43.927498Z",
     "iopub.status.idle": "2026-01-10T07:45:43.931654Z",
     "shell.execute_reply": "2026-01-10T07:45:43.931089Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011633,
     "end_time": "2026-01-10T07:45:43.932661",
     "exception": false,
     "start_time": "2026-01-10T07:45:43.921028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Domain Adversarial: Gradient Reversal Layer (GRL)\n",
    "\n",
    "from torch.autograd import Function\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, l):\n",
    "        ctx.l = l\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.l, None\n",
    "\n",
    "def grad_reverse(x, l=1.0):\n",
    "    return GradReverse.apply(x, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc255ad3",
   "metadata": {
    "_cell_guid": "5d0a6d0b-5ee0-459f-a9c4-f361ce6591ba",
    "_uuid": "73e24310-551f-4554-bb03-48f305bb7ae3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:43.944648Z",
     "iopub.status.busy": "2026-01-10T07:45:43.944459Z",
     "iopub.status.idle": "2026-01-10T07:45:43.958516Z",
     "shell.execute_reply": "2026-01-10T07:45:43.957813Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021438,
     "end_time": "2026-01-10T07:45:43.959540",
     "exception": false,
     "start_time": "2026-01-10T07:45:43.938102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/4242938264.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "# Optimizer + scheduler + mixed precision + clipping\n",
    "# -----------------------------\n",
    "# param groups: smaller LR for backbones, larger for heads\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        head_params.append(param)\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': LR * 0.2},\n",
    "    {'params': head_params, 'lr': LR}\n",
    "], lr=LR, weight_decay=1e-4)\n",
    "\n",
    "# warmup + cosine schedule\n",
    "def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch) / float(max(1.0, warmup_epochs))\n",
    "        # cosine from warmup -> total\n",
    "        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Mixup & CutMix helpers\n",
    "# -----------------------------\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)   # use builtin int\n",
    "    cut_h = int(H * cut_rat)   # use builtin int\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    new_x = x.clone()\n",
    "    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return new_x, y, y[idx], lam_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857fd628",
   "metadata": {
    "_cell_guid": "8cefc78d-dfc4-4a80-853c-de8cdbaa0092",
    "_uuid": "98de1d78-23d6-4f6e-8856-1c2bf4801903",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005545,
     "end_time": "2026-01-10T07:45:43.970800",
     "exception": false,
     "start_time": "2026-01-10T07:45:43.965255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9bc9ccc",
   "metadata": {
    "_cell_guid": "4e9e207b-133f-40e0-8941-e99fb09ea18c",
    "_uuid": "8f0812c9-bc6d-452f-b8dd-b4983872595a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:43.982872Z",
     "iopub.status.busy": "2026-01-10T07:45:43.982688Z",
     "iopub.status.idle": "2026-01-10T10:09:26.390534Z",
     "shell.execute_reply": "2026-01-10T10:09:26.389572Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8623.130171,
     "end_time": "2026-01-10T10:09:27.106539",
     "exception": false,
     "start_time": "2026-01-10T07:45:43.976368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 1/15: 100%|██████████| 1076/1076 [06:11<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 4.1427 Acc: 0.4958 Prec: 0.4956 Rec: 0.4959 F1: 0.4873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val Loss: 1.5799 Acc: 0.4988 Prec: 0.5147 Rec: 0.5065 F1: 0.4226\n",
      "Saved best model at epoch 1 (F1 0.4226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 2/15: 100%|██████████| 1076/1076 [06:12<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 2.6635 Acc: 0.7044 Prec: 0.7097 Rec: 0.7036 F1: 0.7019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Val Loss: 1.8658 Acc: 0.9104 Prec: 0.9171 Rec: 0.9092 F1: 0.9098\n",
      "Saved best model at epoch 2 (F1 0.9098)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 3/15: 100%|██████████| 1076/1076 [06:12<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 2.2852 Acc: 0.8009 Prec: 0.8010 Rec: 0.8009 F1: 0.8009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Val Loss: 1.5356 Acc: 0.8971 Prec: 0.9122 Rec: 0.8952 F1: 0.8958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 4/15: 100%|██████████| 1076/1076 [06:11<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 2.2068 Acc: 0.8015 Prec: 0.8016 Rec: 0.8015 F1: 0.8015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Val Loss: 0.9334 Acc: 0.9607 Prec: 0.9606 Rec: 0.9607 F1: 0.9606\n",
      "Saved best model at epoch 4 (F1 0.9606)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 5/15: 100%|██████████| 1076/1076 [06:12<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 2.2184 Acc: 0.8066 Prec: 0.8066 Rec: 0.8066 F1: 0.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Val Loss: 0.9292 Acc: 0.9776 Prec: 0.9789 Rec: 0.9772 F1: 0.9776\n",
      "Saved best model at epoch 5 (F1 0.9776)\n",
      "--- Unfreezing all backbone layers at epoch 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 6/15: 100%|██████████| 1076/1076 [10:13<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 1.9359 Acc: 0.8460 Prec: 0.8460 Rec: 0.8460 F1: 0.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Val Loss: 0.8862 Acc: 0.9988 Prec: 0.9988 Rec: 0.9988 F1: 0.9988\n",
      "Saved best model at epoch 6 (F1 0.9988)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 7/15: 100%|██████████| 1076/1076 [10:07<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 1.7582 Acc: 0.8727 Prec: 0.8728 Rec: 0.8727 F1: 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Val Loss: 0.8716 Acc: 0.9964 Prec: 0.9964 Rec: 0.9963 F1: 0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 8/15: 100%|██████████| 1076/1076 [10:06<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 1.7066 Acc: 0.8846 Prec: 0.8846 Rec: 0.8846 F1: 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Val Loss: 0.8951 Acc: 0.9970 Prec: 0.9971 Rec: 0.9969 F1: 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 9/15: 100%|██████████| 1076/1076 [10:07<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 1.6354 Acc: 0.8723 Prec: 0.8723 Rec: 0.8723 F1: 0.8723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Val Loss: 0.8839 Acc: 0.9976 Prec: 0.9976 Rec: 0.9975 F1: 0.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 10/15: 100%|██████████| 1076/1076 [10:06<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 1.6336 Acc: 0.8693 Prec: 0.8693 Rec: 0.8692 F1: 0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Val Loss: 0.8609 Acc: 0.9982 Prec: 0.9982 Rec: 0.9981 F1: 0.9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 11/15: 100%|██████████| 1076/1076 [10:06<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 1.6379 Acc: 0.8746 Prec: 0.8746 Rec: 0.8746 F1: 0.8746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Val Loss: 0.8736 Acc: 0.9994 Prec: 0.9994 Rec: 0.9994 F1: 0.9994\n",
      "Saved best model at epoch 11 (F1 0.9994)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 12/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 12/15: 100%|██████████| 1076/1076 [10:06<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 1.4988 Acc: 0.8817 Prec: 0.8816 Rec: 0.8817 F1: 0.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Val Loss: 0.8709 Acc: 0.9927 Prec: 0.9930 Rec: 0.9926 F1: 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 13/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 13/15: 100%|██████████| 1076/1076 [10:06<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 1.4907 Acc: 0.8829 Prec: 0.8829 Rec: 0.8829 F1: 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Val Loss: 0.8572 Acc: 0.9988 Prec: 0.9988 Rec: 0.9988 F1: 0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 14/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 14/15: 100%|██████████| 1076/1076 [10:07<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 1.4672 Acc: 0.8924 Prec: 0.8924 Rec: 0.8924 F1: 0.8924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Val Loss: 0.8594 Acc: 0.9988 Prec: 0.9988 Rec: 0.9988 F1: 0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 15/15:   0%|          | 0/1076 [00:00<?, ?it/s]/tmp/ipykernel_20/1808691167.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
      "Train 15/15: 100%|██████████| 1076/1076 [10:08<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 1.4298 Acc: 0.8760 Prec: 0.8761 Rec: 0.8759 F1: 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Val Loss: 0.8672 Acc: 0.9994 Prec: 0.9994 Rec: 0.9994 F1: 0.9994\n",
      "Training finished. Best val F1: 0.9993944012912597 at epoch 11\n"
     ]
    }
   ],
   "source": [
    "best_vf1 = 0.0\n",
    "best_epoch = 0\n",
    "patience_count = 0\n",
    "\n",
    "def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n",
    "    # mix_info: (mode, y_a, y_b, lam) or None\n",
    "    if mix_info is None:\n",
    "        if use_focal:\n",
    "            return clf_loss_focal(logits, targets)\n",
    "        else:\n",
    "            return clf_loss_ce(logits, targets)\n",
    "    else:\n",
    "        # mixup/cutmix: soft labels\n",
    "        y_a, y_b, lam = mix_info\n",
    "        if use_focal:\n",
    "            # focal is not designed for soft labels; approximate by weighted CE\n",
    "            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n",
    "        else:\n",
    "            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n",
    "        return loss\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_f1': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_f1': [], 'val_acc': []\n",
    "}\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # --- CHANGED: Improved freeze/unfreeze strategy ---\n",
    "    # We set FREEZE_EPOCHS = 5 in cell 5 (to match WARMUP_EPOCHS)\n",
    "    if epoch <= FREEZE_EPOCHS:\n",
    "        # Freeze ALL backbone parameters\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in ['backA', 'backB', 'backS']):\n",
    "                p.requires_grad = False\n",
    "    elif epoch == FREEZE_EPOCHS + 1:\n",
    "        # Unfreeze all parameters *once* after freeze period\n",
    "        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    n_batches = 0\n",
    "\n",
    "    opt.zero_grad() \n",
    "    \n",
    "    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n",
    "        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "\n",
    "        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n",
    "        imgs = weak_imgs\n",
    "\n",
    "        # optionally apply mixup/cutmix on imgs (on weak view)\n",
    "        mix_info = None\n",
    "        rand = random.random()\n",
    "        if rand < PROB_MIXUP:\n",
    "            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "        elif rand < PROB_MIXUP + PROB_CUTMIX:\n",
    "            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
    "            out = model(imgs)  # returns logits, feat, seg, domain_logits\n",
    "            logits = out[\"logits\"]\n",
    "            feat = out[\"feat\"]\n",
    "            seg_out = out.get(\"seg\", None)\n",
    "            domain_logits = out.get(\"domain_logits\", None)\n",
    "\n",
    "            # classification loss (label-smoothing or focal)\n",
    "            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n",
    "\n",
    "            # segmentation loss if available & mask present\n",
    "            seg_loss = 0.0\n",
    "            if USE_SEGMENTATION and (masks is not None):\n",
    "                seg_pred = out[\"seg\"]\n",
    "                seg_loss = seg_loss_fn(seg_pred, masks)\n",
    "            # supcon loss on features (use features from weak)\n",
    "            supcon_loss = supcon_loss_fn(feat, labels)\n",
    "\n",
    "            # consistency: forward strong view and compare predictions\n",
    "            out_strong = model(strong_imgs)\n",
    "            logits_strong = out_strong[\"logits\"]\n",
    "            probs_weak = F.softmax(logits.detach(), dim=1)\n",
    "            probs_strong = F.softmax(logits_strong, dim=1)\n",
    "            # L2 between probability vectors (could be KL)\n",
    "            cons_loss = F.mse_loss(probs_weak, probs_strong)\n",
    "            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n",
    "            dom_loss = 0.0\n",
    "\n",
    "            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n",
    "\n",
    "            # 2. Scale the loss by accumulation steps to average the gradients\n",
    "            total_loss = total_loss / ACCUMULATION_STEPS \n",
    "\n",
    "        # Perform backward pass (gradients are accumulated until step is called)\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        # 3. Optimizer step only every ACCUMULATION_STEPS batches\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            # gradient clipping before step\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad() # Prepare for next accumulation cycle\n",
    "\n",
    "        running_loss += total_loss.item() * ACCUMULATION_STEPS # Re-scale back for correct loss tracking\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "        n_batches += 1\n",
    "\n",
    "    # 4. Take a final step if there are remaining gradients (i.e., last batch was not a multiple of ACCUMULATION_STEPS)\n",
    "    if n_batches % ACCUMULATION_STEPS != 0:\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # metrics (rest of the code remains the same)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n",
    "\n",
    "    # -------------------\n",
    "    # VALIDATION\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_y_true, val_y_pred = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for weak_imgs, _, labels, masks in val_loader:\n",
    "            imgs = weak_imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if masks is not None:\n",
    "                masks = masks.to(device)\n",
    "\n",
    "            out = model(imgs)\n",
    "            logits = out[\"logits\"]\n",
    "            feat = out[\"feat\"]\n",
    "            seg_out = out.get(\"seg\", None)\n",
    "            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n",
    "            if USE_SEGMENTATION and (masks is not None):\n",
    "                # loss += seg_loss_fn(seg_out, masks)\n",
    "                loss += GAMMA_SEG * seg_loss_fn(seg_out, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_y_true.extend(labels.cpu().numpy())\n",
    "            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "    vacc = accuracy_score(val_y_true, val_y_pred)\n",
    "    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n",
    "\n",
    "\n",
    "    history['train_loss'].append(running_loss / max(1, n_batches))\n",
    "    history['train_f1'].append(f1)\n",
    "    history['train_acc'].append(acc)\n",
    "    history['val_loss'].append(val_loss / max(1, len(val_loader)))\n",
    "    history['val_f1'].append(vf1)\n",
    "    history['val_acc'].append(vacc)\n",
    "\n",
    "    # early stopping & save best\n",
    "    if vf1 > best_vf1:\n",
    "        best_vf1 = vf1\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": opt.state_dict(),\n",
    "            \"best_vf1\": best_vf1\n",
    "        }, SAVE_PATH)\n",
    "        patience_count = 0\n",
    "        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        if patience_count >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2932761,
     "sourceId": 5051281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8580489,
     "sourceId": 13514489,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8582404,
     "sourceId": 13517101,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8669.366202,
   "end_time": "2026-01-10T10:09:31.016286",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-10T07:45:01.650084",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f140aac5c58428a9b0438d948df17a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d1bff4a6fc6940659ea9515fdde769b2",
        "IPY_MODEL_e1eed6124e034a84aea67f420d2ecae8",
        "IPY_MODEL_a9a10ed7db3b49f3a39b74e1bbcf48a4"
       ],
       "layout": "IPY_MODEL_a14561600c294f36b0aa726190477c26",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1e838b8e40b7477ca0bf56161017c562": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ce85fccd42742d09f28254a64347562": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "64700d5ef2944aa796a011ea5b62ccf0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81fcc3b9d1bc4a17a1ac9ff5820c3b69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a14561600c294f36b0aa726190477c26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9a10ed7db3b49f3a39b74e1bbcf48a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f256b80e55c34f3d823191cff644ca0c",
       "placeholder": "​",
       "style": "IPY_MODEL_81fcc3b9d1bc4a17a1ac9ff5820c3b69",
       "tabbable": null,
       "tooltip": null,
       "value": " 114M/114M [00:03&lt;00:00, 77.1MB/s]"
      }
     },
     "c6062cf5fe374ad3816b2ae6dd31646b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d1bff4a6fc6940659ea9515fdde769b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_64700d5ef2944aa796a011ea5b62ccf0",
       "placeholder": "​",
       "style": "IPY_MODEL_c6062cf5fe374ad3816b2ae6dd31646b",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "e1eed6124e034a84aea67f420d2ecae8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1e838b8e40b7477ca0bf56161017c562",
       "max": 114286722.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3ce85fccd42742d09f28254a64347562",
       "tabbable": null,
       "tooltip": null,
       "value": 114286722.0
      }
     },
     "f256b80e55c34f3d823191cff644ca0c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
