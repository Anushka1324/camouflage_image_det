{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5051281,"sourceType":"datasetVersion","datasetId":2932761},{"sourceId":13514447,"sourceType":"datasetVersion","datasetId":8580465},{"sourceId":13514489,"sourceType":"datasetVersion","datasetId":8580489},{"sourceId":13517101,"sourceType":"datasetVersion","datasetId":8582404}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:48:58.39367Z","iopub.execute_input":"2026-01-06T16:48:58.393892Z","iopub.status.idle":"2026-01-06T16:48:59.394742Z","shell.execute_reply.started":"2026-01-06T16:48:58.393871Z","shell.execute_reply":"2026-01-06T16:48:59.394066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:48:59.396378Z","iopub.execute_input":"2026-01-06T16:48:59.39679Z","iopub.status.idle":"2026-01-06T16:49:01.179107Z","shell.execute_reply.started":"2026-01-06T16:48:59.396757Z","shell.execute_reply":"2026-01-06T16:49:01.178399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EﬃcientNetV2","metadata":{}},{"cell_type":"code","source":"\nimport os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 20            \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 10\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\nhistory = {\n    'train_loss': [], 'val_loss': [],\n    'train_f1': [], 'val_f1': []\n}\nbest_val_preds = []\nbest_val_labels = []\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\n# class DenseNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         for name, layer in self.features._modules.items():\n#             x = layer(x)\n#             if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n#                 feats.append(x)\n#         return feats\n\n# --- 5. BACKBONE EXTRACTOR (DenseNet169) ---\n# --- 5. UPDATED DENSENET121 EXTRACTOR ---\n\n# --- 5. MOBILENET V3 LARGE EXTRACTOR ---\n# --- 5. EFFICIENTNET V2-S EXTRACTOR ---\n# --- 5. EFFICIENTNET V2-S EXTRACTOR ---\nclass EfficientNetV2Extractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # efficientnet_v2_s is faster and more accurate than MobileNet\n        self.base = models.efficientnet_v2_s(weights='IMAGENET1K_V1' if pretrained else None)\n        self.features = self.base.features\n        \n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            # Resolution reduction points for V2-S\n            if i in (2, 3, 5, 7):\n                feats.append(out)\n        if len(feats) == 0 or feats[-1].shape != out.shape:\n            feats.append(out)\n        return feats\n\n# --- 6. EFFICIENTNET V2 MODEL ---\nEFF_CHANNELS = 1280 \nNUM_CLASSES = 2 \n\nclass EfficientNetModel(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, out_channels=EFF_CHANNELS):\n        super().__init__()\n        self.backbone = EfficientNetV2Extractor(pretrained=True)\n        \n        # Initial Freeze\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(\n            nn.Linear(out_channels, 512), \n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes) \n        )\n        \n    def forward(self, x):\n        eff_feats = self.backbone(x)[-1]\n        feat = self.global_pool(eff_feats).view(x.size(0), -1)\n        logits = self.classifier(feat)\n        return {\n            \"logits\": logits, \n            \"feat\": feat,\n            \"seg\": torch.randn(x.size(0), 1, x.size(-2), x.size(-1)).to(x.device)\n        }\n# class MobileNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         out = x\n#         for i, layer in enumerate(self.features):\n#             out = layer(out)\n#             if i in (2,5,9,12):\n#                 feats.append(out)\n#         if len(feats) < 4:\n#             feats.append(out)\n#         return feats\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\n# DENSE_CHANNELS = 1920\n# # CORRECTION: The actual output features are 112, resulting in 2032 total features (1920+112). \n# # We MUST use 112 here to resolve the mat1 and mat2 dimension mismatch error (2032 vs 3200).\n# # MOBILE_CHANNELS = 112 \n# # TOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1920 + 112 = 2032\n# NUM_CLASSES = 2 \n\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n# --- Scenario 1 Changes ---\n# 1. 80/20 Split of testing-dataset remains the same\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n# 2. Training includes basic sets + 80% of testing-dataset\n    ALL_TRAIN_TXTS = [\n        train_cam_txt, train_noncam_txt,\n        train_cam_txt2, train_noncam_txt2, ]\n    # 2. Validation uses NO text files (Consists only of the 20% paths below)\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2,]\n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n        testing_image_paths=train_paths, testing_labels=train_labels, # 80% here\n        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n    )\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_VAL_TXTS,\n        testing_image_paths=val_paths, testing_labels=val_labels, # 20% here\n        weak_transform=val_tf, strong_transform=None, use_masks=USE_SEGMENTATION\n    )\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP ---\n# --- 10. MODEL INSTANTIATION ---\n# --- 10. MODEL INSTANTIATION ---\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER ---\nmodel = EfficientNetModel().to(device)\n\n# Mutually exclusive parameter grouping\nbackbone_params = []\nhead_params = []\n\nfor name, param in model.named_parameters():\n    if 'backbone' in name:\n        backbone_params.append(param)\n    else:\n        # All other params (classifier, global_pool, etc.) go here\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.1}, # Usually even lower LR for EfficientNet fine-tuning\n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\n# Correctly assign the scheduler function (avoiding the previous return error)\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1, warmup_epochs))\n        progress = float(epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * progress))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\nscaler = torch.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated (EfficientNetV2-S). LR: {LR}, Backbone: Frozen\")\n\n\n\n\n# # Keep your scheduler, scaler, and print statements as they are\n# # scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n# # scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\n# print(f\"\\nModel instantiated (DenseNet201 Only). LR: {LR}, Epochs: {EPOCHS}.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\n# Inside your Epoch Loop in Section 11:\nfor epoch in range(1, EPOCHS + 1):\n    # --- Freeze/Unfreeze Logic ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if 'backbone' in name:\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"\\n--- [Epoch {epoch}] Unfreezing EfficientNet backbone ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n            \n    # The rest of your training and validation code (tqdm loop, history, etc.) follows...\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # Using weak_imgs as the primary input\n        imgs = weak_imgs\n\n        # Apply Mixup or CutMix augmentation\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        # Forward pass with Automatic Mixed Precision\n        with torch.amp.autocast(device_type=device if \"cuda\" in device else \"cpu\", enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary components (kept at 0 for classification focus)\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + (GAMMA_SEG * seg_loss) + (BETA_SUPCON * supcon_loss)\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        # Gradient Accumulation\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # Step the learning rate scheduler\n    scheduler.step()\n\n    # Calculate training metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} F1: {f1:.4f}\")\n\n    # --- VALIDATION PHASE ---\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            \n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    # Calculate validation metrics\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Val-F1: {vf1:.4f}\")\n    history['train_loss'].append(running_loss/max(1,n_batches))\n    history['train_f1'].append(f1)\n    history['val_loss'].append(val_loss/max(1,len(val_loader)))\n    history['val_f1'].append(vf1)\n    # Checkpointing and Early Stopping\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_val_preds = val_y_pred\n        best_val_labels = val_y_true\n        # best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"⭐ New Best Model Saved (Val F1: {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(f\"Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs of no improvement.\")\n            break\n\nprint(f\"\\nTraining complete. Best Val F1: {best_vf1:.4f} at Epoch {best_epoch}\")\n\ndef plot_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss Curve\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss Curve')\n    ax1.legend()\n    \n    # F1 Curve\n    ax2.plot(history['train_f1'], label='Train F1')\n    ax2.plot(history['val_f1'], label='Val F1')\n    ax2.set_title('F1 Score Curve')\n    ax2.legend()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Best Epoch Confusion Matrix')\n    plt.show()\n\n# Call these after training finishes\nplot_history(history)\nplot_confusion_matrix(best_val_labels, best_val_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T11:54:46.510112Z","iopub.execute_input":"2026-01-06T11:54:46.510614Z","execution_failed":"2026-01-06T11:58:48.76Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"mobilenet","metadata":{}},{"cell_type":"code","source":"\nimport os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 20            \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 10\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\nhistory = {\n    'train_loss': [], 'val_loss': [],\n    'train_f1': [], 'val_f1': []\n}\nbest_val_preds = []\nbest_val_labels = []\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\n# class DenseNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         for name, layer in self.features._modules.items():\n#             x = layer(x)\n#             if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n#                 feats.append(x)\n#         return feats\n\n# --- 5. BACKBONE EXTRACTOR (DenseNet169) ---\n# --- 5. UPDATED DENSENET121 EXTRACTOR ---\n\n# --- 5. MOBILENET V3 LARGE EXTRACTOR ---\nclass MobileNetV3Extractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # Using mobilenet_v3_large for high performance/efficiency balance\n        self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n        \n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            # MobileNetV3 Large has 19 blocks in its feature extractor\n            # We extract features at common reduction points (layers 3, 6, 12, 15)\n            if i in (3, 6, 12, 15):\n                feats.append(out)\n        \n        # Ensure the final output is included\n        if len(feats) == 0 or feats[-1].shape != out.shape:\n            feats.append(out)\n        return feats\n\n# --- 6. MOBILENET V3 MODEL ---\nMOBILE_CHANNELS = 960  # Feature dimension for MobileNetV3-Large\nNUM_CLASSES = 2 \n\nclass MobileNetModel(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, out_channels=MOBILE_CHANNELS):\n        super().__init__()\n        \n        self.backbone = MobileNetV3Extractor(pretrained=True)\n        \n        # Freeze backbone initially\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        # Classifier input is now 960\n        self.classifier = nn.Sequential(\n            nn.Linear(out_channels, 512), \n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes) \n        )\n        \n    def forward(self, x):\n        # Feature Extraction\n        mobile_feats = self.backbone(x)[-1]\n        \n        # Global Pooling [Batch, 960, 7, 7] -> [Batch, 960]\n        feat = self.global_pool(mobile_feats).view(x.size(0), -1)\n\n        logits = self.classifier(feat)\n        \n        return {\n            \"logits\": logits, \n            \"feat\": feat,\n            \"seg\": torch.randn(x.size(0), 1, x.size(-2), x.size(-1)).to(x.device)\n        }\n# class MobileNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         out = x\n#         for i, layer in enumerate(self.features):\n#             out = layer(out)\n#             if i in (2,5,9,12):\n#                 feats.append(out)\n#         if len(feats) < 4:\n#             feats.append(out)\n#         return feats\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\n# DENSE_CHANNELS = 1920\n# # CORRECTION: The actual output features are 112, resulting in 2032 total features (1920+112). \n# # We MUST use 112 here to resolve the mat1 and mat2 dimension mismatch error (2032 vs 3200).\n# # MOBILE_CHANNELS = 112 \n# # TOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1920 + 112 = 2032\n# NUM_CLASSES = 2 \n\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n# --- Scenario 1 Changes ---\n# 1. 80/20 Split of testing-dataset remains the same\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n# 2. Training includes basic sets + 80% of testing-dataset\n    ALL_TRAIN_TXTS = [\n        train_cam_txt, train_noncam_txt,\n        train_cam_txt2, train_noncam_txt2, ]\n    # 2. Validation uses NO text files (Consists only of the 20% paths below)\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2,]\n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n        testing_image_paths=train_paths, testing_labels=train_labels, # 80% here\n        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n    )\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_VAL_TXTS,\n        testing_image_paths=val_paths, testing_labels=val_labels, # 20% here\n        weak_transform=val_tf, strong_transform=None, use_masks=USE_SEGMENTATION\n    )\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP ---\n# --- 10. MODEL INSTANTIATION ---\n# --- 10. MODEL INSTANTIATION ---\nmodel = MobileNetModel().to(device)\nfrom torch.optim.lr_scheduler import LambdaLR\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1, warmup_epochs))\n        progress = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * progress))\n    \n    return LambdaLR(optimizer, lr_lambda)\n\n# Adjusted to find 'backbone' instead of 'densenet_base'\nbackbone_params = [p for n, p in model.named_parameters() if 'backbone' in n]\nhead_params = [p for n, p in model.named_parameters() if 'classifier' in n]\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2}, \n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\nscheduler = get_cosine_with_warmup_scheduler(\n    opt,\n    warmup_epochs=WARMUP_EPOCHS,\n    total_epochs=EPOCHS\n)\n\nscaler = torch.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated (MobileNetV3). LR: {LR}, Backbone: Frozen\")\n\n\n\n# # Keep your scheduler, scaler, and print statements as they are\n# # scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n# # scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\n# print(f\"\\nModel instantiated (DenseNet201 Only). LR: {LR}, Epochs: {EPOCHS}.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS + 1):\n    # --- Updated Freeze/Unfreeze Logic for DenseNet ---\n    # --- Inside Section 11 loop ---\n# --- Inside Section 11 loop ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if 'backbone' in name: # Changed from 'densenet_base'\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"\\n--- [Epoch {epoch}] Unfreezing MobileNet backbone ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # Using weak_imgs as the primary input\n        imgs = weak_imgs\n\n        # Apply Mixup or CutMix augmentation\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        # Forward pass with Automatic Mixed Precision\n        with torch.amp.autocast(device_type=device if \"cuda\" in device else \"cpu\", enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary components (kept at 0 for classification focus)\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + (GAMMA_SEG * seg_loss) + (BETA_SUPCON * supcon_loss)\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        # Gradient Accumulation\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # Step the learning rate scheduler\n    scheduler.step()\n\n    # Calculate training metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} F1: {f1:.4f}\")\n\n    # --- VALIDATION PHASE ---\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            \n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    # Calculate validation metrics\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Val-F1: {vf1:.4f}\")\n    history['train_loss'].append(running_loss/max(1,n_batches))\n    history['train_f1'].append(f1)\n    history['val_loss'].append(val_loss/max(1,len(val_loader)))\n    history['val_f1'].append(vf1)\n    # Checkpointing and Early Stopping\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_val_preds = val_y_pred\n        best_val_labels = val_y_true\n        # best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"⭐ New Best Model Saved (Val F1: {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(f\"Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs of no improvement.\")\n            break\n\nprint(f\"\\nTraining complete. Best Val F1: {best_vf1:.4f} at Epoch {best_epoch}\")\n\ndef plot_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss Curve\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss Curve')\n    ax1.legend()\n    \n    # F1 Curve\n    ax2.plot(history['train_f1'], label='Train F1')\n    ax2.plot(history['val_f1'], label='Val F1')\n    ax2.set_title('F1 Score Curve')\n    ax2.legend()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Best Epoch Confusion Matrix')\n    plt.show()\n\n# Call these after training finishes\nplot_history(history)\nplot_confusion_matrix(best_val_labels, best_val_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T17:10:08.141174Z","iopub.execute_input":"2026-01-06T17:10:08.14186Z","iopub.status.idle":"2026-01-06T17:14:50.085306Z","shell.execute_reply.started":"2026-01-06T17:10:08.14183Z","shell.execute_reply":"2026-01-06T17:14:50.083998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"densener121\n","metadata":{}},{"cell_type":"code","source":"\nimport os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 20            \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 10\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\nhistory = {\n    'train_loss': [], 'val_loss': [],\n    'train_f1': [], 'val_f1': []\n}\nbest_val_preds = []\nbest_val_labels = []\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\n# class DenseNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         for name, layer in self.features._modules.items():\n#             x = layer(x)\n#             if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n#                 feats.append(x)\n#         return feats\n\n# --- 5. BACKBONE EXTRACTOR (DenseNet169) ---\n# --- 5. UPDATED DENSENET121 EXTRACTOR ---\nclass DenseNet121Extractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # Using densenet121 instead of densenet201\n        self.features = models.densenet121(weights='IMAGENET1K_V1' if pretrained else None).features\n        \n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            # Keeping the same block-level feature extraction structure\n            if name in [\"denseblock1\", \"denseblock2\", \"denseblock3\", \"denseblock4\"]:\n                feats.append(x)\n        return feats\n\n# --- 6. DENSENET121 ONLY MODEL ---\nDENSE121_CHANNELS = 1024  # DenseNet121 final feature dimension\nNUM_CLASSES = 2 \n\nclass DenseNet121Model(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, out_channels=DENSE121_CHANNELS):\n        super().__init__()\n        \n        self.densenet_base = DenseNet121Extractor(pretrained=True)\n        \n        # Initial Freeze logic\n        for param in self.densenet_base.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        # Classifier input is now 1024\n        self.classifier = nn.Sequential(\n            nn.Linear(out_channels, 512), \n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes) \n        )\n        \n        self.use_seg = False \n        self.domain_head = nn.Identity()\n        \n    def forward(self, x, grl_lambda=0.0):\n        # Feature Extraction\n        densenet_feats = self.densenet_base(x)[-1]\n        \n        # Pooling [Batch, 1024, 7, 7] -> [Batch, 1024]\n        feat = self.global_pool(densenet_feats).view(x.size(0), -1)\n\n        logits = self.classifier(feat)\n        \n        return {\n            \"logits\": logits, \n            \"feat\": feat,\n            \"domain_logits\": torch.randn(x.size(0), 2).to(x.device), \n            \"seg\": torch.randn(x.size(0), 1, x.size(-2), x.size(-1)).to(x.device)\n        }\n# class MobileNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         out = x\n#         for i, layer in enumerate(self.features):\n#             out = layer(out)\n#             if i in (2,5,9,12):\n#                 feats.append(out)\n#         if len(feats) < 4:\n#             feats.append(out)\n#         return feats\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\n# DENSE_CHANNELS = 1920\n# # CORRECTION: The actual output features are 112, resulting in 2032 total features (1920+112). \n# # We MUST use 112 here to resolve the mat1 and mat2 dimension mismatch error (2032 vs 3200).\n# # MOBILE_CHANNELS = 112 \n# # TOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1920 + 112 = 2032\n# NUM_CLASSES = 2 \n\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n# --- Scenario 1 Changes ---\n# 1. 80/20 Split of testing-dataset remains the same\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n# 2. Training includes basic sets + 80% of testing-dataset\n    ALL_TRAIN_TXTS = [\n        train_cam_txt, train_noncam_txt,\n        train_cam_txt2, train_noncam_txt2, ]\n    # 2. Validation uses NO text files (Consists only of the 20% paths below)\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2,]\n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n        testing_image_paths=train_paths, testing_labels=train_labels, # 80% here\n        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n    )\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_VAL_TXTS,\n        testing_image_paths=val_paths, testing_labels=val_labels, # 20% here\n        weak_transform=val_tf, strong_transform=None, use_masks=USE_SEGMENTATION\n    )\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP ---\n# --- 10. MODEL INSTANTIATION ---\nmodel = DenseNet121Model().to(device)\nprint(f\"\\nModel instantiated (DenseNet121). LR: {LR}, Backbone: Frozen\")\nfrom torch.optim.lr_scheduler import LambdaLR\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1, warmup_epochs))\n        progress = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * progress))\n    \n    return LambdaLR(optimizer, lr_lambda)\n\nbackbone_params = [p for n, p in model.named_parameters() if 'densenet_base' in n]\nhead_params = [p for n, p in model.named_parameters() if 'classifier' in n]\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2}, \n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\n# Re-include the scheduler and scaler (ensure the function is defined)\nscheduler = get_cosine_with_warmup_scheduler(\n    opt,\n    warmup_epochs=WARMUP_EPOCHS,\n    total_epochs=EPOCHS\n)\n\nscaler = torch.amp.GradScaler(enabled=(device==\"cuda\"))\n\n\n# Keep your scheduler, scaler, and print statements as they are\n# scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n# scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated (DenseNet201 Only). LR: {LR}, Epochs: {EPOCHS}.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS + 1):\n    # --- Updated Freeze/Unfreeze Logic for DenseNet ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if 'densenet_base' in name:\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"\\n--- [Epoch {epoch}] Unfreezing DenseNet backbone for fine-tuning ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # Using weak_imgs as the primary input\n        imgs = weak_imgs\n\n        # Apply Mixup or CutMix augmentation\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        # Forward pass with Automatic Mixed Precision\n        with torch.amp.autocast(device_type=device if \"cuda\" in device else \"cpu\", enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary components (kept at 0 for classification focus)\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + (GAMMA_SEG * seg_loss) + (BETA_SUPCON * supcon_loss)\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        # Gradient Accumulation\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # Step the learning rate scheduler\n    scheduler.step()\n\n    # Calculate training metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} F1: {f1:.4f}\")\n\n    # --- VALIDATION PHASE ---\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            \n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    # Calculate validation metrics\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Val-F1: {vf1:.4f}\")\n    history['train_loss'].append(running_loss/max(1,n_batches))\n    history['train_f1'].append(f1)\n    history['val_loss'].append(val_loss/max(1,len(val_loader)))\n    history['val_f1'].append(vf1)\n    # Checkpointing and Early Stopping\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_val_preds = val_y_pred\n        best_val_labels = val_y_true\n        # best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"⭐ New Best Model Saved (Val F1: {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(f\"Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs of no improvement.\")\n            break\n\nprint(f\"\\nTraining complete. Best Val F1: {best_vf1:.4f} at Epoch {best_epoch}\")\n\ndef plot_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss Curve\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss Curve')\n    ax1.legend()\n    \n    # F1 Curve\n    ax2.plot(history['train_f1'], label='Train F1')\n    ax2.plot(history['val_f1'], label='Val F1')\n    ax2.set_title('F1 Score Curve')\n    ax2.legend()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Best Epoch Confusion Matrix')\n    plt.show()\n\n# Call these after training finishes\nplot_history(history)\nplot_confusion_matrix(best_val_labels, best_val_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T17:20:02.275628Z","iopub.execute_input":"2026-01-06T17:20:02.276451Z","iopub.status.idle":"2026-01-06T17:24:20.654921Z","shell.execute_reply.started":"2026-01-06T17:20:02.2764Z","shell.execute_reply":"2026-01-06T17:24:20.653295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"densenet169","metadata":{}},{"cell_type":"markdown","source":"Inception V3","metadata":{}},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 20            \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 10\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\n\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\n# class DenseNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         for name, layer in self.features._modules.items():\n#             x = layer(x)\n#             if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n#                 feats.append(x)\n#         return feats\n# --- 1. Update Dimensions ---\n# --- 5. INCEPTION EXTRACTOR ---\n # Required for InceptionV3\n# --- 5. INCEPTION EXTRACTOR (Corrected) ---\nclass InceptionV3Extractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # InceptionV3 requires aux_logits=True when using pretrained weights\n        self.base = models.inception_v3(weights='IMAGENET1K_V1' if pretrained else None, aux_logits=True)\n        \n    def forward(self, x):\n        # Handle the specific layers of InceptionV3\n        # We manually pass through the layers to get the final feature map\n        x = self.base.Conv2d_1a_3x3(x)\n        x = self.base.Conv2d_2a_3x3(x)\n        x = self.base.Conv2d_2b_3x3(x)\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        x = self.base.Conv2d_3b_1x1(x)\n        x = self.base.Conv2d_4a_3x3(x)\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        x = self.base.Mixed_5b(x)\n        x = self.base.Mixed_5c(x)\n        x = self.base.Mixed_5d(x)\n        x = self.base.Mixed_6a(x)\n        x = self.base.Mixed_6b(x)\n        x = self.base.Mixed_6c(x)\n        x = self.base.Mixed_6d(x)\n        x = self.base.Mixed_6e(x)\n        x = self.base.Mixed_7a(x)\n        x = self.base.Mixed_7b(x)\n        x = self.base.Mixed_7c(x)\n        return [x]\n\n# --- 6. INCEPTION MODEL ---\nIMG_SIZE = 299 # Required for InceptionV3\nINCEPTION_CHANNELS = 2048 \n\nclass InceptionOnlyModel(nn.Module):\n    def __init__(self, num_classes=2):\n        super().__init__()\n        self.backbone = InceptionV3Extractor(pretrained=True)\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(\n            nn.Linear(INCEPTION_CHANNELS, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n        \n    def forward(self, x):\n        feats = self.backbone(x)[-1]\n        feat = self.global_pool(feats).view(x.size(0), -1)\n        logits = self.classifier(feat)\n        return {\"logits\": logits, \"seg\": torch.randn(x.size(0), 1, 299, 299).to(x.device)}\n# class MobileNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         out = x\n#         for i, layer in enumerate(self.features):\n#             out = layer(out)\n#             if i in (2,5,9,12):\n#                 feats.append(out)\n#         if len(feats) < 4:\n#             feats.append(out)\n#         return feats\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\n# DENSE_CHANNELS = 1920\n# # CORRECTION: The actual output features are 112, resulting in 2032 total features (1920+112). \n# # We MUST use 112 here to resolve the mat1 and mat2 dimension mismatch error (2032 vs 3200).\n# # MOBILE_CHANNELS = 112 \n# # TOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1920 + 112 = 2032\n# NUM_CLASSES = 2 \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Initialize history\nhistory = {\n    'train_loss': [], 'val_loss': [],\n    'train_f1': [], 'val_f1': []\n}\nbest_val_preds = []\nbest_val_labels = []\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n# --- Scenario 1 Changes ---\n# 1. 80/20 Split of testing-dataset remains the same\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n# 2. Training includes basic sets + 80% of testing-dataset\n    ALL_TRAIN_TXTS = [\n        train_cam_txt, train_noncam_txt,\n        train_cam_txt2, train_noncam_txt2, ]\n    # 2. Validation uses NO text files (Consists only of the 20% paths below)\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2,]\n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n        testing_image_paths=train_paths, testing_labels=train_labels, # 80% here\n        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n    )\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_VAL_TXTS,\n        testing_image_paths=val_paths, testing_labels=val_labels, # 20% here\n        weak_transform=val_tf, strong_transform=None, use_masks=USE_SEGMENTATION\n    )\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP ---\n# --- 10. MODEL INSTANTIATION ---\nmodel = InceptionOnlyModel().to(device)\nfrom torch.optim.lr_scheduler import LambdaLR\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1, warmup_epochs))\n        progress = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * progress))\n    \n    return LambdaLR(optimizer, lr_lambda)\n\nbackbone_params = [p for n, p in model.named_parameters() if 'backbone' in n]\nhead_params = [p for n, p in model.named_parameters() if 'classifier' in n]\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2},\n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\n# This now works because the function is defined above\nscheduler = get_cosine_with_warmup_scheduler(\n    opt,\n    warmup_epochs=WARMUP_EPOCHS,\n    total_epochs=EPOCHS\n)\n\nscaler = torch.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated (InceptionV3). Image Size: {IMG_SIZE}\")\n\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS + 1):\n    # --- Updated Freeze/Unfreeze Logic for DenseNet ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if 'densenet_base' in name:\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"\\n--- [Epoch {epoch}] Unfreezing DenseNet backbone for fine-tuning ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # Using weak_imgs as the primary input\n        imgs = weak_imgs\n\n        # Apply Mixup or CutMix augmentation\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        # Forward pass with Automatic Mixed Precision\n        with torch.amp.autocast(device_type=device if \"cuda\" in device else \"cpu\", enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary components (kept at 0 for classification focus)\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + (GAMMA_SEG * seg_loss) + (BETA_SUPCON * supcon_loss)\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        # Gradient Accumulation\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # Step the learning rate scheduler\n    scheduler.step()\n\n    # Calculate training metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} F1: {f1:.4f}\")\n\n    # --- VALIDATION PHASE ---\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            \n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    # Calculate validation metrics\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Val-F1: {vf1:.4f}\")\n    history['train_loss'].append(running_loss/max(1,n_batches))\n    history['train_f1'].append(f1)\n    history['val_loss'].append(val_loss/max(1,len(val_loader)))\n    history['val_f1'].append(vf1)\n    # Checkpointing and Early Stopping\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_val_preds = val_y_pred\n        best_val_labels = val_y_true\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"⭐ New Best Model Saved (Val F1: {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(f\"Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs of no improvement.\")\n            break\n\nprint(f\"\\nTraining complete. Best Val F1: {best_vf1:.4f} at Epoch {best_epoch}\")\n\ndef plot_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss Curve\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss Curve')\n    ax1.legend()\n    \n    # F1 Curve\n    ax2.plot(history['train_f1'], label='Train F1')\n    ax2.plot(history['val_f1'], label='Val F1')\n    ax2.set_title('F1 Score Curve')\n    ax2.legend()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Best Epoch Confusion Matrix')\n    plt.show()\n\n# Call these after training finishes\nplot_history(history)\nplot_confusion_matrix(best_val_labels, best_val_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T17:37:19.504981Z","iopub.execute_input":"2026-01-06T17:37:19.505395Z","iopub.status.idle":"2026-01-06T17:40:53.21338Z","shell.execute_reply.started":"2026-01-06T17:37:19.505357Z","shell.execute_reply":"2026-01-06T17:40:53.212328Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Densenet201","metadata":{}},{"cell_type":"code","source":"\nimport os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 20            \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 10\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\nhistory = {\n    'train_loss': [], 'val_loss': [],\n    'train_f1': [], 'val_f1': []\n}\nbest_val_preds = []\nbest_val_labels = []\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\n# class DenseNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         for name, layer in self.features._modules.items():\n#             x = layer(x)\n#             if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n#                 feats.append(x)\n#         return feats\n# --- 1. Update Dimensions ---\nDENSE_CHANNELS = 1920  # The output of DenseNet201's last features layer\nNUM_CLASSES = 2 \nclass DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n# --- 2. DenseNet Only Model ---\nclass DenseNetOnly(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, dense_out_channels=DENSE_CHANNELS):\n        super().__init__()\n        \n        # Keep only DenseNet\n        self.densenet_base = DenseNetExtractor(pretrained=True)\n        \n        # Freeze DenseNet initially\n        for param in self.densenet_base.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        # Classifier now takes ONLY DenseNet features (1920)\n        self.classifier = nn.Sequential(\n            nn.Linear(dense_out_channels, 512), \n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes) \n        )\n        \n        # Keep these for training loop compatibility\n        self.use_seg = False \n        self.domain_head = nn.Identity()\n        \n    def forward(self, x, grl_lambda=0.0):\n        # 1. Feature Extraction\n        densenet_feats = self.densenet_base(x)[-1]\n        \n        # 2. Global Pooling\n        # Shape change: [Batch, 1920, 7, 7] -> [Batch, 1920]\n        feat = self.global_pool(densenet_feats).view(x.size(0), -1)\n\n        # 3. Classification\n        logits = self.classifier(feat)\n        \n        # Return same dictionary structure so Training Loop doesn't break\n        return {\n            \"logits\": logits, \n            \"feat\": feat,\n            \"domain_logits\": torch.randn(x.size(0), 2).to(x.device), \n            \"seg\": torch.randn(x.size(0), 1, x.size(-2), x.size(-1)).to(x.device)\n        }\n\n# class MobileNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         out = x\n#         for i, layer in enumerate(self.features):\n#             out = layer(out)\n#             if i in (2,5,9,12):\n#                 feats.append(out)\n#         if len(feats) < 4:\n#             feats.append(out)\n#         return feats\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\nDENSE_CHANNELS = 1920\n# CORRECTION: The actual output features are 112, resulting in 2032 total features (1920+112). \n# We MUST use 112 here to resolve the mat1 and mat2 dimension mismatch error (2032 vs 3200).\n# MOBILE_CHANNELS = 112 \n# TOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1920 + 112 = 2032\nNUM_CLASSES = 2 \n\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n# --- Scenario 1 Changes ---\n# 1. 80/20 Split of testing-dataset remains the same\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n# 2. Training includes basic sets + 80% of testing-dataset\n    ALL_TRAIN_TXTS = [\n        train_cam_txt, train_noncam_txt,\n        train_cam_txt2, train_noncam_txt2, ]\n    # 2. Validation uses NO text files (Consists only of the 20% paths below)\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2,]\n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n        testing_image_paths=train_paths, testing_labels=train_labels, # 80% here\n        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n    )\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_VAL_TXTS,\n        testing_image_paths=val_paths, testing_labels=val_labels, # 20% here\n        weak_transform=val_tf, strong_transform=None, use_masks=USE_SEGMENTATION\n    )\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP ---\nmodel = DenseNetOnly().to(device)\n\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    # Only search for densenet_base; mobilenet_base is gone\n    if 'densenet_base' in name:\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2}, \n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\nfrom torch.optim.lr_scheduler import LambdaLR\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1, warmup_epochs))\n        progress = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * progress))\n    \n    return LambdaLR(optimizer, lr_lambda)\n\n# Keep your scheduler, scaler, and print statements as they are\nscheduler = get_cosine_with_warmup_scheduler(\n    opt,\n    warmup_epochs=WARMUP_EPOCHS,\n    total_epochs=EPOCHS\n)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated (DenseNet201 Only). LR: {LR}, Epochs: {EPOCHS}.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS + 1):\n    # --- Updated Freeze/Unfreeze Logic for DenseNet ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if 'densenet_base' in name:\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"\\n--- [Epoch {epoch}] Unfreezing DenseNet backbone for fine-tuning ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # Using weak_imgs as the primary input\n        imgs = weak_imgs\n\n        # Apply Mixup or CutMix augmentation\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        # Forward pass with Automatic Mixed Precision\n        with torch.amp.autocast(device_type=device if \"cuda\" in device else \"cpu\", enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary components (kept at 0 for classification focus)\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + (GAMMA_SEG * seg_loss) + (BETA_SUPCON * supcon_loss)\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        # Gradient Accumulation\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # Step the learning rate scheduler\n    scheduler.step()\n\n    # Calculate training metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} F1: {f1:.4f}\")\n\n    # --- VALIDATION PHASE ---\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            \n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    # Calculate validation metrics\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Val-F1: {vf1:.4f}\")\n    history['train_loss'].append(running_loss/max(1,n_batches))\n    history['train_f1'].append(f1)\n    history['val_loss'].append(val_loss/max(1,len(val_loader)))\n    history['val_f1'].append(vf1)\n    # Checkpointing and Early Stopping\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_val_preds = val_y_pred\n        best_val_labels = val_y_true\n        # best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"⭐ New Best Model Saved (Val F1: {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(f\"Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs of no improvement.\")\n            break\n\nprint(f\"\\nTraining complete. Best Val F1: {best_vf1:.4f} at Epoch {best_epoch}\")\n\ndef plot_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss Curve\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss Curve')\n    ax1.legend()\n    \n    # F1 Curve\n    ax2.plot(history['train_f1'], label='Train F1')\n    ax2.plot(history['val_f1'], label='Val F1')\n    ax2.set_title('F1 Score Curve')\n    ax2.legend()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Best Epoch Confusion Matrix')\n    plt.show()\n\n# Call these after training finishes\nplot_history(history)\nplot_confusion_matrix(best_val_labels, best_val_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T17:42:11.16443Z","iopub.execute_input":"2026-01-06T17:42:11.165135Z","iopub.status.idle":"2026-01-06T17:45:42.05055Z","shell.execute_reply.started":"2026-01-06T17:42:11.1651Z","shell.execute_reply":"2026-01-06T17:45:42.049441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 20            \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 10\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\nhistory = {\n    'train_loss': [], 'val_loss': [],\n    'train_f1': [], 'val_f1': []\n}\nbest_val_preds = []\nbest_val_labels = []\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\n# class DenseNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         for name, layer in self.features._modules.items():\n#             x = layer(x)\n#             if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n#                 feats.append(x)\n#         return feats\n\n# --- 5. BACKBONE EXTRACTOR (DenseNet169) ---\nclass DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # Changed from densenet201 to densenet169\n        self.features = models.densenet169(weights='IMAGENET1K_V1' if pretrained else None).features\n        \n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            # Intermediate features are still valuable for multi-scale logic\n            if name in [\"denseblock1\", \"denseblock2\", \"denseblock3\", \"denseblock4\"]:\n                feats.append(x)\n        return feats\n# --- 2. DenseNet Only Model ---\n# --- 6. MODEL DIMENSIONS ---\nDENSE_CHANNELS = 1664  # Updated for DenseNet169\nNUM_CLASSES = 2 \n\nclass DenseNetOnly(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, dense_out_channels=DENSE_CHANNELS):\n        super().__init__()\n        self.densenet_base = DenseNetExtractor(pretrained=True)\n        \n        for param in self.densenet_base.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        # Classifier input is now 1664\n        self.classifier = nn.Sequential(\n            nn.Linear(dense_out_channels, 512), \n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes) \n        )\n        \n        self.use_seg = False \n        self.domain_head = nn.Identity()\n        \n    def forward(self, x, grl_lambda=0.0):\n        densenet_feats = self.densenet_base(x)[-1]\n        feat = self.global_pool(densenet_feats).view(x.size(0), -1)\n        logits = self.classifier(feat)\n        \n        return {\n            \"logits\": logits, \n            \"feat\": feat,\n            \"domain_logits\": torch.randn(x.size(0), 2).to(x.device), \n            \"seg\": torch.randn(x.size(0), 1, x.size(-2), x.size(-1)).to(x.device)\n        }\n\n# class MobileNetExtractor(nn.Module):\n#     def __init__(self, pretrained=True):\n#         super().__init__()\n#         self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n#     def forward(self, x):\n#         feats = []\n#         out = x\n#         for i, layer in enumerate(self.features):\n#             out = layer(out)\n#             if i in (2,5,9,12):\n#                 feats.append(out)\n#         if len(feats) < 4:\n#             feats.append(out)\n#         return feats\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\nDENSE_CHANNELS = 1920\n# CORRECTION: The actual output features are 112, resulting in 2032 total features (1920+112). \n# We MUST use 112 here to resolve the mat1 and mat2 dimension mismatch error (2032 vs 3200).\n# MOBILE_CHANNELS = 112 \n# TOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1920 + 112 = 2032\nNUM_CLASSES = 2 \n\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n# --- Scenario 1 Changes ---\n# 1. 80/20 Split of testing-dataset remains the same\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n# 2. Training includes basic sets + 80% of testing-dataset\n    ALL_TRAIN_TXTS = [\n        train_cam_txt, train_noncam_txt,\n        train_cam_txt2, train_noncam_txt2, ]\n    # 2. Validation uses NO text files (Consists only of the 20% paths below)\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2,]\n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n        testing_image_paths=train_paths, testing_labels=train_labels, # 80% here\n        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n    )\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_VAL_TXTS,\n        testing_image_paths=val_paths, testing_labels=val_labels, # 20% here\n        weak_transform=val_tf, strong_transform=None, use_masks=USE_SEGMENTATION\n    )\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP ---\nmodel = DenseNetOnly().to(device)\n\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    # Only search for densenet_base; mobilenet_base is gone\n    if 'densenet_base' in name:\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2}, \n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\nfrom torch.optim.lr_scheduler import LambdaLR\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1, warmup_epochs))\n        progress = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * progress))\n    \n    return LambdaLR(optimizer, lr_lambda)\n\n# Keep your scheduler, scaler, and print statements as they are\nscheduler = get_cosine_with_warmup_scheduler(\n    opt,\n    warmup_epochs=WARMUP_EPOCHS,\n    total_epochs=EPOCHS\n)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated (DenseNet201 Only). LR: {LR}, Epochs: {EPOCHS}.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS + 1):\n    # --- Updated Freeze/Unfreeze Logic for DenseNet ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if 'densenet_base' in name:\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"\\n--- [Epoch {epoch}] Unfreezing DenseNet backbone for fine-tuning ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # Using weak_imgs as the primary input\n        imgs = weak_imgs\n\n        # Apply Mixup or CutMix augmentation\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        # Forward pass with Automatic Mixed Precision\n        with torch.amp.autocast(device_type=device if \"cuda\" in device else \"cpu\", enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary components (kept at 0 for classification focus)\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + (GAMMA_SEG * seg_loss) + (BETA_SUPCON * supcon_loss)\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        # Gradient Accumulation\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # Step the learning rate scheduler\n    scheduler.step()\n\n    # Calculate training metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} F1: {f1:.4f}\")\n\n    # --- VALIDATION PHASE ---\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            \n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    # Calculate validation metrics\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Val-F1: {vf1:.4f}\")\n    history['train_loss'].append(running_loss/max(1,n_batches))\n    history['train_f1'].append(f1)\n    history['val_loss'].append(val_loss/max(1,len(val_loader)))\n    history['val_f1'].append(vf1)\n    # Checkpointing and Early Stopping\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_val_preds = val_y_pred\n        best_val_labels = val_y_true\n        # best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"⭐ New Best Model Saved (Val F1: {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(f\"Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs of no improvement.\")\n            break\n\nprint(f\"\\nTraining complete. Best Val F1: {best_vf1:.4f} at Epoch {best_epoch}\")\n\ndef plot_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss Curve\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss Curve')\n    ax1.legend()\n    \n    # F1 Curve\n    ax2.plot(history['train_f1'], label='Train F1')\n    ax2.plot(history['val_f1'], label='Val F1')\n    ax2.set_title('F1 Score Curve')\n    ax2.legend()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Best Epoch Confusion Matrix')\n    plt.show()\n\n# Call these after training finishes\nplot_history(history)\nplot_confusion_matrix(best_val_labels, best_val_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T17:45:52.375522Z","iopub.execute_input":"2026-01-06T17:45:52.376576Z","iopub.status.idle":"2026-01-06T17:49:11.290819Z","shell.execute_reply.started":"2026-01-06T17:45:52.376525Z","shell.execute_reply":"2026-01-06T17:49:11.289272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}