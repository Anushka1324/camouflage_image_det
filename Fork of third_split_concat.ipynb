{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b959814f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:52:16.350135Z",
     "iopub.status.busy": "2026-01-08T06:52:16.349552Z",
     "iopub.status.idle": "2026-01-08T06:52:17.098122Z",
     "shell.execute_reply": "2026-01-08T06:52:17.097548Z"
    },
    "papermill": {
     "duration": 0.756076,
     "end_time": "2026-01-08T06:52:17.099825",
     "exception": false,
     "start_time": "2026-01-08T06:52:16.343749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456dad09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T06:52:17.108872Z",
     "iopub.status.busy": "2026-01-08T06:52:17.108552Z",
     "iopub.status.idle": "2026-01-08T07:35:17.116601Z",
     "shell.execute_reply": "2026-01-08T07:35:17.115631Z"
    },
    "papermill": {
     "duration": 2580.585433,
     "end_time": "2026-01-08T07:35:17.688968",
     "exception": false,
     "start_time": "2026-01-08T06:52:17.103535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "✅ Loaded 8605 samples from 4 root directories.\n",
      "✅ Loaded 2152 samples from 4 root directories.\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77.4M/77.4M [00:00<00:00, 174MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 154MB/s]\n",
      "/tmp/ipykernel_24/1442150897.py:569: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model instantiated. LR: 0.0003, Epochs: 15.\n",
      "Backbones are initially frozen for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/15: 100%|██████████| 1076/1076 [01:41<00:00, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.6930 Acc: 0.5069 Prec: 0.5094 Rec: 0.5041 F1: 0.4239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val Loss: 1.5834 Acc: 0.3917 Prec: 0.1959 Rec: 0.5000 F1: 0.2815\n",
      "Saved best model at epoch 1 (F1 0.2815)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/15: 100%|██████████| 1076/1076 [01:34<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.5399 Acc: 0.7626 Prec: 0.7659 Rec: 0.7616 F1: 0.7614\n",
      "[Epoch 2] Val Loss: 1.3412 Acc: 0.9572 Prec: 0.9512 Rec: 0.9625 F1: 0.9558\n",
      "Saved best model at epoch 2 (F1 0.9558)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/15: 100%|██████████| 1076/1076 [01:33<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.4726 Acc: 0.8099 Prec: 0.8099 Rec: 0.8099 F1: 0.8099\n",
      "[Epoch 3] Val Loss: 1.3550 Acc: 0.9586 Prec: 0.9536 Rec: 0.9614 F1: 0.9570\n",
      "Saved best model at epoch 3 (F1 0.9570)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/15: 100%|██████████| 1076/1076 [01:33<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.4661 Acc: 0.8092 Prec: 0.8092 Rec: 0.8091 F1: 0.8091\n",
      "[Epoch 4] Val Loss: 1.3739 Acc: 0.9633 Prec: 0.9575 Rec: 0.9686 F1: 0.9620\n",
      "Saved best model at epoch 4 (F1 0.9620)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5/15: 100%|██████████| 1076/1076 [01:32<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.4670 Acc: 0.8123 Prec: 0.8123 Rec: 0.8123 F1: 0.8123\n",
      "[Epoch 5] Val Loss: 1.3442 Acc: 0.9689 Prec: 0.9634 Rec: 0.9738 F1: 0.9677\n",
      "Saved best model at epoch 5 (F1 0.9677)\n",
      "--- Unfreezing all backbone layers at epoch 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6/15: 100%|██████████| 1076/1076 [03:16<00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.4449 Acc: 0.8212 Prec: 0.8211 Rec: 0.8211 F1: 0.8211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Val Loss: 1.3147 Acc: 0.9814 Prec: 0.9780 Rec: 0.9837 F1: 0.9806\n",
      "Saved best model at epoch 6 (F1 0.9806)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7/15: 100%|██████████| 1076/1076 [03:01<00:00,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.4329 Acc: 0.8306 Prec: 0.8305 Rec: 0.8305 F1: 0.8305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Val Loss: 1.2976 Acc: 0.9814 Prec: 0.9776 Rec: 0.9843 F1: 0.9806\n",
      "Saved best model at epoch 7 (F1 0.9806)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8/15: 100%|██████████| 1076/1076 [03:00<00:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.4275 Acc: 0.8460 Prec: 0.8460 Rec: 0.8460 F1: 0.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Val Loss: 1.2932 Acc: 0.9842 Prec: 0.9808 Rec: 0.9868 F1: 0.9835\n",
      "Saved best model at epoch 8 (F1 0.9835)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9/15: 100%|██████████| 1076/1076 [03:01<00:00,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.4242 Acc: 0.8492 Prec: 0.8491 Rec: 0.8491 F1: 0.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Val Loss: 1.3021 Acc: 0.9782 Prec: 0.9736 Rec: 0.9820 F1: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10/15: 100%|██████████| 1076/1076 [03:00<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.4250 Acc: 0.8429 Prec: 0.8429 Rec: 0.8429 F1: 0.8429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Val Loss: 1.3025 Acc: 0.9796 Prec: 0.9753 Rec: 0.9830 F1: 0.9787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11/15: 100%|██████████| 1076/1076 [02:59<00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.4181 Acc: 0.8515 Prec: 0.8514 Rec: 0.8514 F1: 0.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Val Loss: 1.2674 Acc: 0.9875 Prec: 0.9845 Rec: 0.9897 F1: 0.9869\n",
      "Saved best model at epoch 11 (F1 0.9869)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 12/15: 100%|██████████| 1076/1076 [02:59<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.4233 Acc: 0.8482 Prec: 0.8482 Rec: 0.8482 F1: 0.8482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Val Loss: 1.2537 Acc: 0.9870 Prec: 0.9841 Rec: 0.9891 F1: 0.9864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 13/15: 100%|██████████| 1076/1076 [03:00<00:00,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.4148 Acc: 0.8558 Prec: 0.8558 Rec: 0.8558 F1: 0.8558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Val Loss: 1.2731 Acc: 0.9870 Prec: 0.9839 Rec: 0.9893 F1: 0.9864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 14/15: 100%|██████████| 1076/1076 [02:59<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.4195 Acc: 0.8628 Prec: 0.8628 Rec: 0.8628 F1: 0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Val Loss: 1.2593 Acc: 0.9865 Prec: 0.9834 Rec: 0.9889 F1: 0.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 15/15: 100%|██████████| 1076/1076 [03:00<00:00,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.4246 Acc: 0.8454 Prec: 0.8454 Rec: 0.8454 F1: 0.8454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Val Loss: 1.2599 Acc: 0.9865 Prec: 0.9834 Rec: 0.9889 F1: 0.9859\n",
      "\n",
      "Training finished. Best val F1: 0.9869087458202683 at epoch 11\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import RandAugment\n",
    "import timm \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "from torch.autograd import Function\n",
    "\n",
    "# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8         \n",
    "EPOCHS = 15          \n",
    "NUM_WORKERS = 4         \n",
    "LR = 3e-4              \n",
    "LABEL_SMOOTH = 0.1\n",
    "SAVE_PATH = \"best_model.pth\"\n",
    "USE_SEGMENTATION = True \n",
    "\n",
    "# Loss weights \n",
    "ALPHA_DOM = 0.5\n",
    "BETA_SUPCON = 0.3\n",
    "ETA_CONS = 0.1\n",
    "GAMMA_SEG = 0.5\n",
    "\n",
    "# Mixup/CutMix probabilities and alphas\n",
    "PROB_MIXUP = 0.5\n",
    "PROB_CUTMIX = 0.5\n",
    "MIXUP_ALPHA = 0.2\n",
    "CUTMIX_ALPHA = 1.0\n",
    "\n",
    "# Warmup epochs and accumulation steps\n",
    "WARMUP_EPOCHS = 5\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "FREEZE_EPOCHS = 5\n",
    "ACCUMULATION_STEPS = 4\n",
    "\n",
    "# --- 2. TRANSFORMS (From Notebook Cell 6) ---\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.05):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        noisy_tensor = tensor + noise\n",
    "        return torch.clamp(noisy_tensor, 0., 1.)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
    "\n",
    "weak_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.02),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "strong_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.05),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\n",
    "def read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n",
    "\n",
    "def load_testing_dataset_info(info_file, image_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n",
    "    lines = []\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(info_file, 'r', encoding=encoding) as f:\n",
    "                lines = f.readlines()\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            image_filename = parts[0]\n",
    "            try:\n",
    "                label = int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            label = 1 if label == 1 else 0\n",
    "            image_full_path = os.path.join(image_dir, image_filename)\n",
    "            image_paths.append(image_full_path)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n",
    "        self.root_dirs = root_dirs\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "        self.use_masks = use_masks\n",
    "        self.samples = []\n",
    "\n",
    "        if testing_image_paths is not None and testing_labels is not None:\n",
    "            for img_path, label in zip(testing_image_paths, testing_labels):\n",
    "                self.samples.append((img_path, label)) \n",
    "        \n",
    "        if isinstance(txt_files, str):\n",
    "            txt_files = [txt_files]\n",
    "\n",
    "        all_lines = []\n",
    "        for t in txt_files:\n",
    "            if not os.path.exists(t):\n",
    "                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n",
    "\n",
    "            lines = read_file_with_encoding(t)\n",
    "            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n",
    "\n",
    "        for line, src_txt in all_lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 0:\n",
    "                continue\n",
    "\n",
    "            fname = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    lbl = int(parts[1])\n",
    "                except:\n",
    "                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            else:\n",
    "                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            \n",
    "            lbl = 1 if lbl == 1 else 0\n",
    "            base_fname = os.path.basename(fname)  \n",
    "\n",
    "            found = False\n",
    "            search_subs = [\n",
    "                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n",
    "                \"Images/Train\", \"Images/Test\",\n",
    "            ]\n",
    "            \n",
    "            for rdir in self.root_dirs:\n",
    "                for sub in search_subs:\n",
    "                    img_path = os.path.join(rdir, sub, base_fname)\n",
    "                    if os.path.exists(img_path):\n",
    "                        self.samples.append((img_path, lbl, rdir))\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global IMG_SIZE \n",
    "        \n",
    "        sample = self.samples[idx]\n",
    "        img_path = sample[0]\n",
    "        lbl = sample[1]\n",
    "        \n",
    "        if len(sample) == 3:\n",
    "            rdir = sample[2]\n",
    "        else:\n",
    "            rdir = os.path.dirname(os.path.dirname(img_path))\n",
    "            \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n",
    "            \n",
    "        if self.weak_transform:\n",
    "            weak = self.weak_transform(img)\n",
    "        else:\n",
    "            weak = transforms.ToTensor()(img)\n",
    "            \n",
    "        if self.strong_transform:\n",
    "            strong = self.strong_transform(img)\n",
    "        else:\n",
    "            strong = weak.clone()\n",
    "\n",
    "        mask = None\n",
    "        if self.use_masks:\n",
    "            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n",
    "            found_mask = False\n",
    "            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n",
    "                mask_path = os.path.join(rdir, mask_dir, mask_name)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n",
    "                    m = np.array(m).astype(np.float32) / 255.0\n",
    "                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n",
    "                    found_mask = True\n",
    "                    break\n",
    "\n",
    "            if mask is None:\n",
    "                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n",
    "                \n",
    "        return weak, strong, lbl, mask\n",
    "\n",
    "# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n",
    "\n",
    "class DenseNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n",
    "                feats.append(x)\n",
    "        return feats\n",
    "\n",
    "class MobileNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        out = x\n",
    "        for i, layer in enumerate(self.features):\n",
    "            out = layer(out)\n",
    "            if i in (2,5,9,12):\n",
    "                feats.append(out)\n",
    "        if len(feats) < 4:\n",
    "            feats.append(out)\n",
    "        return feats\n",
    "\n",
    "# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n",
    "\n",
    "DENSE_CHANNELS = 1920\n",
    "# CORRECTION: The actual output features are 112, resulting in 2032 total features (1920+112). \n",
    "# We MUST use 112 here to resolve the mat1 and mat2 dimension mismatch error (2032 vs 3200).\n",
    "MOBILE_CHANNELS = 112 \n",
    "TOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1920 + 112 = 2032\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "class KerasStyleFusion(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, dense_out_channels=DENSE_CHANNELS, mobile_out_channels=MOBILE_CHANNELS):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.densenet_base = DenseNetExtractor(pretrained=True)\n",
    "        self.mobilenet_base = MobileNetExtractor(pretrained=True)\n",
    "        \n",
    "        for param in self.densenet_base.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.mobilenet_base.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        in_features = dense_out_channels + mobile_out_channels\n",
    "        \n",
    "        # CORRECTED: The first linear layer now correctly expects 2032 input features.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 512), # nn.Linear(2032, 512)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes) \n",
    "        )\n",
    "        \n",
    "        self.use_seg = False \n",
    "        self.domain_head = nn.Identity()\n",
    "        \n",
    "    def forward(self, x, grl_lambda=0.0):\n",
    "        densenet_feats = self.densenet_base(x)[-1]\n",
    "        x1 = self.global_pool(densenet_feats).view(x.size(0), -1)\n",
    "\n",
    "        mobilenet_feats = self.mobilenet_base(x)[-1]\n",
    "        x2 = self.global_pool(mobilenet_feats).view(x.size(0), -1)\n",
    "\n",
    "        concatenated_features = torch.cat([x1, x2], dim=1)\n",
    "\n",
    "        logits = self.classifier(concatenated_features)\n",
    "        \n",
    "        out = {\"logits\": logits, \"feat\": concatenated_features}\n",
    "        out[\"domain_logits\"] = torch.randn(x.size(0), 2).to(x.device) \n",
    "        out[\"seg\"] = torch.randn(x.size(0), 1, IMG_SIZE, IMG_SIZE).to(x.device)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n",
    "\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.s = smoothing\n",
    "    def forward(self, logits, target):\n",
    "        c = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logp)\n",
    "            true_dist.fill_(self.s / (c - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n",
    "        return (-true_dist * logp).sum(dim=-1).mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, target):\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        ce = F.cross_entropy(logits, target, reduction='none')\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    num = 2 * (pred * target).sum() + smooth\n",
    "    den = pred.sum() + target.sum() + smooth\n",
    "    return 1 - (num / den)\n",
    "\n",
    "def seg_loss_fn(pred, mask):\n",
    "    if pred.shape[-2:] != mask.shape[-2:]:\n",
    "        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        f = F.normalize(features, dim=1)\n",
    "        sim = torch.matmul(f, f.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n",
    "        logits = sim - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n",
    "        denom = exp_logits.sum(1, keepdim=True)\n",
    "        pos_mask = mask - torch.eye(len(features), device=device)\n",
    "        pos_exp = (exp_logits * pos_mask).sum(1)\n",
    "        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n",
    "        valid = (pos_mask.sum(1) > 0).float()\n",
    "        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n",
    "        return loss\n",
    "\n",
    "clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n",
    "clf_loss_focal = FocalLoss(gamma=1.5)\n",
    "supcon_loss_fn = SupConLoss(temperature=0.07)\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, l):\n",
    "        ctx.l = l\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.l, None\n",
    "\n",
    "def grad_reverse(x, l=1.0):\n",
    "    return GradReverse.apply(x, l)\n",
    "\n",
    "\n",
    "# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    new_x = x.clone()\n",
    "    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return new_x, y, y[idx], lam_adjusted\n",
    "\n",
    "\n",
    "# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\n",
    "info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n",
    "train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \n",
    "test_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n",
    "    \n",
    "train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n",
    "train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n",
    "test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n",
    "test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n",
    "\n",
    "info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n",
    "train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n",
    "train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n",
    "test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n",
    "test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n",
    "\n",
    "train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n",
    "train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n",
    "\n",
    "testing_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\n",
    "testing_images_dir = \"/kaggle/input/testing-dataset/Images\"\n",
    "\n",
    "# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\n",
    "try:\n",
    "# --- Scenario 1 Changes ---\n",
    "# 1. 80/20 Split of testing-dataset remains the same\n",
    "    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    ALL_ROOT_DIRS = [\n",
    "        train_dir_cod,       \n",
    "        test_dir_cod,       \n",
    "        train_dir_camo_cam,  \n",
    "        train_dir_camo_noncam\n",
    "    ]\n",
    "\n",
    "# 2. Training includes basic sets + 80% of testing-dataset\n",
    "    ALL_TRAIN_TXTS = [\n",
    "        train_cam_txt2, train_noncam_txt2,\n",
    "    ]\n",
    "\n",
    "# 2. Validation uses CAMO-COCO test text files ONLY\n",
    "    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2]\n",
    "\n",
    "    train_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n",
    "        testing_image_paths=train_paths, testing_labels=train_labels,\n",
    "        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "\n",
    "    val_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS,  \n",
    "        txt_files=ALL_VAL_TXTS,                 \n",
    "        testing_image_paths=val_paths,          \n",
    "        testing_labels=val_labels,              \n",
    "        weak_transform=val_tf, \n",
    "        strong_transform=None, \n",
    "        use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "\n",
    "    def build_weighted_sampler(dataset):\n",
    "        labels = [sample[1] for sample in dataset.samples]  \n",
    "        counts = Counter(labels)\n",
    "        total = len(labels)\n",
    "        if len(counts) <= 1:\n",
    "            weights = [1.0] * total\n",
    "        else:\n",
    "            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n",
    "            weights = [class_weights[lbl] for lbl in labels]\n",
    "        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    train_sampler = build_weighted_sampler(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n",
    "    \n",
    "    class MockDataset(Dataset):\n",
    "        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n",
    "            self.num_samples = num_samples\n",
    "            self.data = torch.randn(num_samples, 3, img_size, img_size)\n",
    "            self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n",
    "\n",
    "    train_ds = MockDataset(num_samples=14150) \n",
    "    val_ds = MockDataset(num_samples=6606)   \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n",
    "\n",
    "\n",
    "# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n",
    "\n",
    "model = KerasStyleFusion().to(device)\n",
    "\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        head_params.append(param)\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': LR * 0.2}, \n",
    "    {'params': head_params, 'lr': LR}\n",
    "], lr=LR, weight_decay=1e-4)\n",
    "\n",
    "def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch) / float(max(1.0, warmup_epochs))\n",
    "        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "\n",
    "print(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\n",
    "print(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n",
    "\n",
    "# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n",
    "\n",
    "def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n",
    "    if mix_info is None:\n",
    "        if use_focal:\n",
    "            return clf_loss_focal(logits, targets)\n",
    "        else:\n",
    "            return clf_loss_ce(logits, targets)\n",
    "    else:\n",
    "        y_a, y_b, lam = mix_info\n",
    "        if use_focal:\n",
    "            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n",
    "        else:\n",
    "            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n",
    "        return loss\n",
    "\n",
    "best_vf1 = 0.0\n",
    "best_epoch = 0\n",
    "patience_count = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # --- Freeze/Unfreeze Logic ---\n",
    "    if epoch <= FREEZE_EPOCHS:\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n",
    "                p.requires_grad = False\n",
    "    elif epoch == FREEZE_EPOCHS + 1:\n",
    "        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    n_batches = 0\n",
    "\n",
    "    opt.zero_grad() \n",
    "    \n",
    "    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n",
    "        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "\n",
    "        imgs = weak_imgs\n",
    "\n",
    "        mix_info = None\n",
    "        rand = random.random()\n",
    "        if rand < PROB_MIXUP:\n",
    "            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "        elif rand < PROB_MIXUP + PROB_CUTMIX:\n",
    "            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "\n",
    "        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n",
    "            out = model(imgs) \n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n",
    "\n",
    "            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n",
    "            seg_loss = 0.0\n",
    "            supcon_loss = 0.0 \n",
    "            cons_loss = 0.0   \n",
    "            dom_loss = 0.0\n",
    "\n",
    "            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n",
    "            \n",
    "            total_loss = total_loss / ACCUMULATION_STEPS \n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad() \n",
    "\n",
    "        running_loss += total_loss.item() * ACCUMULATION_STEPS\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "        n_batches += 1\n",
    "\n",
    "    if n_batches % ACCUMULATION_STEPS != 0:\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n",
    "\n",
    "    # -------------------\n",
    "    # VALIDATION\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_y_true, val_y_pred = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for weak_imgs, _, labels, masks in val_loader:\n",
    "            imgs = weak_imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if masks is not None:\n",
    "                masks = masks.to(device)\n",
    "\n",
    "            out = model(imgs)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n",
    "            if USE_SEGMENTATION and (masks is not None):\n",
    "                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_y_true.extend(labels.cpu().numpy())\n",
    "            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "    vacc = accuracy_score(val_y_true, val_y_pred)\n",
    "    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n",
    "\n",
    "    if vf1 > best_vf1:\n",
    "        best_vf1 = vf1\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": opt.state_dict(),\n",
    "            \"best_vf1\": best_vf1\n",
    "        }, SAVE_PATH)\n",
    "        patience_count = 0\n",
    "        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        if patience_count >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408a9fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:35:18.666792Z",
     "iopub.status.busy": "2026-01-08T07:35:18.666096Z",
     "iopub.status.idle": "2026-01-08T08:24:28.528593Z",
     "shell.execute_reply": "2026-01-08T08:24:28.527651Z"
    },
    "papermill": {
     "duration": 2951.328936,
     "end_time": "2026-01-08T08:24:29.501070",
     "exception": false,
     "start_time": "2026-01-08T07:35:18.172134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "✅ Loaded 8605 samples from 4 root directories.\n",
      "✅ Loaded 2152 samples from 4 root directories.\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104M/104M [00:00<00:00, 183MB/s] \n",
      "/tmp/ipykernel_24/3390631489.py:583: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model instantiated. LR: 0.0003, Epochs: 15.\n",
      "Backbones are initially frozen for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/15: 100%|██████████| 1076/1076 [01:42<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.6917 Acc: 0.5117 Prec: 0.5417 Rec: 0.5134 F1: 0.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val Loss: 1.5427 Acc: 0.6083 Prec: 0.3041 Rec: 0.5000 F1: 0.3782\n",
      "Saved best model at epoch 1 (F1 0.3782)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/15: 100%|██████████| 1076/1076 [01:42<00:00, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.5270 Acc: 0.7568 Prec: 0.7568 Rec: 0.7567 F1: 0.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Val Loss: 1.2601 Acc: 0.9480 Prec: 0.9580 Rec: 0.9350 F1: 0.9441\n",
      "Saved best model at epoch 2 (F1 0.9441)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/15: 100%|██████████| 1076/1076 [01:43<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.4999 Acc: 0.7890 Prec: 0.7890 Rec: 0.7890 F1: 0.7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Val Loss: 1.2406 Acc: 0.9642 Prec: 0.9706 Rec: 0.9554 F1: 0.9619\n",
      "Saved best model at epoch 3 (F1 0.9619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/15: 100%|██████████| 1076/1076 [01:42<00:00, 10.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.4886 Acc: 0.7873 Prec: 0.7872 Rec: 0.7872 F1: 0.7872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Val Loss: 1.2246 Acc: 0.9828 Prec: 0.9847 Rec: 0.9793 F1: 0.9819\n",
      "Saved best model at epoch 4 (F1 0.9819)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5/15: 100%|██████████| 1076/1076 [01:41<00:00, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.4877 Acc: 0.7872 Prec: 0.7872 Rec: 0.7872 F1: 0.7872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Val Loss: 1.2113 Acc: 0.9893 Prec: 0.9876 Rec: 0.9902 F1: 0.9888\n",
      "Saved best model at epoch 5 (F1 0.9888)\n",
      "--- Unfreezing all backbone layers at epoch 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6/15: 100%|██████████| 1076/1076 [03:27<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.4555 Acc: 0.8189 Prec: 0.8189 Rec: 0.8189 F1: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Val Loss: 1.2054 Acc: 0.9851 Prec: 0.9817 Rec: 0.9878 F1: 0.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7/15: 100%|██████████| 1076/1076 [03:27<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.4372 Acc: 0.8307 Prec: 0.8307 Rec: 0.8307 F1: 0.8307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Val Loss: 1.1997 Acc: 0.9926 Prec: 0.9907 Rec: 0.9939 F1: 0.9922\n",
      "Saved best model at epoch 7 (F1 0.9922)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8/15: 100%|██████████| 1076/1076 [03:28<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.4343 Acc: 0.8450 Prec: 0.8450 Rec: 0.8449 F1: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Val Loss: 1.1961 Acc: 0.9944 Prec: 0.9930 Rec: 0.9954 F1: 0.9942\n",
      "Saved best model at epoch 8 (F1 0.9942)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9/15: 100%|██████████| 1076/1076 [03:26<00:00,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.4272 Acc: 0.8402 Prec: 0.8402 Rec: 0.8402 F1: 0.8402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Val Loss: 1.1964 Acc: 0.9935 Prec: 0.9920 Rec: 0.9944 F1: 0.9932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10/15: 100%|██████████| 1076/1076 [03:27<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.4277 Acc: 0.8418 Prec: 0.8418 Rec: 0.8418 F1: 0.8418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Val Loss: 1.1982 Acc: 0.9907 Prec: 0.9886 Rec: 0.9921 F1: 0.9903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11/15: 100%|██████████| 1076/1076 [03:27<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.4243 Acc: 0.8404 Prec: 0.8404 Rec: 0.8404 F1: 0.8404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Val Loss: 1.1977 Acc: 0.9916 Prec: 0.9897 Rec: 0.9929 F1: 0.9913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 12/15: 100%|██████████| 1076/1076 [03:28<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.4245 Acc: 0.8537 Prec: 0.8536 Rec: 0.8536 F1: 0.8536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Val Loss: 1.1969 Acc: 0.9921 Prec: 0.9903 Rec: 0.9933 F1: 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 13/15: 100%|██████████| 1076/1076 [03:28<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.4211 Acc: 0.8526 Prec: 0.8527 Rec: 0.8526 F1: 0.8526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Val Loss: 1.1961 Acc: 0.9921 Prec: 0.9903 Rec: 0.9933 F1: 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 14/15: 100%|██████████| 1076/1076 [03:27<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.4245 Acc: 0.8501 Prec: 0.8501 Rec: 0.8501 F1: 0.8501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Val Loss: 1.1944 Acc: 0.9940 Prec: 0.9926 Rec: 0.9948 F1: 0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 15/15: 100%|██████████| 1076/1076 [03:28<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.4273 Acc: 0.8436 Prec: 0.8436 Rec: 0.8436 F1: 0.8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Val Loss: 1.1949 Acc: 0.9926 Prec: 0.9909 Rec: 0.9937 F1: 0.9922\n",
      "Early stopping triggered.\n",
      "\n",
      "Training finished. Best val F1: 0.9941640519703103 at epoch 8\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import RandAugment\n",
    "import timm \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "from torch.autograd import Function\n",
    "\n",
    "# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "BATCH_SIZE = 8         \n",
    "EPOCHS = 15          \n",
    "NUM_WORKERS = 4         \n",
    "LR = 3e-4              \n",
    "LABEL_SMOOTH = 0.1\n",
    "SAVE_PATH = \"best_model.pth\"\n",
    "USE_SEGMENTATION = True \n",
    "\n",
    "# Loss weights \n",
    "ALPHA_DOM = 0.5\n",
    "BETA_SUPCON = 0.3\n",
    "ETA_CONS = 0.1\n",
    "GAMMA_SEG = 0.5\n",
    "\n",
    "# Mixup/CutMix probabilities and alphas\n",
    "PROB_MIXUP = 0.5\n",
    "PROB_CUTMIX = 0.5\n",
    "MIXUP_ALPHA = 0.2\n",
    "CUTMIX_ALPHA = 1.0\n",
    "\n",
    "# Warmup epochs and accumulation steps\n",
    "WARMUP_EPOCHS = 5\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "FREEZE_EPOCHS = 5\n",
    "ACCUMULATION_STEPS = 4\n",
    "\n",
    "# --- 2. TRANSFORMS (From Notebook Cell 6) ---\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.05):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        noisy_tensor = tensor + noise\n",
    "        return torch.clamp(noisy_tensor, 0., 1.)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
    "\n",
    "weak_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.02),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "strong_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.05),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\n",
    "def read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n",
    "\n",
    "def load_testing_dataset_info(info_file, image_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n",
    "    lines = []\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(info_file, 'r', encoding=encoding) as f:\n",
    "                lines = f.readlines()\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            image_filename = parts[0]\n",
    "            try:\n",
    "                label = int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            label = 1 if label == 1 else 0\n",
    "            image_full_path = os.path.join(image_dir, image_filename)\n",
    "            image_paths.append(image_full_path)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n",
    "        self.root_dirs = root_dirs\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "        self.use_masks = use_masks\n",
    "        self.samples = []\n",
    "\n",
    "        if testing_image_paths is not None and testing_labels is not None:\n",
    "            for img_path, label in zip(testing_image_paths, testing_labels):\n",
    "                self.samples.append((img_path, label)) \n",
    "        \n",
    "        if isinstance(txt_files, str):\n",
    "            txt_files = [txt_files]\n",
    "\n",
    "        all_lines = []\n",
    "        for t in txt_files:\n",
    "            if not os.path.exists(t):\n",
    "                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n",
    "\n",
    "            lines = read_file_with_encoding(t)\n",
    "            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n",
    "\n",
    "        for line, src_txt in all_lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 0:\n",
    "                continue\n",
    "\n",
    "            fname = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    lbl = int(parts[1])\n",
    "                except:\n",
    "                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            else:\n",
    "                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            \n",
    "            lbl = 1 if lbl == 1 else 0\n",
    "            base_fname = os.path.basename(fname)  \n",
    "\n",
    "            found = False\n",
    "            search_subs = [\n",
    "                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n",
    "                \"Images/Train\", \"Images/Test\",\n",
    "            ]\n",
    "            \n",
    "            for rdir in self.root_dirs:\n",
    "                for sub in search_subs:\n",
    "                    img_path = os.path.join(rdir, sub, base_fname)\n",
    "                    if os.path.exists(img_path):\n",
    "                        self.samples.append((img_path, lbl, rdir))\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global IMG_SIZE \n",
    "        \n",
    "        sample = self.samples[idx]\n",
    "        img_path = sample[0]\n",
    "        lbl = sample[1]\n",
    "        \n",
    "        if len(sample) == 3:\n",
    "            rdir = sample[2]\n",
    "        else:\n",
    "            rdir = os.path.dirname(os.path.dirname(img_path))\n",
    "            \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n",
    "            \n",
    "        if self.weak_transform:\n",
    "            weak = self.weak_transform(img)\n",
    "        else:\n",
    "            weak = transforms.ToTensor()(img)\n",
    "            \n",
    "        if self.strong_transform:\n",
    "            strong = self.strong_transform(img)\n",
    "        else:\n",
    "            strong = weak.clone()\n",
    "\n",
    "        mask = None\n",
    "        if self.use_masks:\n",
    "            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n",
    "            found_mask = False\n",
    "            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n",
    "                mask_path = os.path.join(rdir, mask_dir, mask_name)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n",
    "                    m = np.array(m).astype(np.float32) / 255.0\n",
    "                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n",
    "                    found_mask = True\n",
    "                    break\n",
    "\n",
    "            if mask is None:\n",
    "                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n",
    "                \n",
    "        return weak, strong, lbl, mask\n",
    "\n",
    "# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n",
    "\n",
    "class DenseNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n",
    "                feats.append(x)\n",
    "        return feats\n",
    "\n",
    "class InceptionExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # We load the full model but only use the features\n",
    "        inception = models.inception_v3(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        inception.transform_input = True # Normalizes internally for Inception\n",
    "        \n",
    "        # Extract features (everything except the final pooling and FC)\n",
    "        self.features = nn.Sequential(\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            inception.Conv2d_3b_1x1,\n",
    "            inception.Conv2d_4a_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            inception.Mixed_5b, inception.Mixed_5c, inception.Mixed_5d,\n",
    "            inception.Mixed_6a, inception.Mixed_6b, inception.Mixed_6c,\n",
    "            inception.Mixed_6d, inception.Mixed_6e,\n",
    "            inception.Mixed_7a, inception.Mixed_7b, inception.Mixed_7c\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # InceptionV3 expects 299x299. If input is different, we interpolate.\n",
    "        if x.shape[-1] != 299:\n",
    "            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        x = self.features(x)\n",
    "        return [x] # Returning as list to maintain compatibility with your loop\n",
    "# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n",
    "\n",
    "IMG_SIZE = 299  # Updated for InceptionV3\n",
    "# ... other hyperparameters remain same ...\n",
    "INCEPTION_CHANNELS = 2048 # InceptionV3 final feature count\n",
    "DENSE_CHANNELS = 1920\n",
    "TOTAL_FEATURES = DENSE_CHANNELS + INCEPTION_CHANNELS\n",
    "\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "class KerasStyleFusion(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.densenet_base = DenseNetExtractor(pretrained=True)\n",
    "        self.inception_base = InceptionExtractor(pretrained=True)\n",
    "        \n",
    "        # Freeze backbones initially\n",
    "        for param in self.densenet_base.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.inception_base.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # 1920 (DenseNet) + 2048 (Inception) = 3968\n",
    "        in_features = DENSE_CHANNELS + INCEPTION_CHANNELS\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get DenseNet features (expects 224, but works at 299)\n",
    "        d_feats = self.densenet_base(x)[-1]\n",
    "        x1 = self.global_pool(d_feats).view(x.size(0), -1)\n",
    "\n",
    "        # Get Inception features (expects 299)\n",
    "        i_feats = self.inception_base(x)[-1]\n",
    "        x2 = self.global_pool(i_feats).view(x.size(0), -1)\n",
    "\n",
    "        concatenated_features = torch.cat([x1, x2], dim=1)\n",
    "        logits = self.classifier(concatenated_features)\n",
    "        \n",
    "        return {\n",
    "            \"logits\": logits, \n",
    "            \"feat\": concatenated_features,\n",
    "            \"seg\": torch.randn(x.size(0), 1, x.shape[-2], x.shape[-1]).to(x.device) # Placeholder\n",
    "        }\n",
    "\n",
    "# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n",
    "\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.s = smoothing\n",
    "    def forward(self, logits, target):\n",
    "        c = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logp)\n",
    "            true_dist.fill_(self.s / (c - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n",
    "        return (-true_dist * logp).sum(dim=-1).mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, target):\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        ce = F.cross_entropy(logits, target, reduction='none')\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    num = 2 * (pred * target).sum() + smooth\n",
    "    den = pred.sum() + target.sum() + smooth\n",
    "    return 1 - (num / den)\n",
    "\n",
    "def seg_loss_fn(pred, mask):\n",
    "    if pred.shape[-2:] != mask.shape[-2:]:\n",
    "        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        f = F.normalize(features, dim=1)\n",
    "        sim = torch.matmul(f, f.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n",
    "        logits = sim - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n",
    "        denom = exp_logits.sum(1, keepdim=True)\n",
    "        pos_mask = mask - torch.eye(len(features), device=device)\n",
    "        pos_exp = (exp_logits * pos_mask).sum(1)\n",
    "        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n",
    "        valid = (pos_mask.sum(1) > 0).float()\n",
    "        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n",
    "        return loss\n",
    "\n",
    "clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n",
    "clf_loss_focal = FocalLoss(gamma=1.5)\n",
    "supcon_loss_fn = SupConLoss(temperature=0.07)\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, l):\n",
    "        ctx.l = l\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.l, None\n",
    "\n",
    "def grad_reverse(x, l=1.0):\n",
    "    return GradReverse.apply(x, l)\n",
    "\n",
    "\n",
    "# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    new_x = x.clone()\n",
    "    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return new_x, y, y[idx], lam_adjusted\n",
    "\n",
    "\n",
    "# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\n",
    "info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n",
    "train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \n",
    "test_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n",
    "    \n",
    "train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n",
    "train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n",
    "test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n",
    "test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n",
    "\n",
    "info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n",
    "train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n",
    "train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n",
    "test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n",
    "test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n",
    "\n",
    "train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n",
    "train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n",
    "\n",
    "testing_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\n",
    "testing_images_dir = \"/kaggle/input/testing-dataset/Images\"\n",
    "\n",
    "# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\n",
    "try:\n",
    "# --- Scenario 1 Changes ---\n",
    "# 1. 80/20 Split of testing-dataset remains the same\n",
    "    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    ALL_ROOT_DIRS = [\n",
    "        train_dir_cod,       \n",
    "        test_dir_cod,       \n",
    "        train_dir_camo_cam,  \n",
    "        train_dir_camo_noncam\n",
    "    ]\n",
    "\n",
    "# 2. Training includes basic sets + 80% of testing-dataset\n",
    "    ALL_TRAIN_TXTS = [\n",
    "        train_cam_txt2, train_noncam_txt2,\n",
    "    ]\n",
    "\n",
    "# 2. Validation uses CAMO-COCO test text files ONLY\n",
    "    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2]\n",
    "\n",
    "    train_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n",
    "        testing_image_paths=train_paths, testing_labels=train_labels,\n",
    "        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "\n",
    "    val_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS,  \n",
    "        txt_files=ALL_VAL_TXTS,                 \n",
    "        testing_image_paths=val_paths,          \n",
    "        testing_labels=val_labels,              \n",
    "        weak_transform=val_tf, \n",
    "        strong_transform=None, \n",
    "        use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "\n",
    "    def build_weighted_sampler(dataset):\n",
    "        labels = [sample[1] for sample in dataset.samples]  \n",
    "        counts = Counter(labels)\n",
    "        total = len(labels)\n",
    "        if len(counts) <= 1:\n",
    "            weights = [1.0] * total\n",
    "        else:\n",
    "            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n",
    "            weights = [class_weights[lbl] for lbl in labels]\n",
    "        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    train_sampler = build_weighted_sampler(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n",
    "    \n",
    "    class MockDataset(Dataset):\n",
    "        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n",
    "            self.num_samples = num_samples\n",
    "            self.data = torch.randn(num_samples, 3, img_size, img_size)\n",
    "            self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n",
    "\n",
    "    train_ds = MockDataset(num_samples=14150) \n",
    "    val_ds = MockDataset(num_samples=6606)   \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n",
    "\n",
    "\n",
    "# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n",
    "\n",
    "model = KerasStyleFusion().to(device)\n",
    "\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    # Updated key check for Inception\n",
    "    if any(k in name for k in ['densenet_base', 'inception_base']):\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        head_params.append(param)\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': LR * 0.1}, # Lower LR for heavy backbones\n",
    "    {'params': head_params, 'lr': LR}\n",
    "], lr=LR, weight_decay=1e-4)\n",
    "\n",
    "def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch) / float(max(1.0, warmup_epochs))\n",
    "        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "\n",
    "print(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\n",
    "print(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n",
    "\n",
    "# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n",
    "\n",
    "def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n",
    "    if mix_info is None:\n",
    "        if use_focal:\n",
    "            return clf_loss_focal(logits, targets)\n",
    "        else:\n",
    "            return clf_loss_ce(logits, targets)\n",
    "    else:\n",
    "        y_a, y_b, lam = mix_info\n",
    "        if use_focal:\n",
    "            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n",
    "        else:\n",
    "            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n",
    "        return loss\n",
    "\n",
    "best_vf1 = 0.0\n",
    "best_epoch = 0\n",
    "patience_count = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # --- Freeze/Unfreeze Logic ---\n",
    "    if epoch <= FREEZE_EPOCHS:\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n",
    "                p.requires_grad = False\n",
    "    elif epoch == FREEZE_EPOCHS + 1:\n",
    "        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    n_batches = 0\n",
    "\n",
    "    opt.zero_grad() \n",
    "    \n",
    "    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n",
    "        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "\n",
    "        imgs = weak_imgs\n",
    "\n",
    "        mix_info = None\n",
    "        rand = random.random()\n",
    "        if rand < PROB_MIXUP:\n",
    "            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "        elif rand < PROB_MIXUP + PROB_CUTMIX:\n",
    "            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "\n",
    "        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n",
    "            out = model(imgs) \n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n",
    "\n",
    "            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n",
    "            seg_loss = 0.0\n",
    "            supcon_loss = 0.0 \n",
    "            cons_loss = 0.0   \n",
    "            dom_loss = 0.0\n",
    "\n",
    "            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n",
    "            \n",
    "            total_loss = total_loss / ACCUMULATION_STEPS \n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad() \n",
    "\n",
    "        running_loss += total_loss.item() * ACCUMULATION_STEPS\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "        n_batches += 1\n",
    "\n",
    "    if n_batches % ACCUMULATION_STEPS != 0:\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n",
    "\n",
    "    # -------------------\n",
    "    # VALIDATION\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_y_true, val_y_pred = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for weak_imgs, _, labels, masks in val_loader:\n",
    "            imgs = weak_imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if masks is not None:\n",
    "                masks = masks.to(device)\n",
    "\n",
    "            out = model(imgs)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n",
    "            if USE_SEGMENTATION and (masks is not None):\n",
    "                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_y_true.extend(labels.cpu().numpy())\n",
    "            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "    vacc = accuracy_score(val_y_true, val_y_pred)\n",
    "    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n",
    "\n",
    "    if vf1 > best_vf1:\n",
    "        best_vf1 = vf1\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": opt.state_dict(),\n",
    "            \"best_vf1\": best_vf1\n",
    "        }, SAVE_PATH)\n",
    "        patience_count = 0\n",
    "        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        if patience_count >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd63000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:24:31.645521Z",
     "iopub.status.busy": "2026-01-08T08:24:31.645182Z",
     "iopub.status.idle": "2026-01-08T08:46:39.791412Z",
     "shell.execute_reply": "2026-01-08T08:46:39.790523Z"
    },
    "papermill": {
     "duration": 1330.513516,
     "end_time": "2026-01-08T08:46:41.061773",
     "exception": false,
     "start_time": "2026-01-08T08:24:30.548257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "✅ Loaded 3651 samples from 4 root directories.\n",
      "✅ Loaded 7106 samples from 4 root directories.\n",
      "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /root/.cache/torch/hub/checkpoints/densenet169-b2777c0a.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54.7M/54.7M [00:00<00:00, 191MB/s]\n",
      "/tmp/ipykernel_24/1730714653.py:575: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model instantiated. LR: 0.0003, Epochs: 15.\n",
      "Backbones are initially frozen for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/15: 100%|██████████| 457/457 [00:41<00:00, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.6929 Acc: 0.5004 Prec: 0.5075 Rec: 0.5019 F1: 0.3880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val Loss: 1.5345 Acc: 0.5317 Prec: 0.3766 Rec: 0.4956 F1: 0.3509\n",
      "Saved best model at epoch 1 (F1 0.3509)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/15: 100%|██████████| 457/457 [00:38<00:00, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.6189 Acc: 0.7425 Prec: 0.7462 Rec: 0.7420 F1: 0.7413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Val Loss: 1.2672 Acc: 0.9648 Prec: 0.9641 Rec: 0.9659 F1: 0.9647\n",
      "Saved best model at epoch 2 (F1 0.9647)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/15: 100%|██████████| 457/457 [00:38<00:00, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.4852 Acc: 0.7990 Prec: 0.7989 Rec: 0.7989 F1: 0.7989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Val Loss: 1.2966 Acc: 0.9454 Prec: 0.9457 Rec: 0.9445 F1: 0.9450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/15: 100%|██████████| 457/457 [00:38<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.4687 Acc: 0.7962 Prec: 0.7962 Rec: 0.7962 F1: 0.7962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Val Loss: 1.2953 Acc: 0.9671 Prec: 0.9666 Rec: 0.9689 F1: 0.9670\n",
      "Saved best model at epoch 4 (F1 0.9670)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5/15: 100%|██████████| 457/457 [00:39<00:00, 11.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.4721 Acc: 0.8124 Prec: 0.8119 Rec: 0.8121 F1: 0.8120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Val Loss: 1.3059 Acc: 0.9706 Prec: 0.9699 Rec: 0.9720 F1: 0.9705\n",
      "Saved best model at epoch 5 (F1 0.9705)\n",
      "--- Unfreezing all backbone layers at epoch 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6/15: 100%|██████████| 457/457 [01:14<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.4652 Acc: 0.8209 Prec: 0.8208 Rec: 0.8209 F1: 0.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Val Loss: 1.2901 Acc: 0.9740 Prec: 0.9733 Rec: 0.9748 F1: 0.9739\n",
      "Saved best model at epoch 6 (F1 0.9739)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7/15: 100%|██████████| 457/457 [01:09<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.4419 Acc: 0.8329 Prec: 0.8329 Rec: 0.8329 F1: 0.8329\n",
      "[Epoch 7] Val Loss: 1.3211 Acc: 0.9724 Prec: 0.9717 Rec: 0.9736 F1: 0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8/15: 100%|██████████| 457/457 [01:10<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.4402 Acc: 0.8192 Prec: 0.8192 Rec: 0.8192 F1: 0.8192\n",
      "[Epoch 8] Val Loss: 1.3494 Acc: 0.9695 Prec: 0.9688 Rec: 0.9710 F1: 0.9694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9/15: 100%|██████████| 457/457 [01:09<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.4292 Acc: 0.8581 Prec: 0.8580 Rec: 0.8580 F1: 0.8580\n",
      "[Epoch 9] Val Loss: 1.3355 Acc: 0.9697 Prec: 0.9690 Rec: 0.9708 F1: 0.9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10/15: 100%|██████████| 457/457 [01:09<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.4268 Acc: 0.8376 Prec: 0.8375 Rec: 0.8375 F1: 0.8375\n",
      "[Epoch 10] Val Loss: 1.3268 Acc: 0.9724 Prec: 0.9718 Rec: 0.9740 F1: 0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11/15: 100%|██████████| 457/457 [01:10<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.4332 Acc: 0.8244 Prec: 0.8243 Rec: 0.8244 F1: 0.8243\n",
      "[Epoch 11] Val Loss: 1.3400 Acc: 0.9712 Prec: 0.9705 Rec: 0.9728 F1: 0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 12/15: 100%|██████████| 457/457 [01:10<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.4237 Acc: 0.8505 Prec: 0.8503 Rec: 0.8503 F1: 0.8503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Val Loss: 1.3401 Acc: 0.9723 Prec: 0.9717 Rec: 0.9740 F1: 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 13/15: 100%|██████████| 457/457 [01:10<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.4202 Acc: 0.8403 Prec: 0.8400 Rec: 0.8400 F1: 0.8400\n",
      "[Epoch 13] Val Loss: 1.3212 Acc: 0.9734 Prec: 0.9728 Rec: 0.9751 F1: 0.9733\n",
      "Early stopping triggered.\n",
      "\n",
      "Training finished. Best val F1: 0.9738704278441408 at epoch 6\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import RandAugment\n",
    "import timm \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "from torch.autograd import Function\n",
    "\n",
    "# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8         \n",
    "EPOCHS = 15          \n",
    "NUM_WORKERS = 4         \n",
    "LR = 3e-4              \n",
    "LABEL_SMOOTH = 0.1\n",
    "SAVE_PATH = \"best_model.pth\"\n",
    "USE_SEGMENTATION = True \n",
    "\n",
    "# Loss weights \n",
    "ALPHA_DOM = 0.5\n",
    "BETA_SUPCON = 0.3\n",
    "ETA_CONS = 0.1\n",
    "GAMMA_SEG = 0.5\n",
    "\n",
    "# Mixup/CutMix probabilities and alphas\n",
    "PROB_MIXUP = 0.5\n",
    "PROB_CUTMIX = 0.5\n",
    "MIXUP_ALPHA = 0.2\n",
    "CUTMIX_ALPHA = 1.0\n",
    "\n",
    "# Warmup epochs and accumulation steps\n",
    "WARMUP_EPOCHS = 5\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "FREEZE_EPOCHS = 5\n",
    "ACCUMULATION_STEPS = 4\n",
    "\n",
    "# --- 2. TRANSFORMS (From Notebook Cell 6) ---\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.05):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        noisy_tensor = tensor + noise\n",
    "        return torch.clamp(noisy_tensor, 0., 1.)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
    "\n",
    "weak_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.02),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "strong_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.05),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\n",
    "def read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n",
    "\n",
    "def load_testing_dataset_info(info_file, image_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n",
    "    lines = []\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(info_file, 'r', encoding=encoding) as f:\n",
    "                lines = f.readlines()\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            image_filename = parts[0]\n",
    "            try:\n",
    "                label = int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            label = 1 if label == 1 else 0\n",
    "            image_full_path = os.path.join(image_dir, image_filename)\n",
    "            image_paths.append(image_full_path)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n",
    "        self.root_dirs = root_dirs\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "        self.use_masks = use_masks\n",
    "        self.samples = []\n",
    "\n",
    "        if testing_image_paths is not None and testing_labels is not None:\n",
    "            for img_path, label in zip(testing_image_paths, testing_labels):\n",
    "                self.samples.append((img_path, label)) \n",
    "        \n",
    "        if isinstance(txt_files, str):\n",
    "            txt_files = [txt_files]\n",
    "\n",
    "        all_lines = []\n",
    "        for t in txt_files:\n",
    "            if not os.path.exists(t):\n",
    "                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n",
    "\n",
    "            lines = read_file_with_encoding(t)\n",
    "            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n",
    "\n",
    "        for line, src_txt in all_lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 0:\n",
    "                continue\n",
    "\n",
    "            fname = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    lbl = int(parts[1])\n",
    "                except:\n",
    "                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            else:\n",
    "                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            \n",
    "            lbl = 1 if lbl == 1 else 0\n",
    "            base_fname = os.path.basename(fname)  \n",
    "\n",
    "            found = False\n",
    "            search_subs = [\n",
    "                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n",
    "                \"Images/Train\", \"Images/Test\",\n",
    "            ]\n",
    "            \n",
    "            for rdir in self.root_dirs:\n",
    "                for sub in search_subs:\n",
    "                    img_path = os.path.join(rdir, sub, base_fname)\n",
    "                    if os.path.exists(img_path):\n",
    "                        self.samples.append((img_path, lbl, rdir))\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global IMG_SIZE \n",
    "        \n",
    "        sample = self.samples[idx]\n",
    "        img_path = sample[0]\n",
    "        lbl = sample[1]\n",
    "        \n",
    "        if len(sample) == 3:\n",
    "            rdir = sample[2]\n",
    "        else:\n",
    "            rdir = os.path.dirname(os.path.dirname(img_path))\n",
    "            \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n",
    "            \n",
    "        if self.weak_transform:\n",
    "            weak = self.weak_transform(img)\n",
    "        else:\n",
    "            weak = transforms.ToTensor()(img)\n",
    "            \n",
    "        if self.strong_transform:\n",
    "            strong = self.strong_transform(img)\n",
    "        else:\n",
    "            strong = weak.clone()\n",
    "\n",
    "        mask = None\n",
    "        if self.use_masks:\n",
    "            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n",
    "            found_mask = False\n",
    "            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n",
    "                mask_path = os.path.join(rdir, mask_dir, mask_name)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n",
    "                    m = np.array(m).astype(np.float32) / 255.0\n",
    "                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n",
    "                    found_mask = True\n",
    "                    break\n",
    "\n",
    "            if mask is None:\n",
    "                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n",
    "                \n",
    "        return weak, strong, lbl, mask\n",
    "\n",
    "# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n",
    "\n",
    "# --- UPDATED CONSTANTS ---\n",
    "# DenseNet169 final features: 1664\n",
    "# MobileNetV3 Large final features before global pool: 112 (or 160/960 depending on layer, \n",
    "# but based on your previous code 112 is the expected mid-layer output)\n",
    "DENSE_CHANNELS = 1664  \n",
    "MOBILE_CHANNELS = 112  \n",
    "TOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1664 + 112 = 1776\n",
    "\n",
    "class DenseNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Changed from densenet201 to densenet169\n",
    "        self.features = models.densenet169(weights='IMAGENET1K_V1' if pretrained else None).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            # The block names remain the same across DenseNet variants\n",
    "            if name in [\"denseblock1\", \"denseblock2\", \"denseblock3\", \"denseblock4\"]:\n",
    "                feats.append(x)\n",
    "        return feats\n",
    "\n",
    "class MobileNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        out = x\n",
    "        for i, layer in enumerate(self.features):\n",
    "            out = layer(out)\n",
    "            if i in (2,5,9,12):\n",
    "                feats.append(out)\n",
    "        if len(feats) < 4:\n",
    "            feats.append(out)\n",
    "        return feats\n",
    "\n",
    "# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "class KerasStyleFusion(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, dense_out_channels=DENSE_CHANNELS, mobile_out_channels=MOBILE_CHANNELS):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.densenet_base = DenseNetExtractor(pretrained=True)\n",
    "        self.mobilenet_base = MobileNetExtractor(pretrained=True)\n",
    "        \n",
    "        # Freezing logic\n",
    "        for param in self.densenet_base.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.mobilenet_base.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        in_features = dense_out_channels + mobile_out_channels\n",
    "        \n",
    "        # Input features are now 1776 (1664 + 112)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x, grl_lambda=0.0):\n",
    "        # Extract features from DenseNet169\n",
    "        densenet_feats = self.densenet_base(x)[-1]\n",
    "        x1 = self.global_pool(densenet_feats).view(x.size(0), -1)\n",
    "\n",
    "        # Extract features from MobileNetV3\n",
    "        mobilenet_feats = self.mobilenet_base(x)[-1]\n",
    "        x2 = self.global_pool(mobilenet_feats).view(x.size(0), -1)\n",
    "\n",
    "        # Fusion\n",
    "        concatenated_features = torch.cat([x1, x2], dim=1)\n",
    "\n",
    "        logits = self.classifier(concatenated_features)\n",
    "        \n",
    "        # Maintaining your dictionary output structure\n",
    "        out = {\"logits\": logits, \"feat\": concatenated_features}\n",
    "        out[\"domain_logits\"] = torch.zeros(x.size(0), 2).to(x.device) \n",
    "        out[\"seg\"] = torch.zeros(x.size(0), 1, IMG_SIZE, IMG_SIZE).to(x.device)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n",
    "\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.s = smoothing\n",
    "    def forward(self, logits, target):\n",
    "        c = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logp)\n",
    "            true_dist.fill_(self.s / (c - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n",
    "        return (-true_dist * logp).sum(dim=-1).mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, target):\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        ce = F.cross_entropy(logits, target, reduction='none')\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    num = 2 * (pred * target).sum() + smooth\n",
    "    den = pred.sum() + target.sum() + smooth\n",
    "    return 1 - (num / den)\n",
    "\n",
    "def seg_loss_fn(pred, mask):\n",
    "    if pred.shape[-2:] != mask.shape[-2:]:\n",
    "        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        f = F.normalize(features, dim=1)\n",
    "        sim = torch.matmul(f, f.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n",
    "        logits = sim - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n",
    "        denom = exp_logits.sum(1, keepdim=True)\n",
    "        pos_mask = mask - torch.eye(len(features), device=device)\n",
    "        pos_exp = (exp_logits * pos_mask).sum(1)\n",
    "        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n",
    "        valid = (pos_mask.sum(1) > 0).float()\n",
    "        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n",
    "        return loss\n",
    "\n",
    "clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n",
    "clf_loss_focal = FocalLoss(gamma=1.5)\n",
    "supcon_loss_fn = SupConLoss(temperature=0.07)\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, l):\n",
    "        ctx.l = l\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.l, None\n",
    "\n",
    "def grad_reverse(x, l=1.0):\n",
    "    return GradReverse.apply(x, l)\n",
    "\n",
    "\n",
    "# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    new_x = x.clone()\n",
    "    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return new_x, y, y[idx], lam_adjusted\n",
    "\n",
    "\n",
    "# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\n",
    "info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n",
    "train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \n",
    "test_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n",
    "    \n",
    "train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n",
    "train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n",
    "test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n",
    "test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n",
    "\n",
    "info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n",
    "train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n",
    "train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n",
    "test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n",
    "test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n",
    "\n",
    "train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n",
    "train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n",
    "\n",
    "testing_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\n",
    "testing_images_dir = \"/kaggle/input/testing-dataset/Images\"\n",
    "\n",
    "# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\n",
    "try:\n",
    "    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        testing_image_paths, testing_labels, test_size=0.8, random_state=SEED\n",
    "    )\n",
    "    ALL_ROOT_DIRS = [\n",
    "        train_dir_cod,       \n",
    "        test_dir_cod,       \n",
    "        train_dir_camo_cam,  \n",
    "        train_dir_camo_noncam\n",
    "    ]\n",
    "    ALL_TRAIN_TXTS = [\n",
    "        train_cam_txt2, train_noncam_txt2,\n",
    "    ]\n",
    "    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2]\n",
    "    \n",
    "    train_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS, \n",
    "        txt_files=ALL_TRAIN_TXTS,               \n",
    "        testing_image_paths=train_paths,        \n",
    "        testing_labels=train_labels,            \n",
    "        weak_transform=weak_tf, \n",
    "        strong_transform=strong_tf, \n",
    "        use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "    val_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS,  \n",
    "        txt_files=ALL_VAL_TXTS,                 \n",
    "        testing_image_paths=val_paths,          \n",
    "        testing_labels=val_labels,              \n",
    "        weak_transform=val_tf, \n",
    "        strong_transform=None, \n",
    "        use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "\n",
    "    def build_weighted_sampler(dataset):\n",
    "        labels = [sample[1] for sample in dataset.samples]  \n",
    "        counts = Counter(labels)\n",
    "        total = len(labels)\n",
    "        if len(counts) <= 1:\n",
    "            weights = [1.0] * total\n",
    "        else:\n",
    "            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n",
    "            weights = [class_weights[lbl] for lbl in labels]\n",
    "        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    train_sampler = build_weighted_sampler(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n",
    "    \n",
    "    class MockDataset(Dataset):\n",
    "        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n",
    "            self.num_samples = num_samples\n",
    "            self.data = torch.randn(num_samples, 3, img_size, img_size)\n",
    "            self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n",
    "\n",
    "    train_ds = MockDataset(num_samples=14150) \n",
    "    val_ds = MockDataset(num_samples=6606)   \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n",
    "\n",
    "\n",
    "# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n",
    "\n",
    "model = KerasStyleFusion().to(device)\n",
    "\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        head_params.append(param)\n",
    "\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': LR * 0.2}, \n",
    "    {'params': head_params, 'lr': LR}\n",
    "], lr=LR, weight_decay=1e-4)\n",
    "\n",
    "def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch) / float(max(1.0, warmup_epochs))\n",
    "        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "\n",
    "print(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\n",
    "print(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n",
    "\n",
    "# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n",
    "\n",
    "def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n",
    "    if mix_info is None:\n",
    "        if use_focal:\n",
    "            return clf_loss_focal(logits, targets)\n",
    "        else:\n",
    "            return clf_loss_ce(logits, targets)\n",
    "    else:\n",
    "        y_a, y_b, lam = mix_info\n",
    "        if use_focal:\n",
    "            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n",
    "        else:\n",
    "            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n",
    "        return loss\n",
    "\n",
    "best_vf1 = 0.0\n",
    "best_epoch = 0\n",
    "patience_count = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # --- Freeze/Unfreeze Logic ---\n",
    "    if epoch <= FREEZE_EPOCHS:\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n",
    "                p.requires_grad = False\n",
    "    elif epoch == FREEZE_EPOCHS + 1:\n",
    "        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    n_batches = 0\n",
    "\n",
    "    opt.zero_grad() \n",
    "    \n",
    "    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n",
    "        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "\n",
    "        imgs = weak_imgs\n",
    "\n",
    "        mix_info = None\n",
    "        rand = random.random()\n",
    "        if rand < PROB_MIXUP:\n",
    "            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "        elif rand < PROB_MIXUP + PROB_CUTMIX:\n",
    "            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "\n",
    "        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n",
    "            out = model(imgs) \n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n",
    "\n",
    "            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n",
    "            seg_loss = 0.0\n",
    "            supcon_loss = 0.0 \n",
    "            cons_loss = 0.0   \n",
    "            dom_loss = 0.0\n",
    "\n",
    "            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n",
    "            \n",
    "            total_loss = total_loss / ACCUMULATION_STEPS \n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad() \n",
    "\n",
    "        running_loss += total_loss.item() * ACCUMULATION_STEPS\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "        n_batches += 1\n",
    "\n",
    "    if n_batches % ACCUMULATION_STEPS != 0:\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n",
    "\n",
    "    # -------------------\n",
    "    # VALIDATION\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_y_true, val_y_pred = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for weak_imgs, _, labels, masks in val_loader:\n",
    "            imgs = weak_imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if masks is not None:\n",
    "                masks = masks.to(device)\n",
    "\n",
    "            out = model(imgs)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n",
    "            if USE_SEGMENTATION and (masks is not None):\n",
    "                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_y_true.extend(labels.cpu().numpy())\n",
    "            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "    vacc = accuracy_score(val_y_true, val_y_pred)\n",
    "    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n",
    "\n",
    "    if vf1 > best_vf1:\n",
    "        best_vf1 = vf1\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": opt.state_dict(),\n",
    "            \"best_vf1\": best_vf1\n",
    "        }, SAVE_PATH)\n",
    "        patience_count = 0\n",
    "        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        if patience_count >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2ac7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:46:43.578963Z",
     "iopub.status.busy": "2026-01-08T08:46:43.578627Z",
     "iopub.status.idle": "2026-01-08T09:35:32.854541Z",
     "shell.execute_reply": "2026-01-08T09:35:32.853557Z"
    },
    "papermill": {
     "duration": 2930.535867,
     "end_time": "2026-01-08T09:35:32.856247",
     "exception": false,
     "start_time": "2026-01-08T08:46:42.320380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "✅ Loaded 3651 samples from 4 root directories.\n",
      "✅ Loaded 7106 samples from 4 root directories.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec779cee2f44d4c9260f572d6211c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/2847079429.py:596: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model instantiated. LR: 0.0003, Epochs: 15.\n",
      "Backbones are initially frozen for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/15: 100%|██████████| 457/457 [00:58<00:00,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.7162 Acc: 0.5021 Prec: 0.5168 Rec: 0.5122 F1: 0.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val Loss: 1.5395 Acc: 0.5180 Prec: 0.4286 Rec: 0.4852 F1: 0.3719\n",
      "Saved best model at epoch 1 (F1 0.3719)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/15: 100%|██████████| 457/457 [00:56<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.5255 Acc: 0.7623 Prec: 0.7622 Rec: 0.7623 F1: 0.7622\n",
      "[Epoch 2] Val Loss: 1.2113 Acc: 0.9949 Prec: 0.9947 Rec: 0.9951 F1: 0.9949\n",
      "Saved best model at epoch 2 (F1 0.9949)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/15: 100%|██████████| 457/457 [00:57<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.4908 Acc: 0.7869 Prec: 0.7869 Rec: 0.7869 F1: 0.7869\n",
      "[Epoch 3] Val Loss: 1.1871 Acc: 0.9954 Prec: 0.9951 Rec: 0.9955 F1: 0.9953\n",
      "Saved best model at epoch 3 (F1 0.9953)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/15: 100%|██████████| 457/457 [00:56<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.4851 Acc: 0.7946 Prec: 0.7946 Rec: 0.7946 F1: 0.7946\n",
      "[Epoch 4] Val Loss: 1.1951 Acc: 0.9916 Prec: 0.9920 Rec: 0.9911 F1: 0.9915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5/15: 100%|██████████| 457/457 [00:57<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.4774 Acc: 0.8042 Prec: 0.8042 Rec: 0.8041 F1: 0.8041\n",
      "[Epoch 5] Val Loss: 1.1897 Acc: 0.9949 Prec: 0.9951 Rec: 0.9947 F1: 0.9949\n",
      "--- Unfreezing all 3 backbones at epoch 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6/15: 100%|██████████| 457/457 [02:15<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.4749 Acc: 0.8066 Prec: 0.8066 Rec: 0.8066 F1: 0.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Val Loss: 1.1851 Acc: 0.9942 Prec: 0.9945 Rec: 0.9940 F1: 0.9942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7/15: 100%|██████████| 457/457 [02:10<00:00,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.4634 Acc: 0.8192 Prec: 0.8191 Rec: 0.8192 F1: 0.8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Val Loss: 1.1983 Acc: 0.9961 Prec: 0.9959 Rec: 0.9962 F1: 0.9960\n",
      "Saved best model at epoch 7 (F1 0.9960)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8/15: 100%|██████████| 457/457 [02:11<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.4590 Acc: 0.8195 Prec: 0.8195 Rec: 0.8195 F1: 0.8195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Val Loss: 1.1875 Acc: 0.9962 Prec: 0.9960 Rec: 0.9964 F1: 0.9962\n",
      "Saved best model at epoch 8 (F1 0.9962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9/15: 100%|██████████| 457/457 [02:10<00:00,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.4460 Acc: 0.8461 Prec: 0.8459 Rec: 0.8459 F1: 0.8459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Val Loss: 1.1879 Acc: 0.9963 Prec: 0.9964 Rec: 0.9962 F1: 0.9963\n",
      "Saved best model at epoch 9 (F1 0.9963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10/15: 100%|██████████| 457/457 [02:09<00:00,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.4385 Acc: 0.8359 Prec: 0.8359 Rec: 0.8359 F1: 0.8359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Val Loss: 1.1884 Acc: 0.9973 Prec: 0.9973 Rec: 0.9973 F1: 0.9973\n",
      "Saved best model at epoch 10 (F1 0.9973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11/15: 100%|██████████| 457/457 [02:09<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.4455 Acc: 0.8190 Prec: 0.8192 Rec: 0.8191 F1: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Val Loss: 1.1917 Acc: 0.9973 Prec: 0.9974 Rec: 0.9973 F1: 0.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 12/15: 100%|██████████| 457/457 [02:08<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.4385 Acc: 0.8376 Prec: 0.8376 Rec: 0.8375 F1: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Val Loss: 1.1879 Acc: 0.9976 Prec: 0.9976 Rec: 0.9975 F1: 0.9976\n",
      "Saved best model at epoch 12 (F1 0.9976)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 13/15: 100%|██████████| 457/457 [02:07<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.4316 Acc: 0.8261 Prec: 0.8262 Rec: 0.8261 F1: 0.8261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Val Loss: 1.1884 Acc: 0.9977 Prec: 0.9978 Rec: 0.9977 F1: 0.9977\n",
      "Saved best model at epoch 13 (F1 0.9977)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 14/15: 100%|██████████| 457/457 [02:08<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.4304 Acc: 0.8332 Prec: 0.8333 Rec: 0.8331 F1: 0.8331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Val Loss: 1.1903 Acc: 0.9969 Prec: 0.9968 Rec: 0.9970 F1: 0.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 15/15: 100%|██████████| 457/457 [02:08<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.4314 Acc: 0.8381 Prec: 0.8384 Rec: 0.8383 F1: 0.8381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Val Loss: 1.1883 Acc: 0.9977 Prec: 0.9977 Rec: 0.9977 F1: 0.9977\n",
      "Saved best model at epoch 15 (F1 0.9977)\n",
      "\n",
      "Training finished. Best val F1: 0.9977361654733695 at epoch 15\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import RandAugment\n",
    "import timm \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "from torch.autograd import Function\n",
    "\n",
    "# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8         \n",
    "EPOCHS = 15\n",
    "NUM_WORKERS = 4         \n",
    "LR = 3e-4              \n",
    "LABEL_SMOOTH = 0.1\n",
    "SAVE_PATH = \"best_model.pth\"\n",
    "USE_SEGMENTATION = True \n",
    "\n",
    "# Loss weights \n",
    "ALPHA_DOM = 0.5\n",
    "BETA_SUPCON = 0.3\n",
    "ETA_CONS = 0.1\n",
    "GAMMA_SEG = 0.5\n",
    "\n",
    "# Mixup/CutMix probabilities and alphas\n",
    "PROB_MIXUP = 0.5\n",
    "PROB_CUTMIX = 0.5\n",
    "MIXUP_ALPHA = 0.2\n",
    "CUTMIX_ALPHA = 1.0\n",
    "\n",
    "# Warmup epochs and accumulation steps\n",
    "WARMUP_EPOCHS = 5\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "FREEZE_EPOCHS = 5\n",
    "ACCUMULATION_STEPS = 4\n",
    "\n",
    "# --- 2. TRANSFORMS (From Notebook Cell 6) ---\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.05):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        noisy_tensor = tensor + noise\n",
    "        return torch.clamp(noisy_tensor, 0., 1.)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
    "\n",
    "weak_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.02),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "strong_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.05),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\n",
    "def read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n",
    "\n",
    "def load_testing_dataset_info(info_file, image_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n",
    "    lines = []\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(info_file, 'r', encoding=encoding) as f:\n",
    "                lines = f.readlines()\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            image_filename = parts[0]\n",
    "            try:\n",
    "                label = int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            label = 1 if label == 1 else 0\n",
    "            image_full_path = os.path.join(image_dir, image_filename)\n",
    "            image_paths.append(image_full_path)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n",
    "        self.root_dirs = root_dirs\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "        self.use_masks = use_masks\n",
    "        self.samples = []\n",
    "\n",
    "        if testing_image_paths is not None and testing_labels is not None:\n",
    "            for img_path, label in zip(testing_image_paths, testing_labels):\n",
    "                self.samples.append((img_path, label)) \n",
    "        \n",
    "        if isinstance(txt_files, str):\n",
    "            txt_files = [txt_files]\n",
    "\n",
    "        all_lines = []\n",
    "        for t in txt_files:\n",
    "            if not os.path.exists(t):\n",
    "                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n",
    "\n",
    "            lines = read_file_with_encoding(t)\n",
    "            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n",
    "\n",
    "        for line, src_txt in all_lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 0:\n",
    "                continue\n",
    "\n",
    "            fname = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    lbl = int(parts[1])\n",
    "                except:\n",
    "                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            else:\n",
    "                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            \n",
    "            lbl = 1 if lbl == 1 else 0\n",
    "            base_fname = os.path.basename(fname)  \n",
    "\n",
    "            found = False\n",
    "            search_subs = [\n",
    "                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n",
    "                \"Images/Train\", \"Images/Test\",\n",
    "            ]\n",
    "            \n",
    "            for rdir in self.root_dirs:\n",
    "                for sub in search_subs:\n",
    "                    img_path = os.path.join(rdir, sub, base_fname)\n",
    "                    if os.path.exists(img_path):\n",
    "                        self.samples.append((img_path, lbl, rdir))\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global IMG_SIZE \n",
    "        \n",
    "        sample = self.samples[idx]\n",
    "        img_path = sample[0]\n",
    "        lbl = sample[1]\n",
    "        \n",
    "        if len(sample) == 3:\n",
    "            rdir = sample[2]\n",
    "        else:\n",
    "            rdir = os.path.dirname(os.path.dirname(img_path))\n",
    "           \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n",
    "            \n",
    "        if self.weak_transform:\n",
    "            weak = self.weak_transform(img)\n",
    "        else:\n",
    "            weak = transforms.ToTensor()(img)\n",
    "            \n",
    "        if self.strong_transform:\n",
    "            strong = self.strong_transform(img)\n",
    "        else:\n",
    "            strong = weak.clone()\n",
    "\n",
    "        mask = None\n",
    "        if self.use_masks:\n",
    "            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n",
    "            found_mask = False\n",
    "            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n",
    "                mask_path = os.path.join(rdir, mask_dir, mask_name)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n",
    "                    m = np.array(m).astype(np.float32) / 255.0\n",
    "                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n",
    "                    found_mask = True\n",
    "                    break\n",
    "\n",
    "            if mask is None:\n",
    "                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n",
    "                \n",
    "        return weak, strong, lbl, mask\n",
    "\n",
    "# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n",
    "\n",
    "# --- UPDATED CONSTANTS ---\n",
    "# DenseNet169 final features: 1664\n",
    "# MobileNetV3 Large final features before global pool: 112 (or 160/960 depending on layer, \n",
    "# but based on your previous code 112 is the expected mid-layer output)\n",
    "# --- UPDATED CONSTANTS FOR TRIPLE FUSION ---\n",
    "# Final feature maps after Global Average Pooling:\n",
    "DENSE_CHANNELS = 1920      # DenseNet201\n",
    "INCEPTION_CHANNELS = 2048  # InceptionV3\n",
    "EFFICIENT_CHANNELS = 1280  # EfficientNetV2-S (timm default)\n",
    "\n",
    "TOTAL_FEATURES = DENSE_CHANNELS + INCEPTION_CHANNELS + EFFICIENT_CHANNELS \n",
    "\n",
    "class DenseNet201Extractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class InceptionExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # aux_logits=False is necessary to simplify the output to just features\n",
    "        self.model = models.inception_v3(weights='IMAGENET1K_V1' if pretrained else None, aux_logits=True)\n",
    "    def forward(self, x):\n",
    "        # InceptionV3 requires 299x299 ideally, but will work with 224x224\n",
    "        # We extract features before the final FC layer\n",
    "        x = self.model.Conv2d_1a_3x3(x)\n",
    "        x = self.model.Conv2d_2a_3x3(x)\n",
    "        x = self.model.Conv2d_2b_3x3(x)\n",
    "        x = self.model.maxpool1(x)\n",
    "        x = self.model.Conv2d_3b_1x1(x)\n",
    "        x = self.model.Conv2d_4a_3x3(x)\n",
    "        x = self.model.maxpool2(x)\n",
    "        x = self.model.Mixed_5b(x)\n",
    "        x = self.model.Mixed_5c(x)\n",
    "        x = self.model.Mixed_5d(x)\n",
    "        x = self.model.Mixed_6a(x)\n",
    "        x = self.model.Mixed_6b(x)\n",
    "        x = self.model.Mixed_6c(x)\n",
    "        x = self.model.Mixed_6d(x)\n",
    "        x = self.model.Mixed_6e(x)\n",
    "        x = self.model.Mixed_7a(x)\n",
    "        x = self.model.Mixed_7b(x)\n",
    "        x = self.model.Mixed_7c(x)\n",
    "        return x\n",
    "\n",
    "class EfficientNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Using timm for a modern EfficientNetV2 implementation\n",
    "        self.model = timm.create_model('tf_efficientnetv2_s', pretrained=pretrained, num_classes=0, global_pool='')\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "class TripleFusionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.densenet_base = DenseNet201Extractor(pretrained=True)\n",
    "        self.inception_base = InceptionExtractor(pretrained=True)\n",
    "        self.efficient_base = EfficientNetExtractor(pretrained=True)\n",
    "        \n",
    "        # Freezing logic for the first phase\n",
    "        for base in [self.densenet_base, self.inception_base, self.efficient_base]:\n",
    "            for param in base.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Classifier input: 1920 + 2048 + 1280 = 5248\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(TOTAL_FEATURES, 1024), \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Feature Extraction\n",
    "        feat_dense = self.densenet_base(x)\n",
    "        feat_inc = self.inception_base(x)\n",
    "        feat_eff = self.efficient_base(x)\n",
    "\n",
    "        # 2. Global Average Pooling & Flattening\n",
    "        x1 = self.global_pool(feat_dense).view(x.size(0), -1)\n",
    "        x2 = self.global_pool(feat_inc).view(x.size(0), -1)\n",
    "        x3 = self.global_pool(feat_eff).view(x.size(0), -1)\n",
    "\n",
    "        # 3. Concatenation (Triple Fusion)\n",
    "        merged = torch.cat([x1, x2, x3], dim=1)\n",
    "\n",
    "        # 4. Classification\n",
    "        logits = self.classifier(merged)\n",
    "        \n",
    "        return {\n",
    "            \"logits\": logits, \n",
    "            \"feat\": merged,\n",
    "            \"domain_logits\": torch.zeros(x.size(0), 2).to(x.device),\n",
    "            \"seg\": torch.zeros(x.size(0), 1, IMG_SIZE, IMG_SIZE).to(x.device)\n",
    "        }\n",
    "\n",
    "# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n",
    "\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.s = smoothing\n",
    "    def forward(self, logits, target):\n",
    "        c = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logp)\n",
    "            true_dist.fill_(self.s / (c - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n",
    "        return (-true_dist * logp).sum(dim=-1).mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, target):\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        ce = F.cross_entropy(logits, target, reduction='none')\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    num = 2 * (pred * target).sum() + smooth\n",
    "    den = pred.sum() + target.sum() + smooth\n",
    "    return 1 - (num / den)\n",
    "\n",
    "def seg_loss_fn(pred, mask):\n",
    "    if pred.shape[-2:] != mask.shape[-2:]:\n",
    "        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        f = F.normalize(features, dim=1)\n",
    "        sim = torch.matmul(f, f.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n",
    "        logits = sim - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n",
    "        denom = exp_logits.sum(1, keepdim=True)\n",
    "        pos_mask = mask - torch.eye(len(features), device=device)\n",
    "        pos_exp = (exp_logits * pos_mask).sum(1)\n",
    "        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n",
    "        valid = (pos_mask.sum(1) > 0).float()\n",
    "        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n",
    "        return loss\n",
    "\n",
    "clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n",
    "clf_loss_focal = FocalLoss(gamma=1.5)\n",
    "supcon_loss_fn = SupConLoss(temperature=0.07)\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, l):\n",
    "        ctx.l = l\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.l, None\n",
    "\n",
    "def grad_reverse(x, l=1.0):\n",
    "    return GradReverse.apply(x, l)\n",
    "\n",
    "\n",
    "# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    new_x = x.clone()\n",
    "    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return new_x, y, y[idx], lam_adjusted\n",
    "\n",
    "\n",
    "# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\n",
    "info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n",
    "train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \n",
    "test_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n",
    "    \n",
    "train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n",
    "train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n",
    "test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n",
    "test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n",
    "\n",
    "info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n",
    "train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n",
    "train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n",
    "test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n",
    "test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n",
    "\n",
    "train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n",
    "train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n",
    "\n",
    "testing_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\n",
    "testing_images_dir = \"/kaggle/input/testing-dataset/Images\"\n",
    "\n",
    "# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\n",
    "try:\n",
    "    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        testing_image_paths, testing_labels, test_size=0.8, random_state=SEED\n",
    "    )\n",
    "    ALL_ROOT_DIRS = [\n",
    "        train_dir_cod,       \n",
    "        test_dir_cod,       \n",
    "        train_dir_camo_cam,  \n",
    "        train_dir_camo_noncam\n",
    "    ]\n",
    "    ALL_TRAIN_TXTS = [\n",
    "        train_cam_txt2, train_noncam_txt2,\n",
    "    ]\n",
    "    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2]\n",
    "    \n",
    "    train_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS, \n",
    "        txt_files=ALL_TRAIN_TXTS,               \n",
    "        testing_image_paths=train_paths,        \n",
    "        testing_labels=train_labels,            \n",
    "        weak_transform=weak_tf, \n",
    "        strong_transform=strong_tf, \n",
    "        use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "    val_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS,  \n",
    "        txt_files=ALL_VAL_TXTS,                 \n",
    "        testing_image_paths=val_paths,          \n",
    "        testing_labels=val_labels,              \n",
    "        weak_transform=val_tf, \n",
    "        strong_transform=None, \n",
    "        use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "\n",
    "    def build_weighted_sampler(dataset):\n",
    "        labels = [sample[1] for sample in dataset.samples]  \n",
    "        counts = Counter(labels)\n",
    "        total = len(labels)\n",
    "        if len(counts) <= 1:\n",
    "            weights = [1.0] * total\n",
    "        else:\n",
    "            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n",
    "            weights = [class_weights[lbl] for lbl in labels]\n",
    "        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    train_sampler = build_weighted_sampler(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n",
    "    \n",
    "    class MockDataset(Dataset):\n",
    "        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n",
    "            self.num_samples = num_samples\n",
    "            self.data = torch.randn(num_samples, 3, img_size, img_size)\n",
    "            self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n",
    "\n",
    "    train_ds = MockDataset(num_samples=14150) \n",
    "    val_ds = MockDataset(num_samples=6606)   \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n",
    "\n",
    "\n",
    "# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n",
    "\n",
    "model = TripleFusionModel(num_classes=2).to(device)\n",
    "\n",
    "# Update parameter groups for Optimizer\n",
    "backbone_names = ['densenet_base', 'inception_base', 'efficient_base']\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if any(k in name for k in backbone_names):\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        head_params.append(param)\n",
    "\n",
    "def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch) / float(max(1.0, warmup_epochs))\n",
    "        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': LR / 10}, # Backbones usually need a smaller LR\n",
    "    {'params': head_params, 'lr': LR}\n",
    "], weight_decay=1e-4)\n",
    "scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "\n",
    "print(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\n",
    "print(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n",
    "\n",
    "# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n",
    "\n",
    "def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n",
    "    if mix_info is None:\n",
    "        if use_focal:\n",
    "            return clf_loss_focal(logits, targets)\n",
    "        else:\n",
    "            return clf_loss_ce(logits, targets)\n",
    "    else:\n",
    "        y_a, y_b, lam = mix_info\n",
    "        if use_focal:\n",
    "            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n",
    "        else:\n",
    "            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n",
    "        return loss\n",
    "\n",
    "best_vf1 = 0.0\n",
    "best_epoch = 0\n",
    "patience_count = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # --- Freeze/Unfreeze Logic ---\n",
    "    if epoch <= FREEZE_EPOCHS:\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n",
    "                p.requires_grad = False\n",
    "    if epoch == FREEZE_EPOCHS + 1:\n",
    "        print(f\"--- Unfreezing all 3 backbones at epoch {epoch} ---\")\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in backbone_names):\n",
    "                p.requires_grad = True\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    n_batches = 0\n",
    "    opt.zero_grad() \n",
    "    \n",
    "    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n",
    "        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "\n",
    "        imgs = weak_imgs\n",
    "\n",
    "        mix_info = None\n",
    "        rand = random.random()\n",
    "        if rand < PROB_MIXUP:\n",
    "            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "        elif rand < PROB_MIXUP + PROB_CUTMIX:\n",
    "            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "\n",
    "        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n",
    "            out = model(imgs) \n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n",
    "\n",
    "            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n",
    "            seg_loss = 0.0\n",
    "            supcon_loss = 0.0 \n",
    "            cons_loss = 0.0   \n",
    "            dom_loss = 0.0\n",
    "\n",
    "            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n",
    "            \n",
    "            total_loss = total_loss / ACCUMULATION_STEPS \n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad() \n",
    "\n",
    "        running_loss += total_loss.item() * ACCUMULATION_STEPS\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "        n_batches += 1\n",
    "\n",
    "    if n_batches % ACCUMULATION_STEPS != 0:\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n",
    "\n",
    "    # -------------------\n",
    "    # VALIDATION\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_y_true, val_y_pred = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for weak_imgs, _, labels, masks in val_loader:\n",
    "            imgs = weak_imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if masks is not None:\n",
    "                masks = masks.to(device)\n",
    "\n",
    "            out = model(imgs)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n",
    "            if USE_SEGMENTATION and (masks is not None):\n",
    "                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_y_true.extend(labels.cpu().numpy())\n",
    "            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "    vacc = accuracy_score(val_y_true, val_y_pred)\n",
    "    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n",
    "\n",
    "    if vf1 > best_vf1:\n",
    "        best_vf1 = vf1\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": opt.state_dict(),\n",
    "            \"best_vf1\": best_vf1\n",
    "        }, SAVE_PATH)\n",
    "        patience_count = 0\n",
    "        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        if patience_count >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2932761,
     "sourceId": 5051281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8580465,
     "sourceId": 13514447,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8580489,
     "sourceId": 13514489,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8582404,
     "sourceId": 13517101,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9803.136493,
   "end_time": "2026-01-08T09:35:37.020843",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-08T06:52:13.884350",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "116e4dfc14f44478a76eacbea7c9acec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_35ea7824d8474cc990776fc7abf5ca3a",
       "placeholder": "​",
       "style": "IPY_MODEL_e4a812d821a74027811802730f99b8bd",
       "tabbable": null,
       "tooltip": null,
       "value": " 86.5M/86.5M [00:01&lt;00:00, 117MB/s]"
      }
     },
     "35ea7824d8474cc990776fc7abf5ca3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c5638e3fdcf49208aed2919c300ae32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "707431770428487bb1f636e83bae8b4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7cae098fbd0c47ca811a1cd0de44e356": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82d550d169fb4542b5f7069bbd643643": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e4c9b332a30a4044a9ca84e15277daf2",
       "placeholder": "​",
       "style": "IPY_MODEL_4c5638e3fdcf49208aed2919c300ae32",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "c2c093e4594949e68397604c1f9ae42a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7cae098fbd0c47ca811a1cd0de44e356",
       "max": 86523256.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_707431770428487bb1f636e83bae8b4d",
       "tabbable": null,
       "tooltip": null,
       "value": 86523256.0
      }
     },
     "cec779cee2f44d4c9260f572d6211c27": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_82d550d169fb4542b5f7069bbd643643",
        "IPY_MODEL_c2c093e4594949e68397604c1f9ae42a",
        "IPY_MODEL_116e4dfc14f44478a76eacbea7c9acec"
       ],
       "layout": "IPY_MODEL_d5dd76b30c9546a8b79d7df32d4c63eb",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d5dd76b30c9546a8b79d7df32d4c63eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4a812d821a74027811802730f99b8bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e4c9b332a30a4044a9ca84e15277daf2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
