{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5051281,"sourceType":"datasetVersion","datasetId":2932761},{"sourceId":13514447,"sourceType":"datasetVersion","datasetId":8580465},{"sourceId":13514489,"sourceType":"datasetVersion","datasetId":8580489},{"sourceId":13517101,"sourceType":"datasetVersion","datasetId":8582404}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T14:47:12.409203Z","iopub.execute_input":"2026-01-07T14:47:12.409476Z","iopub.status.idle":"2026-01-07T14:47:13.521524Z","shell.execute_reply.started":"2026-01-07T14:47:12.409443Z","shell.execute_reply":"2026-01-07T14:47:13.520702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 15          \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 7\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\n\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\nclass DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n\nclass MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):\n                feats.append(out)\n        if len(feats) < 4:\n            feats.append(out)\n        return feats\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\nDENSE_CHANNELS = 1920\n# CORRECTION: The actual output features are 112, resulting in 2032 total features (1920+112). \n# We MUST use 112 here to resolve the mat1 and mat2 dimension mismatch error (2032 vs 3200).\nMOBILE_CHANNELS = 112 \nTOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1920 + 112 = 2032\nNUM_CLASSES = 2 \n\nclass KerasStyleFusion(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, dense_out_channels=DENSE_CHANNELS, mobile_out_channels=MOBILE_CHANNELS):\n        super().__init__()\n        \n        self.densenet_base = DenseNetExtractor(pretrained=True)\n        self.mobilenet_base = MobileNetExtractor(pretrained=True)\n        \n        for param in self.densenet_base.parameters():\n            param.requires_grad = False\n        for param in self.mobilenet_base.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        in_features = dense_out_channels + mobile_out_channels\n        \n        # CORRECTED: The first linear layer now correctly expects 2032 input features.\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features, 512), # nn.Linear(2032, 512)\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes) \n        )\n        \n        self.use_seg = False \n        self.domain_head = nn.Identity()\n        \n    def forward(self, x, grl_lambda=0.0):\n        densenet_feats = self.densenet_base(x)[-1]\n        x1 = self.global_pool(densenet_feats).view(x.size(0), -1)\n\n        mobilenet_feats = self.mobilenet_base(x)[-1]\n        x2 = self.global_pool(mobilenet_feats).view(x.size(0), -1)\n\n        concatenated_features = torch.cat([x1, x2], dim=1)\n\n        logits = self.classifier(concatenated_features)\n        \n        out = {\"logits\": logits, \"feat\": concatenated_features}\n        out[\"domain_logits\"] = torch.randn(x.size(0), 2).to(x.device) \n        out[\"seg\"] = torch.randn(x.size(0), 1, IMG_SIZE, IMG_SIZE).to(x.device)\n        \n        return out\n\n# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n# --- Scenario 1 Changes ---\n# 1. 80/20 Split of testing-dataset remains the same\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n\n# 2. Training includes basic sets + 80% of testing-dataset\n    ALL_TRAIN_TXTS = [\n        train_cam_txt2, train_noncam_txt2,\n    ]\n\n# 2. Validation uses CAMO-COCO test text files ONLY\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2]\n\n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n        testing_image_paths=train_paths, testing_labels=train_labels,\n        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n    )\n\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS,  \n        txt_files=ALL_VAL_TXTS,                 \n        testing_image_paths=val_paths,          \n        testing_labels=val_labels,              \n        weak_transform=val_tf, \n        strong_transform=None, \n        use_masks=USE_SEGMENTATION\n    )\n\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\nmodel = KerasStyleFusion().to(device)\n\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2}, \n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\nprint(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS+1):\n    # --- Freeze/Unfreeze Logic ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        imgs = weak_imgs\n\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n            \n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    if n_batches % ACCUMULATION_STEPS != 0:\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad()\n\n    scheduler.step()\n\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:10:39.20556Z","iopub.execute_input":"2026-01-07T15:10:39.206121Z","iopub.status.idle":"2026-01-07T15:12:23.08651Z","shell.execute_reply.started":"2026-01-07T15:10:39.206083Z","shell.execute_reply":"2026-01-07T15:12:23.085468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nBATCH_SIZE = 8         \nEPOCHS = 15          \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 7\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\n\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\nclass DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n\nclass InceptionExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # We load the full model but only use the features\n        inception = models.inception_v3(weights='IMAGENET1K_V1' if pretrained else None)\n        inception.transform_input = True # Normalizes internally for Inception\n        \n        # Extract features (everything except the final pooling and FC)\n        self.features = nn.Sequential(\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            inception.Conv2d_3b_1x1,\n            inception.Conv2d_4a_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            inception.Mixed_5b, inception.Mixed_5c, inception.Mixed_5d,\n            inception.Mixed_6a, inception.Mixed_6b, inception.Mixed_6c,\n            inception.Mixed_6d, inception.Mixed_6e,\n            inception.Mixed_7a, inception.Mixed_7b, inception.Mixed_7c\n        )\n\n    def forward(self, x):\n        # InceptionV3 expects 299x299. If input is different, we interpolate.\n        if x.shape[-1] != 299:\n            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n        \n        x = self.features(x)\n        return [x] # Returning as list to maintain compatibility with your loop\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\nIMG_SIZE = 299  # Updated for InceptionV3\n# ... other hyperparameters remain same ...\nINCEPTION_CHANNELS = 2048 # InceptionV3 final feature count\nDENSE_CHANNELS = 1920\nTOTAL_FEATURES = DENSE_CHANNELS + INCEPTION_CHANNELS\n\nNUM_CLASSES = 2 \n\nclass KerasStyleFusion(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        \n        self.densenet_base = DenseNetExtractor(pretrained=True)\n        self.inception_base = InceptionExtractor(pretrained=True)\n        \n        # Freeze backbones initially\n        for param in self.densenet_base.parameters():\n            param.requires_grad = False\n        for param in self.inception_base.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        # 1920 (DenseNet) + 2048 (Inception) = 3968\n        in_features = DENSE_CHANNELS + INCEPTION_CHANNELS\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes) \n        )\n        \n    def forward(self, x):\n        # Get DenseNet features (expects 224, but works at 299)\n        d_feats = self.densenet_base(x)[-1]\n        x1 = self.global_pool(d_feats).view(x.size(0), -1)\n\n        # Get Inception features (expects 299)\n        i_feats = self.inception_base(x)[-1]\n        x2 = self.global_pool(i_feats).view(x.size(0), -1)\n\n        concatenated_features = torch.cat([x1, x2], dim=1)\n        logits = self.classifier(concatenated_features)\n        \n        return {\n            \"logits\": logits, \n            \"feat\": concatenated_features,\n            \"seg\": torch.randn(x.size(0), 1, x.shape[-2], x.shape[-1]).to(x.device) # Placeholder\n        }\n\n# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n# --- Scenario 1 Changes ---\n# 1. 80/20 Split of testing-dataset remains the same\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.2, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n\n# 2. Training includes basic sets + 80% of testing-dataset\n    ALL_TRAIN_TXTS = [\n        train_cam_txt2, train_noncam_txt2,\n    ]\n\n# 2. Validation uses CAMO-COCO test text files ONLY\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2]\n\n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, txt_files=ALL_TRAIN_TXTS,\n        testing_image_paths=train_paths, testing_labels=train_labels,\n        weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION\n    )\n\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS,  \n        txt_files=ALL_VAL_TXTS,                 \n        testing_image_paths=val_paths,          \n        testing_labels=val_labels,              \n        weak_transform=val_tf, \n        strong_transform=None, \n        use_masks=USE_SEGMENTATION\n    )\n\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\nmodel = KerasStyleFusion().to(device)\n\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    # Updated key check for Inception\n    if any(k in name for k in ['densenet_base', 'inception_base']):\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.1}, # Lower LR for heavy backbones\n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\nprint(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS+1):\n    # --- Freeze/Unfreeze Logic ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        imgs = weak_imgs\n\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n            \n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    if n_batches % ACCUMULATION_STEPS != 0:\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad()\n\n    scheduler.step()\n\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:17:48.676293Z","iopub.execute_input":"2026-01-07T15:17:48.676955Z","iopub.status.idle":"2026-01-07T15:21:22.98209Z","shell.execute_reply.started":"2026-01-07T15:17:48.676915Z","shell.execute_reply":"2026-01-07T15:21:22.981124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 15          \nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 7\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\n\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\n# --- UPDATED CONSTANTS ---\n# DenseNet169 final features: 1664\n# MobileNetV3 Large final features before global pool: 112 (or 160/960 depending on layer, \n# but based on your previous code 112 is the expected mid-layer output)\nDENSE_CHANNELS = 1664  \nMOBILE_CHANNELS = 112  \nTOTAL_FEATURES = DENSE_CHANNELS + MOBILE_CHANNELS # 1664 + 112 = 1776\n\nclass DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # Changed from densenet201 to densenet169\n        self.features = models.densenet169(weights='IMAGENET1K_V1' if pretrained else None).features\n        \n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            # The block names remain the same across DenseNet variants\n            if name in [\"denseblock1\", \"denseblock2\", \"denseblock3\", \"denseblock4\"]:\n                feats.append(x)\n        return feats\n\nclass MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):\n                feats.append(out)\n        if len(feats) < 4:\n            feats.append(out)\n        return feats\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\n\nNUM_CLASSES = 2 \n\nclass KerasStyleFusion(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, dense_out_channels=DENSE_CHANNELS, mobile_out_channels=MOBILE_CHANNELS):\n        super().__init__()\n        \n        self.densenet_base = DenseNetExtractor(pretrained=True)\n        self.mobilenet_base = MobileNetExtractor(pretrained=True)\n        \n        # Freezing logic\n        for param in self.densenet_base.parameters():\n            param.requires_grad = False\n        for param in self.mobilenet_base.parameters():\n            param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        in_features = dense_out_channels + mobile_out_channels\n        \n        # Input features are now 1776 (1664 + 112)\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features, 512), \n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes) \n        )\n        \n    def forward(self, x, grl_lambda=0.0):\n        # Extract features from DenseNet169\n        densenet_feats = self.densenet_base(x)[-1]\n        x1 = self.global_pool(densenet_feats).view(x.size(0), -1)\n\n        # Extract features from MobileNetV3\n        mobilenet_feats = self.mobilenet_base(x)[-1]\n        x2 = self.global_pool(mobilenet_feats).view(x.size(0), -1)\n\n        # Fusion\n        concatenated_features = torch.cat([x1, x2], dim=1)\n\n        logits = self.classifier(concatenated_features)\n        \n        # Maintaining your dictionary output structure\n        out = {\"logits\": logits, \"feat\": concatenated_features}\n        out[\"domain_logits\"] = torch.zeros(x.size(0), 2).to(x.device) \n        out[\"seg\"] = torch.zeros(x.size(0), 1, IMG_SIZE, IMG_SIZE).to(x.device)\n        \n        return out\n\n# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.8, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n    ALL_TRAIN_TXTS = [\n        train_cam_txt2, train_noncam_txt2,\n    ]\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2]\n    \n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, \n        txt_files=ALL_TRAIN_TXTS,               \n        testing_image_paths=train_paths,        \n        testing_labels=train_labels,            \n        weak_transform=weak_tf, \n        strong_transform=strong_tf, \n        use_masks=USE_SEGMENTATION\n    )\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS,  \n        txt_files=ALL_VAL_TXTS,                 \n        testing_image_paths=val_paths,          \n        testing_labels=val_labels,              \n        weak_transform=val_tf, \n        strong_transform=None, \n        use_masks=USE_SEGMENTATION\n    )\n\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\nmodel = KerasStyleFusion().to(device)\n\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2}, \n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\nprint(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS+1):\n    # --- Freeze/Unfreeze Logic ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        imgs = weak_imgs\n\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n            \n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    if n_batches % ACCUMULATION_STEPS != 0:\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad()\n\n    scheduler.step()\n\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T15:21:26.109271Z","iopub.execute_input":"2026-01-07T15:21:26.109597Z","iopub.status.idle":"2026-01-07T15:22:49.310049Z","shell.execute_reply.started":"2026-01-07T15:21:26.109562Z","shell.execute_reply":"2026-01-07T15:22:49.308894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm \nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom collections import Counter\nfrom torch.autograd import Function\n\n# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8         \nEPOCHS = 15\nNUM_WORKERS = 4         \nLR = 3e-4              \nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True \n\n# Loss weights \nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# Warmup epochs and accumulation steps\nWARMUP_EPOCHS = 5\nEARLY_STOPPING_PATIENCE = 7\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4\n\n# --- 2. TRANSFORMS (From Notebook Cell 6) ---\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\n# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    image_paths = []\n    labels = []\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break\n        except UnicodeDecodeError:\n            pass\n        except Exception as e:\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue\n                \n            label = 1 if label == 1 else 0\n            image_full_path = os.path.join(image_dir, image_filename)\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                self.samples.append((img_path, label)) \n        \n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            lbl = 1 if lbl == 1 else 0\n            base_fname = os.path.basename(fname)  \n\n            found = False\n            search_subs = [\n                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n                \"Images/Train\", \"Images/Test\",\n            ]\n            \n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        if len(sample) == 3:\n            rdir = sample[2]\n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n           \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except:\n            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n            \n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\n# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n\n# --- UPDATED CONSTANTS ---\n# DenseNet169 final features: 1664\n# MobileNetV3 Large final features before global pool: 112 (or 160/960 depending on layer, \n# but based on your previous code 112 is the expected mid-layer output)\n# --- UPDATED CONSTANTS FOR TRIPLE FUSION ---\n# Final feature maps after Global Average Pooling:\nDENSE_CHANNELS = 1920      # DenseNet201\nINCEPTION_CHANNELS = 2048  # InceptionV3\nEFFICIENT_CHANNELS = 1280  # EfficientNetV2-S (timm default)\n\nTOTAL_FEATURES = DENSE_CHANNELS + INCEPTION_CHANNELS + EFFICIENT_CHANNELS \n\nclass DenseNet201Extractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n    def forward(self, x):\n        return self.features(x)\n\nclass InceptionExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # aux_logits=False is necessary to simplify the output to just features\n        self.model = models.inception_v3(weights='IMAGENET1K_V1' if pretrained else None, aux_logits=True)\n    def forward(self, x):\n        # InceptionV3 requires 299x299 ideally, but will work with 224x224\n        # We extract features before the final FC layer\n        x = self.model.Conv2d_1a_3x3(x)\n        x = self.model.Conv2d_2a_3x3(x)\n        x = self.model.Conv2d_2b_3x3(x)\n        x = self.model.maxpool1(x)\n        x = self.model.Conv2d_3b_1x1(x)\n        x = self.model.Conv2d_4a_3x3(x)\n        x = self.model.maxpool2(x)\n        x = self.model.Mixed_5b(x)\n        x = self.model.Mixed_5c(x)\n        x = self.model.Mixed_5d(x)\n        x = self.model.Mixed_6a(x)\n        x = self.model.Mixed_6b(x)\n        x = self.model.Mixed_6c(x)\n        x = self.model.Mixed_6d(x)\n        x = self.model.Mixed_6e(x)\n        x = self.model.Mixed_7a(x)\n        x = self.model.Mixed_7b(x)\n        x = self.model.Mixed_7c(x)\n        return x\n\nclass EfficientNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        # Using timm for a modern EfficientNetV2 implementation\n        self.model = timm.create_model('tf_efficientnetv2_s', pretrained=pretrained, num_classes=0, global_pool='')\n    def forward(self, x):\n        return self.model(x)\n\n\n\n# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n\n\nNUM_CLASSES = 2 \n\nclass TripleFusionModel(nn.Module):\n    def __init__(self, num_classes=2):\n        super().__init__()\n        \n        self.densenet_base = DenseNet201Extractor(pretrained=True)\n        self.inception_base = InceptionExtractor(pretrained=True)\n        self.efficient_base = EfficientNetExtractor(pretrained=True)\n        \n        # Freezing logic for the first phase\n        for base in [self.densenet_base, self.inception_base, self.efficient_base]:\n            for param in base.parameters():\n                param.requires_grad = False\n            \n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        # Classifier input: 1920 + 2048 + 1280 = 5248\n        self.classifier = nn.Sequential(\n            nn.Linear(TOTAL_FEATURES, 1024), \n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes) \n        )\n        \n    def forward(self, x):\n        # 1. Feature Extraction\n        feat_dense = self.densenet_base(x)\n        feat_inc = self.inception_base(x)\n        feat_eff = self.efficient_base(x)\n\n        # 2. Global Average Pooling & Flattening\n        x1 = self.global_pool(feat_dense).view(x.size(0), -1)\n        x2 = self.global_pool(feat_inc).view(x.size(0), -1)\n        x3 = self.global_pool(feat_eff).view(x.size(0), -1)\n\n        # 3. Concatenation (Triple Fusion)\n        merged = torch.cat([x1, x2, x3], dim=1)\n\n        # 4. Classification\n        logits = self.classifier(merged)\n        \n        return {\n            \"logits\": logits, \n            \"feat\": merged,\n            \"domain_logits\": torch.zeros(x.size(0), 2).to(x.device),\n            \"seg\": torch.zeros(x.size(0), 1, IMG_SIZE, IMG_SIZE).to(x.device)\n        }\n\n# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n\n# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\n\n# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\n\n# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\ntry:\n    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        testing_image_paths, testing_labels, test_size=0.8, random_state=SEED\n    )\n    ALL_ROOT_DIRS = [\n        train_dir_cod,       \n        test_dir_cod,       \n        train_dir_camo_cam,  \n        train_dir_camo_noncam\n    ]\n    ALL_TRAIN_TXTS = [\n        train_cam_txt2, train_noncam_txt2,\n    ]\n    ALL_VAL_TXTS = [test_cam_txt2, test_noncam_txt2]\n    \n    train_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS, \n        txt_files=ALL_TRAIN_TXTS,               \n        testing_image_paths=train_paths,        \n        testing_labels=train_labels,            \n        weak_transform=weak_tf, \n        strong_transform=strong_tf, \n        use_masks=USE_SEGMENTATION\n    )\n    val_ds = MultiDataset(\n        root_dirs=ALL_ROOT_DIRS,  \n        txt_files=ALL_VAL_TXTS,                 \n        testing_image_paths=val_paths,          \n        testing_labels=val_labels,              \n        weak_transform=val_tf, \n        strong_transform=None, \n        use_masks=USE_SEGMENTATION\n    )\n\n    def build_weighted_sampler(dataset):\n        labels = [sample[1] for sample in dataset.samples]  \n        counts = Counter(labels)\n        total = len(labels)\n        if len(counts) <= 1:\n            weights = [1.0] * total\n        else:\n            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n            weights = [class_weights[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n    train_sampler = build_weighted_sampler(train_ds)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nexcept RuntimeError as e:\n    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n    \n    class MockDataset(Dataset):\n        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n            self.num_samples = num_samples\n            self.data = torch.randn(num_samples, 3, img_size, img_size)\n            self.labels = torch.randint(0, num_classes, (num_samples,))\n            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n\n    train_ds = MockDataset(num_samples=14150) \n    val_ds = MockDataset(num_samples=6606)   \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n\n\n# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n\nmodel = TripleFusionModel(num_classes=2).to(device)\n\n# Update parameter groups for Optimizer\nbackbone_names = ['densenet_base', 'inception_base', 'efficient_base']\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if any(k in name for k in backbone_names):\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR / 10}, # Backbones usually need a smaller LR\n    {'params': head_params, 'lr': LR}\n], weight_decay=1e-4)\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\nprint(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\nprint(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n\n# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\nfor epoch in range(1, EPOCHS+1):\n    # --- Freeze/Unfreeze Logic ---\n    if epoch <= FREEZE_EPOCHS:\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n                p.requires_grad = False\n    if epoch == FREEZE_EPOCHS + 1:\n        print(f\"--- Unfreezing all 3 backbones at epoch {epoch} ---\")\n        for name, p in model.named_parameters():\n            if any(k in name for k in backbone_names):\n                p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        imgs = weak_imgs\n\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n            out = model(imgs) \n            logits = out[\"logits\"]\n            \n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n            seg_loss = 0.0\n            supcon_loss = 0.0 \n            cons_loss = 0.0   \n            dom_loss = 0.0\n\n            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n            \n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        scaler.scale(total_loss).backward()\n\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() \n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    if n_batches % ACCUMULATION_STEPS != 0:\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad()\n\n    scheduler.step()\n\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            \n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}