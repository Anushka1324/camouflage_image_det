{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e28098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T05:27:18.090117Z",
     "iopub.status.busy": "2026-01-08T05:27:18.089814Z",
     "iopub.status.idle": "2026-01-08T06:14:59.758905Z",
     "shell.execute_reply": "2026-01-08T06:14:59.757991Z"
    },
    "papermill": {
     "duration": 2861.945799,
     "end_time": "2026-01-08T06:15:00.031231",
     "exception": false,
     "start_time": "2026-01-08T05:27:18.085432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "✅ Loaded 3651 samples from 4 root directories.\n",
      "✅ Loaded 6606 samples from 4 root directories.\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77.4M/77.4M [00:00<00:00, 167MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104M/104M [00:00<00:00, 208MB/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86301d62c7f6431b99220fce430e0e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/3173890725.py:596: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model instantiated. LR: 0.0003, Epochs: 15.\n",
      "Backbones are initially frozen for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/15: 100%|██████████| 457/457 [01:02<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.7162 Acc: 0.5021 Prec: 0.5168 Rec: 0.5122 F1: 0.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val Loss: 1.5509 Acc: 0.4864 Prec: 0.4257 Rec: 0.4849 F1: 0.3575\n",
      "Saved best model at epoch 1 (F1 0.3575)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 2/15: 100%|██████████| 457/457 [00:54<00:00,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.5255 Acc: 0.7623 Prec: 0.7622 Rec: 0.7623 F1: 0.7622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Val Loss: 1.2131 Acc: 0.9976 Prec: 0.9976 Rec: 0.9976 F1: 0.9976\n",
      "Saved best model at epoch 2 (F1 0.9976)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 3/15: 100%|██████████| 457/457 [00:54<00:00,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.4908 Acc: 0.7869 Prec: 0.7869 Rec: 0.7869 F1: 0.7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Val Loss: 1.1878 Acc: 0.9974 Prec: 0.9974 Rec: 0.9974 F1: 0.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 4/15: 100%|██████████| 457/457 [00:54<00:00,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.4851 Acc: 0.7946 Prec: 0.7946 Rec: 0.7946 F1: 0.7946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Val Loss: 1.1987 Acc: 0.9920 Prec: 0.9921 Rec: 0.9920 F1: 0.9920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 5/15: 100%|██████████| 457/457 [00:55<00:00,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.4774 Acc: 0.8042 Prec: 0.8042 Rec: 0.8041 F1: 0.8041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Val Loss: 1.1934 Acc: 0.9956 Prec: 0.9957 Rec: 0.9956 F1: 0.9956\n",
      "--- Unfreezing all 3 backbones at epoch 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 6/15: 100%|██████████| 457/457 [02:11<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.4749 Acc: 0.8066 Prec: 0.8066 Rec: 0.8066 F1: 0.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Val Loss: 1.1876 Acc: 0.9947 Prec: 0.9948 Rec: 0.9947 F1: 0.9947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 7/15: 100%|██████████| 457/457 [02:03<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.4634 Acc: 0.8192 Prec: 0.8191 Rec: 0.8192 F1: 0.8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Val Loss: 1.1997 Acc: 0.9982 Prec: 0.9982 Rec: 0.9982 F1: 0.9982\n",
      "Saved best model at epoch 7 (F1 0.9982)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 8/15: 100%|██████████| 457/457 [02:02<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.4590 Acc: 0.8195 Prec: 0.8195 Rec: 0.8195 F1: 0.8195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Val Loss: 1.1882 Acc: 0.9983 Prec: 0.9983 Rec: 0.9983 F1: 0.9983\n",
      "Saved best model at epoch 8 (F1 0.9983)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 9/15: 100%|██████████| 457/457 [02:01<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.4460 Acc: 0.8461 Prec: 0.8459 Rec: 0.8459 F1: 0.8459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Val Loss: 1.1915 Acc: 0.9970 Prec: 0.9970 Rec: 0.9970 F1: 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 10/15: 100%|██████████| 457/457 [02:03<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.4385 Acc: 0.8359 Prec: 0.8359 Rec: 0.8359 F1: 0.8359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Val Loss: 1.1905 Acc: 0.9983 Prec: 0.9983 Rec: 0.9983 F1: 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 11/15: 100%|██████████| 457/457 [02:01<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.4455 Acc: 0.8190 Prec: 0.8192 Rec: 0.8191 F1: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Val Loss: 1.1947 Acc: 0.9979 Prec: 0.9979 Rec: 0.9979 F1: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 12/15: 100%|██████████| 457/457 [02:02<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.4385 Acc: 0.8376 Prec: 0.8376 Rec: 0.8375 F1: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Val Loss: 1.1912 Acc: 0.9982 Prec: 0.9982 Rec: 0.9982 F1: 0.9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 13/15: 100%|██████████| 457/457 [02:03<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.4316 Acc: 0.8261 Prec: 0.8262 Rec: 0.8261 F1: 0.8261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Val Loss: 1.1913 Acc: 0.9985 Prec: 0.9985 Rec: 0.9985 F1: 0.9985\n",
      "Saved best model at epoch 13 (F1 0.9985)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 14/15: 100%|██████████| 457/457 [02:02<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.4304 Acc: 0.8332 Prec: 0.8333 Rec: 0.8331 F1: 0.8331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Val Loss: 1.1918 Acc: 0.9986 Prec: 0.9986 Rec: 0.9986 F1: 0.9986\n",
      "Saved best model at epoch 14 (F1 0.9986)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 15/15: 100%|██████████| 457/457 [02:02<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.4314 Acc: 0.8381 Prec: 0.8384 Rec: 0.8383 F1: 0.8381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Val Loss: 1.1908 Acc: 0.9986 Prec: 0.9986 Rec: 0.9986 F1: 0.9986\n",
      "\n",
      "Training finished. Best val F1: 0.9986375856644882 at epoch 14\n"
     ]
    }
   ],
   "source": [
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import RandAugment\n",
    "import timm \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "from torch.autograd import Function\n",
    "\n",
    "# --- 1. SETUP AND HYPERPARAMETERS (From Notebook Cell 4 & 5) ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8         \n",
    "EPOCHS = 15\n",
    "NUM_WORKERS = 4         \n",
    "LR = 3e-4              \n",
    "LABEL_SMOOTH = 0.1\n",
    "SAVE_PATH = \"best_model.pth\"\n",
    "USE_SEGMENTATION = True \n",
    "\n",
    "# Loss weights \n",
    "ALPHA_DOM = 0.5\n",
    "BETA_SUPCON = 0.3\n",
    "ETA_CONS = 0.1\n",
    "GAMMA_SEG = 0.5\n",
    "\n",
    "# Mixup/CutMix probabilities and alphas\n",
    "PROB_MIXUP = 0.5\n",
    "PROB_CUTMIX = 0.5\n",
    "MIXUP_ALPHA = 0.2\n",
    "CUTMIX_ALPHA = 1.0\n",
    "\n",
    "# Warmup epochs and accumulation steps\n",
    "WARMUP_EPOCHS = 5\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "FREEZE_EPOCHS = 5\n",
    "ACCUMULATION_STEPS = 4\n",
    "\n",
    "# --- 2. TRANSFORMS (From Notebook Cell 6) ---\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.05):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        noisy_tensor = tensor + noise\n",
    "        return torch.clamp(noisy_tensor, 0., 1.)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
    "\n",
    "weak_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.02),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "strong_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.05),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# --- 3. DATASET HELPER FUNCTIONS (From Notebook Cell 7) ---\n",
    "def read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n",
    "\n",
    "def load_testing_dataset_info(info_file, image_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n",
    "    lines = []\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(info_file, 'r', encoding=encoding) as f:\n",
    "                lines = f.readlines()\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            raise \n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            image_filename = parts[0]\n",
    "            try:\n",
    "                label = int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            label = 1 if label == 1 else 0\n",
    "            image_full_path = os.path.join(image_dir, image_filename)\n",
    "            image_paths.append(image_full_path)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# --- 4. MULTIDATASET CLASS (From Notebook Cell 7) ---\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n",
    "        self.root_dirs = root_dirs\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "        self.use_masks = use_masks\n",
    "        self.samples = []\n",
    "\n",
    "        if testing_image_paths is not None and testing_labels is not None:\n",
    "            for img_path, label in zip(testing_image_paths, testing_labels):\n",
    "                self.samples.append((img_path, label)) \n",
    "        \n",
    "        if isinstance(txt_files, str):\n",
    "            txt_files = [txt_files]\n",
    "\n",
    "        all_lines = []\n",
    "        for t in txt_files:\n",
    "            if not os.path.exists(t):\n",
    "                raise RuntimeError(f\"TXT file not found: {t}. Please ensure all Kaggle input datasets are mounted.\")\n",
    "\n",
    "            lines = read_file_with_encoding(t)\n",
    "            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n",
    "\n",
    "        for line, src_txt in all_lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 0:\n",
    "                continue\n",
    "\n",
    "            fname = parts[0]\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    lbl = int(parts[1])\n",
    "                except:\n",
    "                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            else:\n",
    "                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n",
    "            \n",
    "            lbl = 1 if lbl == 1 else 0\n",
    "            base_fname = os.path.basename(fname)  \n",
    "\n",
    "            found = False\n",
    "            search_subs = [\n",
    "                \"\", \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", \n",
    "                \"Images/Train\", \"Images/Test\",\n",
    "            ]\n",
    "            \n",
    "            for rdir in self.root_dirs:\n",
    "                for sub in search_subs:\n",
    "                    img_path = os.path.join(rdir, sub, base_fname)\n",
    "                    if os.path.exists(img_path):\n",
    "                        self.samples.append((img_path, lbl, rdir))\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        global IMG_SIZE \n",
    "        \n",
    "        sample = self.samples[idx]\n",
    "        img_path = sample[0]\n",
    "        lbl = sample[1]\n",
    "        \n",
    "        if len(sample) == 3:\n",
    "            rdir = sample[2]\n",
    "        else:\n",
    "            rdir = os.path.dirname(os.path.dirname(img_path))\n",
    "           \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n",
    "            \n",
    "        if self.weak_transform:\n",
    "            weak = self.weak_transform(img)\n",
    "        else:\n",
    "            weak = transforms.ToTensor()(img)\n",
    "            \n",
    "        if self.strong_transform:\n",
    "            strong = self.strong_transform(img)\n",
    "        else:\n",
    "            strong = weak.clone()\n",
    "\n",
    "        mask = None\n",
    "        if self.use_masks:\n",
    "            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n",
    "            found_mask = False\n",
    "            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n",
    "                mask_path = os.path.join(rdir, mask_dir, mask_name)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n",
    "                    m = np.array(m).astype(np.float32) / 255.0\n",
    "                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n",
    "                    found_mask = True\n",
    "                    break\n",
    "\n",
    "            if mask is None:\n",
    "                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n",
    "                \n",
    "        return weak, strong, lbl, mask\n",
    "\n",
    "# --- 5. BACKBONE EXTRACTORS (From Notebook Cell 9) ---\n",
    "\n",
    "# --- UPDATED CONSTANTS ---\n",
    "# DenseNet169 final features: 1664\n",
    "# MobileNetV3 Large final features before global pool: 112 (or 160/960 depending on layer, \n",
    "# but based on your previous code 112 is the expected mid-layer output)\n",
    "# --- UPDATED CONSTANTS FOR TRIPLE FUSION ---\n",
    "# Final feature maps after Global Average Pooling:\n",
    "DENSE_CHANNELS = 1920      # DenseNet201\n",
    "INCEPTION_CHANNELS = 2048  # InceptionV3\n",
    "EFFICIENT_CHANNELS = 1280  # EfficientNetV2-S (timm default)\n",
    "\n",
    "TOTAL_FEATURES = DENSE_CHANNELS + INCEPTION_CHANNELS + EFFICIENT_CHANNELS \n",
    "\n",
    "class DenseNet201Extractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.densenet201(weights='IMAGENET1K_V1' if pretrained else None).features\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class InceptionExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # aux_logits=False is necessary to simplify the output to just features\n",
    "        self.model = models.inception_v3(weights='IMAGENET1K_V1' if pretrained else None, aux_logits=True)\n",
    "    def forward(self, x):\n",
    "        # InceptionV3 requires 299x299 ideally, but will work with 224x224\n",
    "        # We extract features before the final FC layer\n",
    "        x = self.model.Conv2d_1a_3x3(x)\n",
    "        x = self.model.Conv2d_2a_3x3(x)\n",
    "        x = self.model.Conv2d_2b_3x3(x)\n",
    "        x = self.model.maxpool1(x)\n",
    "        x = self.model.Conv2d_3b_1x1(x)\n",
    "        x = self.model.Conv2d_4a_3x3(x)\n",
    "        x = self.model.maxpool2(x)\n",
    "        x = self.model.Mixed_5b(x)\n",
    "        x = self.model.Mixed_5c(x)\n",
    "        x = self.model.Mixed_5d(x)\n",
    "        x = self.model.Mixed_6a(x)\n",
    "        x = self.model.Mixed_6b(x)\n",
    "        x = self.model.Mixed_6c(x)\n",
    "        x = self.model.Mixed_6d(x)\n",
    "        x = self.model.Mixed_6e(x)\n",
    "        x = self.model.Mixed_7a(x)\n",
    "        x = self.model.Mixed_7b(x)\n",
    "        x = self.model.Mixed_7c(x)\n",
    "        return x\n",
    "\n",
    "class EfficientNetExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Using timm for a modern EfficientNetV2 implementation\n",
    "        self.model = timm.create_model('tf_efficientnetv2_s', pretrained=pretrained, num_classes=0, global_pool='')\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# --- 6. USER'S Keras-Style Fusion Model (Corrected) ---\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "class TripleFusionModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.densenet_base = DenseNet201Extractor(pretrained=True)\n",
    "        self.inception_base = InceptionExtractor(pretrained=True)\n",
    "        self.efficient_base = EfficientNetExtractor(pretrained=True)\n",
    "        \n",
    "        # Freezing logic for the first phase\n",
    "        for base in [self.densenet_base, self.inception_base, self.efficient_base]:\n",
    "            for param in base.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Classifier input: 1920 + 2048 + 1280 = 5248\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(TOTAL_FEATURES, 1024), \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Feature Extraction\n",
    "        feat_dense = self.densenet_base(x)\n",
    "        feat_inc = self.inception_base(x)\n",
    "        feat_eff = self.efficient_base(x)\n",
    "\n",
    "        # 2. Global Average Pooling & Flattening\n",
    "        x1 = self.global_pool(feat_dense).view(x.size(0), -1)\n",
    "        x2 = self.global_pool(feat_inc).view(x.size(0), -1)\n",
    "        x3 = self.global_pool(feat_eff).view(x.size(0), -1)\n",
    "\n",
    "        # 3. Concatenation (Triple Fusion)\n",
    "        merged = torch.cat([x1, x2, x3], dim=1)\n",
    "\n",
    "        # 4. Classification\n",
    "        logits = self.classifier(merged)\n",
    "        \n",
    "        return {\n",
    "            \"logits\": logits, \n",
    "            \"feat\": merged,\n",
    "            \"domain_logits\": torch.zeros(x.size(0), 2).to(x.device),\n",
    "            \"seg\": torch.zeros(x.size(0), 1, IMG_SIZE, IMG_SIZE).to(x.device)\n",
    "        }\n",
    "\n",
    "# --- 7. LOSS AND ADVERSARIAL HELPERS (From Notebook Cells 17-19) ---\n",
    "\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.s = smoothing\n",
    "    def forward(self, logits, target):\n",
    "        c = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logp)\n",
    "            true_dist.fill_(self.s / (c - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n",
    "        return (-true_dist * logp).sum(dim=-1).mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, target):\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        ce = F.cross_entropy(logits, target, reduction='none')\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()\n",
    "\n",
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    num = 2 * (pred * target).sum() + smooth\n",
    "    den = pred.sum() + target.sum() + smooth\n",
    "    return 1 - (num / den)\n",
    "\n",
    "def seg_loss_fn(pred, mask):\n",
    "    if pred.shape[-2:] != mask.shape[-2:]:\n",
    "        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        f = F.normalize(features, dim=1)\n",
    "        sim = torch.matmul(f, f.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n",
    "        logits = sim - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n",
    "        denom = exp_logits.sum(1, keepdim=True)\n",
    "        pos_mask = mask - torch.eye(len(features), device=device)\n",
    "        pos_exp = (exp_logits * pos_mask).sum(1)\n",
    "        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n",
    "        valid = (pos_mask.sum(1) > 0).float()\n",
    "        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n",
    "        return loss\n",
    "\n",
    "clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n",
    "clf_loss_focal = FocalLoss(gamma=1.5)\n",
    "supcon_loss_fn = SupConLoss(temperature=0.07)\n",
    "\n",
    "class GradReverse(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, l):\n",
    "        ctx.l = l\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.l, None\n",
    "\n",
    "def grad_reverse(x, l=1.0):\n",
    "    return GradReverse.apply(x, l)\n",
    "\n",
    "\n",
    "# --- 8. DATA AUGMENTATION HELPERS (From Notebook Cell 20) ---\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    new_x = x.clone()\n",
    "    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return new_x, y, y[idx], lam_adjusted\n",
    "\n",
    "\n",
    "# --- 9. DATA LOADING SETUP (From Notebook Cell 8) ---\n",
    "info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n",
    "train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \n",
    "test_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n",
    "    \n",
    "train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n",
    "train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n",
    "test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n",
    "test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n",
    "\n",
    "info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n",
    "train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n",
    "train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n",
    "test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n",
    "test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n",
    "\n",
    "train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n",
    "train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n",
    "\n",
    "testing_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\n",
    "testing_images_dir = \"/kaggle/input/testing-dataset/Images\"\n",
    "\n",
    "# --- MOCKING DATASET LOADING FOR RUNNABILITY ---\n",
    "try:\n",
    "    testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        testing_image_paths, testing_labels, test_size=0.8, random_state=SEED\n",
    "    )\n",
    "    ALL_ROOT_DIRS = [\n",
    "        train_dir_cod,       \n",
    "        test_dir_cod,       \n",
    "        train_dir_camo_cam,  \n",
    "        train_dir_camo_noncam\n",
    "    ]\n",
    "    ALL_TRAIN_TXTS = [\n",
    "        train_cam_txt2, train_noncam_txt2,\n",
    "    ]\n",
    "    ALL_VAL_TXTS = []\n",
    "    \n",
    "    train_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS, \n",
    "        txt_files=ALL_TRAIN_TXTS,               \n",
    "        testing_image_paths=train_paths,        \n",
    "        testing_labels=train_labels,            \n",
    "        weak_transform=weak_tf, \n",
    "        strong_transform=strong_tf, \n",
    "        use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "    val_ds = MultiDataset(\n",
    "        root_dirs=ALL_ROOT_DIRS,  \n",
    "        txt_files=ALL_VAL_TXTS,                 \n",
    "        testing_image_paths=val_paths,          \n",
    "        testing_labels=val_labels,              \n",
    "        weak_transform=val_tf, \n",
    "        strong_transform=None, \n",
    "        use_masks=USE_SEGMENTATION\n",
    "    )\n",
    "\n",
    "    def build_weighted_sampler(dataset):\n",
    "        labels = [sample[1] for sample in dataset.samples]  \n",
    "        counts = Counter(labels)\n",
    "        total = len(labels)\n",
    "        if len(counts) <= 1:\n",
    "            weights = [1.0] * total\n",
    "        else:\n",
    "            class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n",
    "            weights = [class_weights[lbl] for lbl in labels]\n",
    "        return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    train_sampler = build_weighted_sampler(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"⚠️ Warning: Cannot access Kaggle paths ({e}). Using Mock DataLoaders for demonstration.\")\n",
    "    \n",
    "    class MockDataset(Dataset):\n",
    "        def __init__(self, num_samples, num_classes=2, img_size=IMG_SIZE):\n",
    "            self.num_samples = num_samples\n",
    "            self.data = torch.randn(num_samples, 3, img_size, img_size)\n",
    "            self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "            self.masks = torch.randint(0, 2, (num_samples, 1, img_size, img_size)).float()\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx], self.data[idx].clone(), self.labels[idx], self.masks[idx]\n",
    "\n",
    "    train_ds = MockDataset(num_samples=14150) \n",
    "    val_ds = MockDataset(num_samples=6606)   \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Mock DataLoaders initialized with {len(train_ds)} train and {len(val_ds)} val samples.\")\n",
    "\n",
    "\n",
    "# --- 10. MODEL INSTANTIATION AND OPTIMIZER SETUP (From Notebook Cell 20) ---\n",
    "\n",
    "model = TripleFusionModel(num_classes=2).to(device)\n",
    "\n",
    "# Update parameter groups for Optimizer\n",
    "backbone_names = ['densenet_base', 'inception_base', 'efficient_base']\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if any(k in name for k in backbone_names):\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        head_params.append(param)\n",
    "\n",
    "def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch) / float(max(1.0, warmup_epochs))\n",
    "        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': LR / 10}, # Backbones usually need a smaller LR\n",
    "    {'params': head_params, 'lr': LR}\n",
    "], weight_decay=1e-4)\n",
    "scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "\n",
    "print(f\"\\nModel instantiated. LR: {LR}, Epochs: {EPOCHS}.\")\n",
    "print(f\"Backbones are initially frozen for {FREEZE_EPOCHS} epochs.\")\n",
    "\n",
    "# --- 11. TRAINING LOOP (Adapted from Notebook Cell 21) ---\n",
    "\n",
    "def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n",
    "    if mix_info is None:\n",
    "        if use_focal:\n",
    "            return clf_loss_focal(logits, targets)\n",
    "        else:\n",
    "            return clf_loss_ce(logits, targets)\n",
    "    else:\n",
    "        y_a, y_b, lam = mix_info\n",
    "        if use_focal:\n",
    "            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n",
    "        else:\n",
    "            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n",
    "        return loss\n",
    "\n",
    "best_vf1 = 0.0\n",
    "best_epoch = 0\n",
    "patience_count = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # --- Freeze/Unfreeze Logic ---\n",
    "    if epoch <= FREEZE_EPOCHS:\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in ['densenet_base', 'mobilenet_base']):\n",
    "                p.requires_grad = False\n",
    "    if epoch == FREEZE_EPOCHS + 1:\n",
    "        print(f\"--- Unfreezing all 3 backbones at epoch {epoch} ---\")\n",
    "        for name, p in model.named_parameters():\n",
    "            if any(k in name for k in backbone_names):\n",
    "                p.requires_grad = True\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    n_batches = 0\n",
    "    opt.zero_grad() \n",
    "    \n",
    "    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n",
    "        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if masks is not None:\n",
    "            masks = masks.to(device)\n",
    "\n",
    "        imgs = weak_imgs\n",
    "\n",
    "        mix_info = None\n",
    "        rand = random.random()\n",
    "        if rand < PROB_MIXUP:\n",
    "            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "        elif rand < PROB_MIXUP + PROB_CUTMIX:\n",
    "            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n",
    "            mix_info = (y_a.to(device), y_b.to(device), lam)\n",
    "\n",
    "        with torch.autocast(device_type=device, enabled=(device==\"cuda\")):\n",
    "            out = model(imgs) \n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n",
    "\n",
    "            # Auxiliary losses are set to 0 as the user's simple fusion model excludes these complex heads.\n",
    "            seg_loss = 0.0\n",
    "            supcon_loss = 0.0 \n",
    "            cons_loss = 0.0   \n",
    "            dom_loss = 0.0\n",
    "\n",
    "            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n",
    "            \n",
    "            total_loss = total_loss / ACCUMULATION_STEPS \n",
    "\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad() \n",
    "\n",
    "        running_loss += total_loss.item() * ACCUMULATION_STEPS\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "        n_batches += 1\n",
    "\n",
    "    if n_batches % ACCUMULATION_STEPS != 0:\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n",
    "\n",
    "    # -------------------\n",
    "    # VALIDATION\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_y_true, val_y_pred = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for weak_imgs, _, labels, masks in val_loader:\n",
    "            imgs = weak_imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if masks is not None:\n",
    "                masks = masks.to(device)\n",
    "\n",
    "            out = model(imgs)\n",
    "            logits = out[\"logits\"]\n",
    "            \n",
    "            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n",
    "            if USE_SEGMENTATION and (masks is not None):\n",
    "                loss += GAMMA_SEG * seg_loss_fn(out[\"seg\"], masks) \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_y_true.extend(labels.cpu().numpy())\n",
    "            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "    vacc = accuracy_score(val_y_true, val_y_pred)\n",
    "    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n",
    "\n",
    "    if vf1 > best_vf1:\n",
    "        best_vf1 = vf1\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\": opt.state_dict(),\n",
    "            \"best_vf1\": best_vf1\n",
    "        }, SAVE_PATH)\n",
    "        patience_count = 0\n",
    "        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n",
    "    else:\n",
    "        patience_count += 1\n",
    "        if patience_count >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2932761,
     "sourceId": 5051281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8580465,
     "sourceId": 13514447,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8580489,
     "sourceId": 13514489,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8582404,
     "sourceId": 13517101,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2869.321705,
   "end_time": "2026-01-08T06:15:03.466399",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-08T05:27:14.144694",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f0d149d09de45f1ab1390e0a600e7cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_35cf230d3b854dc6b112ef22666543b2",
       "max": 86523256.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5f1a15a15c104af79ddc4f4c281e7368",
       "tabbable": null,
       "tooltip": null,
       "value": 86523256.0
      }
     },
     "179d810f557b4bfd88d499a1848c59e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "35cf230d3b854dc6b112ef22666543b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "55f56f4b08af42db9a46ed0565128ba5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f1a15a15c104af79ddc4f4c281e7368": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7f60e1f6d3cd4a578ecd1770ac435e3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "86301d62c7f6431b99220fce430e0e68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d485139550454854b3e9ae8bd18dd25f",
        "IPY_MODEL_0f0d149d09de45f1ab1390e0a600e7cd",
        "IPY_MODEL_f3ee050f2d1f42989dfec388ea5922ce"
       ],
       "layout": "IPY_MODEL_8bdf6fe5cdfd462295b09c4ae6bf30ce",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8bdf6fe5cdfd462295b09c4ae6bf30ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d485139550454854b3e9ae8bd18dd25f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d5a68c0861b2482ea74504a898f8f548",
       "placeholder": "​",
       "style": "IPY_MODEL_179d810f557b4bfd88d499a1848c59e2",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "d5a68c0861b2482ea74504a898f8f548": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3ee050f2d1f42989dfec388ea5922ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_55f56f4b08af42db9a46ed0565128ba5",
       "placeholder": "​",
       "style": "IPY_MODEL_7f60e1f6d3cd4a578ecd1770ac435e3b",
       "tabbable": null,
       "tooltip": null,
       "value": " 86.5M/86.5M [00:01&lt;00:00, 110MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
